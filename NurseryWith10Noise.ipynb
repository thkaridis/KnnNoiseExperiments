{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Parents', ' Has_nurs', ' Form', ' Children', ' Housing',\n",
       "       ' Finance', ' Social', ' Health', 'Class'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nurseryNum = pd.read_csv(\"nursery_numerical.csv\")\n",
    "nurseryNum.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "j = 1\n",
    "features = ['Parents', ' Has_nurs', ' Form', ' Children', ' Housing', ' Finance', ' Social', ' Health']\n",
    "for index, m in nurseryNum.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        nurseryNum.at[index+2, features[j]] = i + 1\n",
    "        j += 3\n",
    "        i+=10\n",
    "        if j >= 7:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for index, m in nurseryNum.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        if index < 12954:\n",
    "            nurseryNum.at[index+5,:] = nurseryNum.loc[j,:]\n",
    "        else:\n",
    "            nurseryNum.at[index+1,:] = nurseryNum.loc[j,:]\n",
    "        j += 120\n",
    "        if j >= 12959:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurseryNum.to_csv('/home/valia/Documents/AppliedDataScience/nureryNoise.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nurseryNum.iloc[:,0:8]\n",
    "labels = nurseryNum.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True) #5 fores me 2 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean and k tuning on 10% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.85      0.83      2112\n",
      "    priority       0.78      0.82      0.79      2197\n",
      "  spec_prior       0.60      1.00      0.75         3\n",
      "   recommend       0.85      0.75      0.80      2007\n",
      "   not_recom       0.59      0.70      0.64       161\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.72      0.82      0.76      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8015432098765433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.85      0.83      2161\n",
      "    priority       0.78      0.80      0.79      2070\n",
      "  spec_prior       0.57      1.00      0.73         4\n",
      "   recommend       0.84      0.76      0.80      2079\n",
      "   not_recom       0.58      0.77      0.66       166\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.72      0.84      0.76      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8029320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.83      0.82      2133\n",
      "    priority       0.76      0.81      0.79      2140\n",
      "  spec_prior       0.50      1.00      0.67         3\n",
      "   recommend       0.86      0.76      0.80      2050\n",
      "   not_recom       0.55      0.73      0.63       154\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.70      0.83      0.74      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7987654320987654\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.84      0.82      2140\n",
      "    priority       0.77      0.79      0.78      2127\n",
      "  spec_prior       0.67      1.00      0.80         4\n",
      "   recommend       0.84      0.76      0.80      2036\n",
      "   not_recom       0.60      0.77      0.67       173\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.74      0.83      0.77      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7964506172839506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.83      0.82      2142\n",
      "    priority       0.77      0.80      0.78      2146\n",
      "  spec_prior       0.67      1.00      0.80         4\n",
      "   recommend       0.84      0.77      0.80      2024\n",
      "   not_recom       0.56      0.73      0.63       164\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.73      0.82      0.77      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.796141975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.86      0.84      2131\n",
      "    priority       0.77      0.82      0.80      2121\n",
      "  spec_prior       0.38      1.00      0.55         3\n",
      "   recommend       0.86      0.76      0.81      2062\n",
      "   not_recom       0.61      0.70      0.65       163\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.69      0.83      0.73      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8106481481481481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.85      0.84      2164\n",
      "    priority       0.77      0.81      0.79      2127\n",
      "  spec_prior       0.38      1.00      0.55         3\n",
      "   recommend       0.84      0.76      0.79      2029\n",
      "   not_recom       0.59      0.82      0.68       157\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.68      0.84      0.73      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8041666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.85      0.84      2109\n",
      "    priority       0.78      0.82      0.80      2140\n",
      "  spec_prior       0.67      1.00      0.80         4\n",
      "   recommend       0.86      0.77      0.82      2057\n",
      "   not_recom       0.58      0.79      0.67       170\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.74      0.85      0.78      6480\n",
      "weighted avg       0.82      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8123456790123457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.83      0.83      2157\n",
      "    priority       0.77      0.80      0.79      2099\n",
      "  spec_prior       0.71      1.00      0.83         5\n",
      "   recommend       0.83      0.78      0.80      2063\n",
      "   not_recom       0.54      0.77      0.64       156\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.74      0.84      0.78      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8016975308641975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.86      0.83      2116\n",
      "    priority       0.77      0.81      0.79      2168\n",
      "  spec_prior       0.29      1.00      0.44         2\n",
      "   recommend       0.87      0.76      0.81      2023\n",
      "   not_recom       0.57      0.67      0.62       171\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.66      0.82      0.70      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8041666666666667\n",
      "mean accuracy 0.802885802469136\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.94      0.91      2142\n",
      "    priority       0.86      0.90      0.88      2183\n",
      "  spec_prior       0.43      1.00      0.60         3\n",
      "   recommend       0.92      0.84      0.88      2000\n",
      "   not_recom       0.90      0.64      0.75       152\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.80      0.86      0.80      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8888888888888888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.94      0.92      2131\n",
      "    priority       0.84      0.92      0.88      2084\n",
      "  spec_prior       0.50      1.00      0.67         4\n",
      "   recommend       0.93      0.83      0.88      2086\n",
      "   not_recom       0.92      0.62      0.74       175\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.82      0.86      0.82      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8887345679012346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.94      0.91      2125\n",
      "    priority       0.85      0.89      0.87      2169\n",
      "  spec_prior       0.33      1.00      0.50         3\n",
      "   recommend       0.92      0.84      0.88      2015\n",
      "   not_recom       0.96      0.60      0.74       168\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.79      0.85      0.78      6480\n",
      "weighted avg       0.89      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8847222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2148\n",
      "    priority       0.86      0.92      0.89      2098\n",
      "  spec_prior       0.33      1.00      0.50         4\n",
      "   recommend       0.95      0.85      0.89      2071\n",
      "   not_recom       0.93      0.66      0.77       159\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.80      0.88      0.80      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9013888888888889\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2135\n",
      "    priority       0.86      0.91      0.88      2134\n",
      "  spec_prior       0.29      1.00      0.44         2\n",
      "   recommend       0.93      0.85      0.89      2043\n",
      "   not_recom       0.93      0.62      0.74       166\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.78      0.87      0.78      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8959876543209877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.93      0.92      2138\n",
      "    priority       0.85      0.93      0.88      2133\n",
      "  spec_prior       0.71      1.00      0.83         5\n",
      "   recommend       0.93      0.84      0.88      2043\n",
      "   not_recom       0.94      0.59      0.73       161\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.87      0.86      0.85      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8925925925925926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.94      0.93      2186\n",
      "    priority       0.84      0.93      0.88      2076\n",
      "  spec_prior       0.50      1.00      0.67         4\n",
      "   recommend       0.93      0.82      0.87      2049\n",
      "   not_recom       0.97      0.70      0.81       165\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.83      0.88      0.83      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8922839506172839\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.94      0.92      2087\n",
      "    priority       0.85      0.91      0.88      2191\n",
      "  spec_prior       0.38      1.00      0.55         3\n",
      "   recommend       0.94      0.83      0.88      2037\n",
      "   not_recom       0.94      0.65      0.77       162\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.80      0.87      0.80      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8888888888888888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.94      0.93      2165\n",
      "    priority       0.86      0.92      0.89      2123\n",
      "  spec_prior       0.23      1.00      0.38         3\n",
      "   recommend       0.93      0.84      0.88      2004\n",
      "   not_recom       0.95      0.57      0.71       185\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.77      0.86      0.76      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8945987654320988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.94      0.92      2108\n",
      "    priority       0.85      0.91      0.88      2144\n",
      "  spec_prior       0.80      1.00      0.89         4\n",
      "   recommend       0.93      0.83      0.88      2082\n",
      "   not_recom       0.88      0.69      0.77       142\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.87      0.88      0.87      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8912037037037037\n",
      "mean accuracy 0.891929012345679\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2119\n",
      "    priority       0.84      0.92      0.88      2134\n",
      "  spec_prior       0.38      1.00      0.55         3\n",
      "   recommend       0.93      0.82      0.87      2060\n",
      "   not_recom       0.90      0.68      0.78       164\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.79      0.87      0.80      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8912037037037037\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valia/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2154\n",
      "    priority       0.86      0.89      0.88      2133\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.90      0.88      0.89      2026\n",
      "   not_recom       0.91      0.64      0.75       163\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8944444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2140\n",
      "    priority       0.86      0.92      0.89      2165\n",
      "  spec_prior       0.33      1.00      0.50         3\n",
      "   recommend       0.92      0.83      0.87      2005\n",
      "   not_recom       0.94      0.62      0.74       167\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.79      0.86      0.79      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8945987654320988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.93      0.92      2133\n",
      "    priority       0.83      0.91      0.87      2102\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.91      0.82      0.87      2081\n",
      "   not_recom       0.90      0.71      0.80       160\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.71      0.67      0.69      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8828703703703704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.94      0.92      2129\n",
      "    priority       0.86      0.89      0.88      2119\n",
      "  spec_prior       0.60      1.00      0.75         3\n",
      "   recommend       0.90      0.86      0.88      2053\n",
      "   not_recom       0.96      0.59      0.73       176\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.85      0.86      0.83      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8898148148148148\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.94      0.93      2144\n",
      "    priority       0.85      0.91      0.88      2148\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.92      0.84      0.88      2033\n",
      "   not_recom       0.92      0.72      0.81       151\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.68      0.70      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8929012345679013\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2123\n",
      "    priority       0.85      0.92      0.88      2104\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.93      0.85      0.89      2088\n",
      "   not_recom       0.88      0.64      0.74       162\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8958333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2150\n",
      "    priority       0.86      0.91      0.88      2163\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.91      0.85      0.88      1998\n",
      "   not_recom       0.86      0.73      0.79       165\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.71      0.68      0.70      6480\n",
      "weighted avg       0.90      0.90      0.89      6480\n",
      "\n",
      "accuracy:  0.8950617283950617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.94      0.92      2113\n",
      "    priority       0.85      0.90      0.87      2123\n",
      "  spec_prior       0.17      1.00      0.29         2\n",
      "   recommend       0.90      0.84      0.87      2073\n",
      "   not_recom       0.97      0.61      0.75       169\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.76      0.86      0.74      6480\n",
      "weighted avg       0.89      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8834876543209876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.94      2160\n",
      "    priority       0.85      0.92      0.89      2144\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.93      0.85      0.88      2013\n",
      "   not_recom       0.89      0.64      0.74       158\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.899537037037037\n",
      "mean accuracy 0.8919753086419753\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.93      0.93      2112\n",
      "    priority       0.86      0.91      0.89      2127\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.90      0.87      0.89      2075\n",
      "   not_recom       0.92      0.69      0.79       163\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.68      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8981481481481481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2161\n",
      "    priority       0.85      0.92      0.88      2140\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.91      0.84      0.87      2011\n",
      "   not_recom       0.91      0.62      0.74       164\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.66      0.68      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8918209876543209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.93      0.93      2191\n",
      "    priority       0.87      0.91      0.89      2150\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.89      0.88      0.89      1969\n",
      "   not_recom       0.91      0.57      0.71       167\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.66      0.68      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.899074074074074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.94      0.93      2082\n",
      "    priority       0.88      0.92      0.90      2117\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.91      0.87      0.89      2117\n",
      "   not_recom       0.93      0.62      0.75       160\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9006172839506172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.93      0.93      2141\n",
      "    priority       0.87      0.92      0.89      2110\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.91      0.87      0.89      2064\n",
      "   not_recom       0.93      0.62      0.74       162\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9006172839506172\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2132\n",
      "    priority       0.86      0.90      0.88      2157\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.89      0.86      0.88      2022\n",
      "   not_recom       0.91      0.64      0.75       165\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8933641975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2141\n",
      "    priority       0.84      0.92      0.88      2088\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.84      0.88      2083\n",
      "   not_recom       0.94      0.64      0.76       165\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8941358024691358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.93      0.93      2132\n",
      "    priority       0.88      0.90      0.89      2179\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.88      0.89      0.89      2003\n",
      "   not_recom       0.84      0.67      0.74       162\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.71      0.68      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8998456790123457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.94      0.94      2159\n",
      "    priority       0.85      0.92      0.88      2125\n",
      "  spec_prior       0.11      1.00      0.20         1\n",
      "   recommend       0.91      0.86      0.89      2030\n",
      "   not_recom       0.93      0.61      0.74       165\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.75      0.86      0.73      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8986111111111111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.93      2114\n",
      "    priority       0.86      0.90      0.88      2142\n",
      "  spec_prior       0.00      0.00      0.00         6\n",
      "   recommend       0.90      0.86      0.88      2056\n",
      "   not_recom       0.90      0.66      0.76       162\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.894753086419753\n",
      "mean accuracy 0.8970987654320988\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2123\n",
      "    priority       0.84      0.91      0.88      2089\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.92      0.84      0.88      2096\n",
      "   not_recom       0.95      0.52      0.67       168\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.65      0.67      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8904320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.94      0.94      2150\n",
      "    priority       0.87      0.90      0.89      2178\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.89      0.89      0.89      1990\n",
      "   not_recom       0.90      0.57      0.70       159\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.66      0.68      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9016975308641976\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2133\n",
      "    priority       0.87      0.92      0.90      2143\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.92      0.86      0.89      2044\n",
      "   not_recom       0.92      0.56      0.70       156\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.66      0.68      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9020061728395061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.94      2140\n",
      "    priority       0.86      0.91      0.88      2124\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.91      0.85      0.88      2042\n",
      "   not_recom       0.96      0.56      0.71       171\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.65      0.68      6480\n",
      "weighted avg       0.90      0.90      0.89      6480\n",
      "\n",
      "accuracy:  0.8953703703703704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2089\n",
      "    priority       0.85      0.91      0.88      2154\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.91      0.85      0.88      2066\n",
      "   not_recom       0.92      0.55      0.69       166\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.65      0.68      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8918209876543209\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.93      0.93      2184\n",
      "    priority       0.86      0.91      0.88      2113\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.90      0.87      0.88      2020\n",
      "   not_recom       0.94      0.55      0.70       161\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.65      0.68      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8932098765432098\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.93      2126\n",
      "    priority       0.84      0.92      0.88      2106\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.91      0.85      0.88      2083\n",
      "   not_recom       0.94      0.56      0.70       163\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.65      0.68      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8933641975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.94      2147\n",
      "    priority       0.87      0.89      0.88      2161\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.89      0.88      0.89      2003\n",
      "   not_recom       0.86      0.62      0.72       164\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.71      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8967592592592593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2157\n",
      "    priority       0.89      0.89      0.89      2105\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.89      0.90      0.89      2048\n",
      "   not_recom       0.92      0.59      0.72       167\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.66      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8996913580246914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.95      0.94      2116\n",
      "    priority       0.86      0.92      0.89      2162\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.92      0.86      0.89      2038\n",
      "   not_recom       0.94      0.62      0.75       160\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9\n",
      "mean accuracy 0.8964351851851852\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.92      0.91      2124\n",
      "    priority       0.88      0.87      0.88      2207\n",
      "  spec_prior       0.87      0.87      0.87      1995\n",
      "   not_recom       0.97      0.51      0.67       154\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.90      0.79      0.83      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8800925925925925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.92      0.91      2149\n",
      "    priority       0.83      0.90      0.86      2060\n",
      "  spec_prior       0.00      0.00      0.00         7\n",
      "   recommend       0.89      0.84      0.87      2091\n",
      "   not_recom       0.91      0.45      0.60       173\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.70      0.62      0.65      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.8719135802469136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.90      0.91      2137\n",
      "    priority       0.86      0.90      0.88      2154\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.87      0.88      0.87      2009\n",
      "   not_recom       0.94      0.45      0.60       175\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.72      0.62      0.65      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8799382716049383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.93      0.91      2136\n",
      "    priority       0.86      0.90      0.88      2113\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.85      0.87      2077\n",
      "   not_recom       0.96      0.45      0.61       152\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.72      0.63      0.65      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8828703703703704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.92      0.91      2141\n",
      "    priority       0.85      0.89      0.87      2139\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.87      0.86      0.86      2037\n",
      "   not_recom       0.97      0.47      0.63       161\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.72      0.63      0.66      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8774691358024691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.88      0.93      0.91      2132\n",
      "    priority       0.84      0.89      0.86      2128\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.91      0.84      0.87      2049\n",
      "   not_recom       0.90      0.45      0.60       166\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.71      0.62      0.65      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.8739197530864198\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.95      0.92      2117\n",
      "    priority       0.86      0.89      0.87      2147\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.90      0.85      0.87      2051\n",
      "   not_recom       0.94      0.50      0.65       160\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.64      0.66      6480\n",
      "weighted avg       0.89      0.89      0.88      6480\n",
      "\n",
      "accuracy:  0.8850308641975309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.90      0.90      2156\n",
      "    priority       0.84      0.89      0.87      2120\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.88      0.85      0.86      2035\n",
      "   not_recom       0.96      0.51      0.66       167\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.71      0.63      0.66      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.871141975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.93      0.92      2126\n",
      "    priority       0.85      0.91      0.88      2114\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.91      0.83      0.87      2086\n",
      "   not_recom       0.96      0.53      0.68       152\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.72      0.64      0.67      6480\n",
      "weighted avg       0.89      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8837962962962963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.91      0.90      2147\n",
      "    priority       0.87      0.89      0.88      2153\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.88      0.88      0.88      2000\n",
      "   not_recom       0.94      0.44      0.60       175\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.71      0.62      0.65      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.879320987654321\n",
      "mean accuracy 0.8785493827160493\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.90      0.87      2135\n",
      "    priority       0.81      0.87      0.84      2147\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.87      0.79      0.83      2028\n",
      "   not_recom       0.90      0.28      0.43       165\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6480\n",
      "   macro avg       0.68      0.57      0.59      6480\n",
      "weighted avg       0.84      0.84      0.83      6480\n",
      "\n",
      "accuracy:  0.8382716049382716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.88      0.89      2138\n",
      "    priority       0.84      0.88      0.86      2120\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.85      0.85      0.85      2058\n",
      "   not_recom       0.92      0.43      0.59       162\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.70      0.61      0.64      6480\n",
      "weighted avg       0.86      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.859104938271605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.89      0.85      2142\n",
      "    priority       0.82      0.85      0.84      2135\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.86      0.80      0.83      2041\n",
      "   not_recom       0.90      0.35      0.50       158\n",
      "\n",
      "   micro avg       0.83      0.83      0.83      6480\n",
      "   macro avg       0.68      0.58      0.60      6480\n",
      "weighted avg       0.83      0.83      0.83      6480\n",
      "\n",
      "accuracy:  0.832716049382716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.85      0.88      0.86      2131\n",
      "    priority       0.84      0.88      0.86      2132\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.86      0.84      0.85      2045\n",
      "   not_recom       0.95      0.34      0.50       169\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.70      0.59      0.62      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.8529320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.90      0.87      2104\n",
      "    priority       0.81      0.87      0.84      2123\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.87      0.80      0.83      2070\n",
      "   not_recom       0.94      0.28      0.44       180\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6480\n",
      "   macro avg       0.69      0.57      0.60      6480\n",
      "weighted avg       0.84      0.84      0.84      6480\n",
      "\n",
      "accuracy:  0.8410493827160493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.86      0.87      0.87      2169\n",
      "    priority       0.84      0.88      0.86      2144\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.85      0.83      0.84      2016\n",
      "   not_recom       0.88      0.41      0.56       147\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.69      0.60      0.63      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.8524691358024692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.89      0.86      2114\n",
      "    priority       0.83      0.85      0.84      2134\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.85      0.82      0.84      2053\n",
      "   not_recom       0.94      0.33      0.49       176\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6480\n",
      "   macro avg       0.69      0.58      0.60      6480\n",
      "weighted avg       0.84      0.84      0.83      6480\n",
      "\n",
      "accuracy:  0.8384259259259259\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.85      0.89      0.87      2159\n",
      "    priority       0.82      0.88      0.85      2133\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.88      0.80      0.83      2033\n",
      "   not_recom       0.92      0.38      0.54       151\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.69      0.59      0.62      6480\n",
      "weighted avg       0.85      0.85      0.84      6480\n",
      "\n",
      "accuracy:  0.8467592592592592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.87      0.88      0.87      2142\n",
      "    priority       0.83      0.87      0.85      2131\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.85      0.84      0.84      2038\n",
      "   not_recom       0.89      0.35      0.50       164\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.69      0.59      0.61      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.8492283950617284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.85      0.89      0.87      2131\n",
      "    priority       0.82      0.86      0.84      2136\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.87      0.82      0.85      2048\n",
      "   not_recom       0.96      0.43      0.59       163\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.70      0.60      0.63      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.8484567901234568\n",
      "mean accuracy 0.8459413580246913\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.75      0.81      0.78      2143\n",
      "    priority       0.79      0.81      0.80      2130\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.81      0.79      0.80      2032\n",
      "   not_recom       0.89      0.15      0.25       172\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.65      0.51      0.53      6480\n",
      "weighted avg       0.79      0.78      0.78      6480\n",
      "\n",
      "accuracy:  0.7847222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.74      0.81      0.77      2130\n",
      "    priority       0.79      0.81      0.80      2137\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.82      0.77      0.79      2054\n",
      "   not_recom       0.89      0.21      0.34       155\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.65      0.52      0.54      6480\n",
      "weighted avg       0.78      0.78      0.78      6480\n",
      "\n",
      "accuracy:  0.7813271604938271\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.71      0.82      0.76      2127\n",
      "    priority       0.78      0.80      0.79      2133\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.84      0.72      0.78      2056\n",
      "   not_recom       0.88      0.14      0.24       161\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.64      0.50      0.51      6480\n",
      "weighted avg       0.78      0.77      0.76      6480\n",
      "\n",
      "accuracy:  0.7666666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.77      0.76      0.77      2146\n",
      "    priority       0.76      0.83      0.79      2134\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.80      0.78      0.79      2030\n",
      "   not_recom       0.88      0.17      0.28       166\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.64      0.51      0.53      6480\n",
      "weighted avg       0.78      0.77      0.77      6480\n",
      "\n",
      "accuracy:  0.774074074074074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.74      0.76      0.75      2147\n",
      "    priority       0.75      0.82      0.79      2091\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.82      0.77      0.79      2060\n",
      "   not_recom       0.86      0.14      0.24       178\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.63      0.50      0.51      6480\n",
      "weighted avg       0.77      0.77      0.76      6480\n",
      "\n",
      "accuracy:  0.7675925925925926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.70      0.83      0.76      2126\n",
      "    priority       0.83      0.77      0.80      2176\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.81      0.76      0.78      2026\n",
      "   not_recom       0.85      0.15      0.26       149\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.64      0.50      0.52      6480\n",
      "weighted avg       0.78      0.77      0.77      6480\n",
      "\n",
      "accuracy:  0.7725308641975308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.73      0.79      0.76      2157\n",
      "    priority       0.81      0.80      0.81      2166\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.79      0.80      0.80      1985\n",
      "   not_recom       0.88      0.14      0.24       169\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.64      0.50      0.52      6480\n",
      "weighted avg       0.78      0.78      0.77      6480\n",
      "\n",
      "accuracy:  0.7776234567901235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.75      0.82      0.78      2116\n",
      "    priority       0.76      0.84      0.80      2101\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.84      0.73      0.78      2101\n",
      "   not_recom       0.87      0.17      0.29       158\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.64      0.51      0.53      6480\n",
      "weighted avg       0.79      0.78      0.77      6480\n",
      "\n",
      "accuracy:  0.7802469135802469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.74      0.81      0.77      2126\n",
      "    priority       0.79      0.81      0.80      2138\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.82      0.76      0.79      2046\n",
      "   not_recom       0.89      0.10      0.17       168\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.65      0.50      0.51      6480\n",
      "weighted avg       0.78      0.78      0.77      6480\n",
      "\n",
      "accuracy:  0.7787037037037037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.74      0.79      0.77      2147\n",
      "    priority       0.77      0.80      0.79      2129\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.80      0.77      0.78      2040\n",
      "   not_recom       0.86      0.24      0.37       159\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.64      0.52      0.54      6480\n",
      "weighted avg       0.77      0.77      0.77      6480\n",
      "\n",
      "accuracy:  0.7725308641975308\n",
      "mean accuracy 0.7756018518518519\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.69      0.72      0.70      2165\n",
      "    priority       0.77      0.81      0.79      2098\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.78      0.76      0.77      2048\n",
      "   not_recom       0.76      0.10      0.17       164\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.60      0.48      0.49      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7450617283950617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.67      0.78      0.72      2108\n",
      "    priority       0.79      0.73      0.76      2169\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.80      0.79      0.79      2038\n",
      "   not_recom       0.67      0.02      0.05       163\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.58      0.46      0.46      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7464506172839506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.66      0.74      0.70      2116\n",
      "    priority       0.78      0.77      0.77      2139\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.78      0.74      0.76      2052\n",
      "   not_recom       0.77      0.06      0.11       171\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.60      0.46      0.47      6480\n",
      "weighted avg       0.74      0.73      0.73      6480\n",
      "\n",
      "accuracy:  0.7344135802469136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.66      0.73      0.69      2157\n",
      "    priority       0.75      0.75      0.75      2128\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.78      0.76      0.77      2034\n",
      "   not_recom       0.69      0.07      0.13       156\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.58      0.46      0.47      6480\n",
      "weighted avg       0.73      0.73      0.72      6480\n",
      "\n",
      "accuracy:  0.7285493827160494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.68      0.72      0.70      2150\n",
      "    priority       0.76      0.78      0.77      2148\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.78      0.79      0.78      2007\n",
      "   not_recom       1.00      0.01      0.02       173\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.65      0.46      0.46      6480\n",
      "weighted avg       0.75      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.741820987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.67      0.75      0.70      2123\n",
      "    priority       0.77      0.78      0.77      2119\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.79      0.74      0.76      2079\n",
      "   not_recom       0.77      0.11      0.19       154\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.60      0.47      0.49      6480\n",
      "weighted avg       0.74      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.7376543209876543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.69      0.75      0.72      2148\n",
      "    priority       0.78      0.75      0.77      2137\n",
      "  spec_prior       0.00      0.00      0.00         6\n",
      "   recommend       0.78      0.77      0.77      2036\n",
      "   not_recom       0.83      0.19      0.31       153\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.61      0.49      0.51      6480\n",
      "weighted avg       0.75      0.74      0.74      6480\n",
      "\n",
      "accuracy:  0.7444444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.65      0.72      0.68      2125\n",
      "    priority       0.74      0.78      0.76      2130\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.80      0.75      0.78      2050\n",
      "   not_recom       0.00      0.00      0.00       174\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.44      0.45      0.44      6480\n",
      "weighted avg       0.71      0.73      0.72      6480\n",
      "\n",
      "accuracy:  0.7282407407407407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.64      0.76      0.69      2088\n",
      "    priority       0.77      0.75      0.76      2180\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.81      0.72      0.76      2046\n",
      "   not_recom       0.75      0.07      0.13       162\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.59      0.46      0.47      6480\n",
      "weighted avg       0.74      0.73      0.72      6480\n",
      "\n",
      "accuracy:  0.7266975308641975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.71      0.72      0.72      2185\n",
      "    priority       0.77      0.79      0.78      2087\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.77      0.81      0.79      2040\n",
      "   not_recom       0.70      0.04      0.08       165\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.59      0.47      0.47      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7520061728395062\n",
      "mean accuracy 0.738533950617284\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.802885802469136, 0.891929012345679, 0.8919753086419753, 0.8970987654320988, 0.8964351851851852, 0.8785493827160493, 0.8459413580246913, 0.7756018518518519, 0.738533950617284]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_euclidean\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski and k tuning on 10% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.86      0.83      2126\n",
      "    priority       0.78      0.81      0.79      2142\n",
      "  spec_prior       1.00      1.00      1.00         3\n",
      "   recommend       0.87      0.76      0.81      2047\n",
      "   not_recom       0.61      0.69      0.65       162\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.81      0.82      0.82      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8067901234567901\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.82      0.82      2147\n",
      "    priority       0.75      0.81      0.78      2125\n",
      "  spec_prior       0.44      1.00      0.62         4\n",
      "   recommend       0.85      0.75      0.80      2039\n",
      "   not_recom       0.51      0.70      0.59       165\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6480\n",
      "   macro avg       0.68      0.82      0.72      6480\n",
      "weighted avg       0.80      0.79      0.80      6480\n",
      "\n",
      "accuracy:  0.7938271604938272\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.84      0.83      2162\n",
      "    priority       0.76      0.81      0.79      2105\n",
      "  spec_prior       0.43      1.00      0.60         3\n",
      "   recommend       0.86      0.77      0.81      2064\n",
      "   not_recom       0.49      0.71      0.58       146\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.67      0.83      0.72      6480\n",
      "weighted avg       0.81      0.80      0.81      6480\n",
      "\n",
      "accuracy:  0.8044753086419754\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.80      0.85      0.82      2111\n",
      "    priority       0.78      0.79      0.78      2162\n",
      "  spec_prior       0.50      1.00      0.67         4\n",
      "   recommend       0.85      0.75      0.80      2022\n",
      "   not_recom       0.57      0.70      0.63       181\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.70      0.82      0.74      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7952160493827161\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.84      0.83      2143\n",
      "    priority       0.78      0.81      0.80      2149\n",
      "  spec_prior       0.67      1.00      0.80         4\n",
      "   recommend       0.86      0.77      0.81      2029\n",
      "   not_recom       0.56      0.74      0.64       155\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.74      0.83      0.78      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8084876543209877\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.82      0.82      2130\n",
      "    priority       0.78      0.80      0.79      2118\n",
      "  spec_prior       0.50      1.00      0.67         3\n",
      "   recommend       0.84      0.79      0.82      2057\n",
      "   not_recom       0.55      0.77      0.64       172\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.70      0.84      0.75      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.803395061728395\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.85      0.83      2103\n",
      "    priority       0.76      0.82      0.79      2134\n",
      "  spec_prior       0.62      1.00      0.77         5\n",
      "   recommend       0.86      0.73      0.79      2087\n",
      "   not_recom       0.58      0.72      0.64       151\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.73      0.83      0.77      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8012345679012346\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.84      0.83      2170\n",
      "    priority       0.79      0.80      0.79      2133\n",
      "  spec_prior       0.40      1.00      0.57         2\n",
      "   recommend       0.83      0.79      0.81      1999\n",
      "   not_recom       0.61      0.71      0.66       176\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.69      0.83      0.73      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8067901234567901\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.85      0.84      2140\n",
      "    priority       0.79      0.82      0.80      2158\n",
      "  spec_prior       0.40      1.00      0.57         2\n",
      "   recommend       0.86      0.78      0.82      2016\n",
      "   not_recom       0.58      0.72      0.64       164\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.69      0.83      0.73      6480\n",
      "weighted avg       0.82      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8138888888888889\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.84      0.82      2133\n",
      "    priority       0.78      0.79      0.79      2109\n",
      "  spec_prior       0.56      1.00      0.71         5\n",
      "   recommend       0.83      0.77      0.80      2070\n",
      "   not_recom       0.55      0.72      0.63       163\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.71      0.82      0.75      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7979938271604938\n",
      "mean accuracy 0.80320987654321\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_minkowski = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.95      0.92      2135\n",
      "    priority       0.85      0.91      0.88      2128\n",
      "  spec_prior       0.50      1.00      0.67         4\n",
      "   recommend       0.92      0.83      0.87      2052\n",
      "   not_recom       0.96      0.66      0.78       161\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.83      0.87      0.82      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8887345679012346\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.93      0.92      2138\n",
      "    priority       0.86      0.92      0.89      2139\n",
      "  spec_prior       0.38      1.00      0.55         3\n",
      "   recommend       0.92      0.85      0.88      2034\n",
      "   not_recom       0.90      0.57      0.70       166\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.79      0.85      0.79      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8907407407407407\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.94      0.92      2138\n",
      "    priority       0.85      0.91      0.88      2114\n",
      "  spec_prior       0.36      1.00      0.53         4\n",
      "   recommend       0.92      0.84      0.88      2057\n",
      "   not_recom       0.96      0.59      0.73       167\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.80      0.86      0.79      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.890895061728395\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.95      0.92      2135\n",
      "    priority       0.86      0.91      0.89      2153\n",
      "  spec_prior       0.33      1.00      0.50         3\n",
      "   recommend       0.94      0.85      0.89      2029\n",
      "   not_recom       0.92      0.61      0.73       160\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.79      0.86      0.79      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8967592592592593\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.95      0.92      2154\n",
      "    priority       0.85      0.92      0.88      2102\n",
      "  spec_prior       0.50      1.00      0.67         3\n",
      "   recommend       0.94      0.83      0.88      2050\n",
      "   not_recom       0.94      0.69      0.79       171\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.82      0.88      0.83      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8929012345679013\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.95      0.92      2119\n",
      "    priority       0.86      0.91      0.88      2165\n",
      "  spec_prior       0.44      1.00      0.62         4\n",
      "   recommend       0.93      0.84      0.88      2036\n",
      "   not_recom       0.89      0.65      0.75       156\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.80      0.87      0.81      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8912037037037037\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.88      0.94      0.91      2096\n",
      "    priority       0.86      0.92      0.89      2148\n",
      "  spec_prior       0.50      1.00      0.67         4\n",
      "   recommend       0.93      0.84      0.88      2057\n",
      "   not_recom       0.95      0.57      0.71       175\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.83      0.85      0.81      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8893518518518518\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.94      0.92      2177\n",
      "    priority       0.84      0.91      0.87      2119\n",
      "  spec_prior       0.33      1.00      0.50         3\n",
      "   recommend       0.92      0.83      0.87      2029\n",
      "   not_recom       0.93      0.66      0.77       152\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.79      0.87      0.79      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8862654320987654\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.94      0.92      2110\n",
      "    priority       0.85      0.92      0.88      2129\n",
      "  spec_prior       0.57      1.00      0.73         4\n",
      "   recommend       0.93      0.83      0.88      2076\n",
      "   not_recom       0.93      0.60      0.73       161\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.84      0.86      0.83      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8905864197530864\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.94      0.92      2163\n",
      "    priority       0.86      0.91      0.88      2138\n",
      "  spec_prior       0.33      1.00      0.50         3\n",
      "   recommend       0.92      0.84      0.88      2010\n",
      "   not_recom       0.95      0.67      0.79       166\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.79      0.87      0.79      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8919753086419753\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.93      0.92      2136\n",
      "    priority       0.85      0.91      0.88      2106\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.93      0.85      0.89      2070\n",
      "   not_recom       0.86      0.76      0.81       163\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.71      0.69      0.70      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8949074074074074\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.94      0.93      2137\n",
      "    priority       0.85      0.90      0.88      2161\n",
      "  spec_prior       0.15      1.00      0.27         2\n",
      "   recommend       0.91      0.84      0.88      2016\n",
      "   not_recom       0.92      0.59      0.72       164\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.75      0.86      0.73      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8888888888888888\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2151\n",
      "    priority       0.86      0.90      0.88      2160\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.92      0.85      0.88      2001\n",
      "   not_recom       0.89      0.68      0.77       164\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.68      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8962962962962963\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.93      0.93      2122\n",
      "    priority       0.84      0.91      0.87      2107\n",
      "  spec_prior       0.60      1.00      0.75         3\n",
      "   recommend       0.91      0.84      0.87      2085\n",
      "   not_recom       0.94      0.72      0.81       163\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.84      0.88      0.85      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.888425925925926\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.93      0.92      2147\n",
      "    priority       0.86      0.92      0.89      2110\n",
      "  spec_prior       0.15      1.00      0.27         2\n",
      "   recommend       0.92      0.85      0.89      2050\n",
      "   not_recom       0.95      0.67      0.79       171\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.76      0.88      0.75      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8975308641975308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2126\n",
      "    priority       0.87      0.91      0.89      2157\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.91      0.85      0.88      2036\n",
      "   not_recom       0.84      0.72      0.78       156\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.71      0.69      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8981481481481481\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2127\n",
      "    priority       0.85      0.90      0.88      2150\n",
      "  spec_prior       0.33      1.00      0.50         2\n",
      "   recommend       0.92      0.85      0.88      2034\n",
      "   not_recom       0.89      0.67      0.76       167\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.78      0.87      0.79      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8939814814814815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.95      0.93      2146\n",
      "    priority       0.86      0.91      0.88      2117\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.92      0.84      0.88      2052\n",
      "   not_recom       0.91      0.72      0.81       160\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.68      0.70      6480\n",
      "weighted avg       0.90      0.90      0.89      6480\n",
      "\n",
      "accuracy:  0.8950617283950617\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2161\n",
      "    priority       0.87      0.92      0.89      2092\n",
      "  spec_prior       0.14      1.00      0.25         2\n",
      "   recommend       0.93      0.86      0.89      2060\n",
      "   not_recom       0.88      0.65      0.75       165\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.75      0.87      0.74      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9015432098765432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.95      0.92      2112\n",
      "    priority       0.86      0.91      0.88      2175\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.92      0.83      0.87      2026\n",
      "   not_recom       0.87      0.65      0.74       162\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.71      0.67      0.68      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8882716049382716\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2122\n",
      "    priority       0.87      0.90      0.88      2156\n",
      "  spec_prior       0.00      0.00      0.00         6\n",
      "   recommend       0.90      0.87      0.88      2038\n",
      "   not_recom       0.84      0.61      0.71       158\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.71      0.66      0.68      6480\n",
      "weighted avg       0.89      0.90      0.89      6480\n",
      "\n",
      "accuracy:  0.8958333333333334\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.93      0.93      2151\n",
      "    priority       0.86      0.92      0.89      2111\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.92      0.85      0.88      2048\n",
      "   not_recom       0.95      0.62      0.75       169\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.66      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8961419753086419\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.93      0.93      2165\n",
      "    priority       0.87      0.91      0.89      2132\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.90      0.87      0.89      2022\n",
      "   not_recom       0.90      0.72      0.80       157\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.69      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9001543209876544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2108\n",
      "    priority       0.85      0.92      0.88      2135\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.91      0.84      0.88      2064\n",
      "   not_recom       0.93      0.63      0.75       170\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8916666666666667\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.93      0.93      2137\n",
      "    priority       0.86      0.91      0.88      2141\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.85      0.87      2044\n",
      "   not_recom       0.93      0.60      0.73       156\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.66      0.68      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8907407407407407\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2136\n",
      "    priority       0.86      0.91      0.89      2126\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.91      0.85      0.88      2042\n",
      "   not_recom       0.91      0.67      0.77       171\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.89      6480\n",
      "\n",
      "accuracy:  0.8958333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.93      0.93      2160\n",
      "    priority       0.86      0.92      0.88      2116\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.90      0.85      0.87      2039\n",
      "   not_recom       0.87      0.64      0.74       161\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.71      0.67      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8933641975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2113\n",
      "    priority       0.88      0.91      0.90      2151\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.91      0.88      0.89      2047\n",
      "   not_recom       0.91      0.71      0.80       166\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.69      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9032407407407408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.93      0.93      2109\n",
      "    priority       0.87      0.91      0.89      2182\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.91      0.88      0.89      2002\n",
      "   not_recom       0.92      0.55      0.69       182\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.66      0.68      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8983024691358025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.92      0.92      2164\n",
      "    priority       0.84      0.91      0.88      2085\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.90      0.84      0.87      2084\n",
      "   not_recom       0.93      0.77      0.85       145\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.69      0.70      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8881172839506173\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.94      2091\n",
      "    priority       0.87      0.91      0.89      2209\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.87      0.89      2018\n",
      "   not_recom       0.92      0.59      0.72       159\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.66      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9018518518518519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.94      0.94      2182\n",
      "    priority       0.86      0.91      0.88      2058\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.90      0.88      0.89      2068\n",
      "   not_recom       0.93      0.53      0.67       168\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.65      0.68      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8998456790123457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2149\n",
      "    priority       0.88      0.90      0.89      2165\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.89      0.87      0.88      1997\n",
      "   not_recom       0.89      0.65      0.75       164\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8975308641975308\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.93      0.93      2124\n",
      "    priority       0.85      0.92      0.88      2102\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.91      0.86      0.88      2089\n",
      "   not_recom       0.91      0.52      0.66       163\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.64      0.67      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8933641975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.95      0.94      2136\n",
      "    priority       0.85      0.93      0.89      2139\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.92      0.85      0.88      2025\n",
      "   not_recom       0.93      0.52      0.66       176\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.65      0.67      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8978395061728395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.93      2137\n",
      "    priority       0.86      0.91      0.88      2128\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.90      0.86      0.88      2061\n",
      "   not_recom       0.93      0.64      0.76       151\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8942901234567902\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2138\n",
      "    priority       0.86      0.91      0.88      2122\n",
      "  spec_prior       0.00      0.00      0.00         6\n",
      "   recommend       0.91      0.87      0.89      2043\n",
      "   not_recom       0.91      0.51      0.66       171\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.65      0.67      6480\n",
      "weighted avg       0.90      0.90      0.89      6480\n",
      "\n",
      "accuracy:  0.8966049382716049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2135\n",
      "    priority       0.86      0.91      0.88      2145\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.90      0.85      0.87      2043\n",
      "   not_recom       0.98      0.66      0.79       156\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.73      0.67      0.70      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8941358024691358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.93      0.92      2105\n",
      "    priority       0.84      0.91      0.88      2111\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.91      0.84      0.87      2092\n",
      "   not_recom       0.92      0.57      0.71       167\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.65      0.68      6480\n",
      "weighted avg       0.89      0.89      0.88      6480\n",
      "\n",
      "accuracy:  0.8850308641975309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.94      2168\n",
      "    priority       0.88      0.90      0.89      2156\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.90      0.89      0.90      1994\n",
      "   not_recom       0.91      0.59      0.72       160\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9037037037037037\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.92      0.90      2108\n",
      "    priority       0.86      0.88      0.87      2152\n",
      "  spec_prior       0.00      0.00      0.00         6\n",
      "   recommend       0.88      0.86      0.87      2058\n",
      "   not_recom       0.90      0.54      0.67       156\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.71      0.64      0.66      6480\n",
      "weighted avg       0.88      0.88      0.87      6480\n",
      "\n",
      "accuracy:  0.8760802469135802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.92      0.92      2165\n",
      "    priority       0.84      0.91      0.87      2115\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.91      0.85      0.88      2028\n",
      "   not_recom       0.99      0.43      0.60       171\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.73      0.62      0.65      6480\n",
      "weighted avg       0.89      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8820987654320988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.93      0.91      2112\n",
      "    priority       0.86      0.88      0.87      2165\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.86      0.87      2053\n",
      "   not_recom       0.95      0.53      0.68       148\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.72      0.64      0.67      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8820987654320988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.92      0.91      2161\n",
      "    priority       0.84      0.91      0.87      2102\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.90      0.85      0.88      2033\n",
      "   not_recom       0.93      0.39      0.55       179\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.72      0.62      0.64      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8808641975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.93      0.91      2153\n",
      "    priority       0.87      0.88      0.87      2154\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.89      0.86      0.87      2010\n",
      "   not_recom       0.91      0.51      0.66       158\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.71      0.64      0.66      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8800925925925925\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.93      0.92      2120\n",
      "    priority       0.84      0.90      0.87      2113\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.84      0.87      2076\n",
      "   not_recom       0.93      0.39      0.55       169\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.71      0.61      0.64      6480\n",
      "weighted avg       0.88      0.88      0.87      6480\n",
      "\n",
      "accuracy:  0.8773148148148148\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.92      0.92      2133\n",
      "    priority       0.86      0.91      0.88      2155\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.87      0.88      2024\n",
      "   not_recom       0.98      0.49      0.65       166\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.73      0.64      0.67      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8901234567901235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.94      0.91      2140\n",
      "    priority       0.85      0.89      0.87      2112\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.90      0.84      0.87      2062\n",
      "   not_recom       0.90      0.47      0.62       161\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.71      0.63      0.65      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8774691358024691\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.92      0.91      2114\n",
      "    priority       0.85      0.89      0.87      2131\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.87      0.88      2059\n",
      "   not_recom       0.97      0.51      0.67       174\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.72      0.64      0.67      6480\n",
      "weighted avg       0.88      0.88      0.88      6480\n",
      "\n",
      "accuracy:  0.8817901234567901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.88      0.94      0.91      2159\n",
      "    priority       0.85      0.88      0.87      2136\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.90      0.83      0.86      2027\n",
      "   not_recom       0.93      0.54      0.69       153\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.71      0.64      0.66      6480\n",
      "weighted avg       0.88      0.88      0.87      6480\n",
      "\n",
      "accuracy:  0.8753086419753087\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.90      0.87      2094\n",
      "    priority       0.83      0.85      0.84      2191\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.87      0.82      0.84      2032\n",
      "   not_recom       0.88      0.38      0.53       158\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6480\n",
      "   macro avg       0.68      0.59      0.62      6480\n",
      "weighted avg       0.84      0.84      0.84      6480\n",
      "\n",
      "accuracy:  0.8438271604938271\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.87      0.88      0.87      2179\n",
      "    priority       0.81      0.88      0.85      2076\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.87      0.83      0.85      2054\n",
      "   not_recom       0.95      0.37      0.53       169\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.70      0.59      0.62      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.8495370370370371\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.86      0.90      0.88      2139\n",
      "    priority       0.84      0.86      0.85      2163\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.86      0.83      0.84      2009\n",
      "   not_recom       0.92      0.41      0.57       165\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.70      0.60      0.63      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.8523148148148149\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.89      0.86      2134\n",
      "    priority       0.81      0.89      0.85      2104\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.87      0.79      0.83      2077\n",
      "   not_recom       0.95      0.36      0.53       162\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6480\n",
      "   macro avg       0.70      0.58      0.61      6480\n",
      "weighted avg       0.84      0.84      0.84      6480\n",
      "\n",
      "accuracy:  0.8410493827160493\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.87      0.88      0.87      2156\n",
      "    priority       0.82      0.86      0.84      2150\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.84      0.84      0.84      1986\n",
      "   not_recom       0.89      0.26      0.40       183\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6480\n",
      "   macro avg       0.68      0.57      0.59      6480\n",
      "weighted avg       0.84      0.84      0.84      6480\n",
      "\n",
      "accuracy:  0.8416666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.89      0.87      2117\n",
      "    priority       0.83      0.89      0.86      2117\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.80      0.84      2100\n",
      "   not_recom       0.93      0.45      0.61       144\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.70      0.61      0.63      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.8507716049382716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.91      0.87      2097\n",
      "    priority       0.85      0.87      0.86      2158\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.88      0.83      0.85      2045\n",
      "   not_recom       0.94      0.36      0.52       176\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.70      0.59      0.62      6480\n",
      "weighted avg       0.86      0.86      0.85      6480\n",
      "\n",
      "accuracy:  0.8554012345679012\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.87      0.87      0.87      2176\n",
      "    priority       0.81      0.88      0.84      2109\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.85      0.82      0.83      2041\n",
      "   not_recom       0.93      0.43      0.59       151\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6480\n",
      "   macro avg       0.69      0.60      0.63      6480\n",
      "weighted avg       0.85      0.84      0.84      6480\n",
      "\n",
      "accuracy:  0.8445987654320988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.86      0.88      0.87      2165\n",
      "    priority       0.80      0.87      0.84      2124\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.86      0.81      0.84      2006\n",
      "   not_recom       0.96      0.27      0.42       183\n",
      "\n",
      "   micro avg       0.84      0.84      0.84      6480\n",
      "   macro avg       0.70      0.57      0.59      6480\n",
      "weighted avg       0.84      0.84      0.84      6480\n",
      "\n",
      "accuracy:  0.8405864197530865\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.89      0.86      2108\n",
      "    priority       0.84      0.88      0.86      2143\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.88      0.81      0.84      2080\n",
      "   not_recom       0.90      0.53      0.67       144\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.69      0.62      0.65      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.850462962962963\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.71      0.79      0.75      2154\n",
      "    priority       0.78      0.78      0.78      2162\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.79      0.75      0.77      2008\n",
      "   not_recom       0.76      0.15      0.24       151\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      6480\n",
      "   macro avg       0.61      0.49      0.51      6480\n",
      "weighted avg       0.76      0.76      0.75      6480\n",
      "\n",
      "accuracy:  0.7577160493827161\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.75      0.82      0.78      2119\n",
      "    priority       0.79      0.84      0.82      2105\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.83      0.77      0.80      2078\n",
      "   not_recom       0.94      0.18      0.30       176\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6480\n",
      "   macro avg       0.66      0.52      0.54      6480\n",
      "weighted avg       0.80      0.79      0.79      6480\n",
      "\n",
      "accuracy:  0.7908950617283951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.76      0.75      0.76      2196\n",
      "    priority       0.77      0.84      0.80      2111\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.79      0.78      0.78      2001\n",
      "   not_recom       0.89      0.15      0.25       169\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.64      0.50      0.52      6480\n",
      "weighted avg       0.77      0.77      0.77      6480\n",
      "\n",
      "accuracy:  0.7714506172839506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.68      0.86      0.76      2077\n",
      "    priority       0.82      0.76      0.79      2156\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.85      0.75      0.80      2085\n",
      "   not_recom       0.88      0.19      0.31       158\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.65      0.51      0.53      6480\n",
      "weighted avg       0.79      0.77      0.77      6480\n",
      "\n",
      "accuracy:  0.7723765432098766\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.73      0.83      0.78      2106\n",
      "    priority       0.77      0.82      0.79      2136\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.85      0.73      0.78      2065\n",
      "   not_recom       0.93      0.15      0.26       171\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.66      0.51      0.52      6480\n",
      "weighted avg       0.79      0.78      0.77      6480\n",
      "\n",
      "accuracy:  0.7765432098765432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.78      0.79      0.78      2167\n",
      "    priority       0.78      0.82      0.80      2131\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.80      0.79      0.80      2021\n",
      "   not_recom       0.87      0.22      0.35       156\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6480\n",
      "   macro avg       0.65      0.52      0.55      6480\n",
      "weighted avg       0.79      0.79      0.78      6480\n",
      "\n",
      "accuracy:  0.7868827160493828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.70      0.85      0.77      2065\n",
      "    priority       0.80      0.81      0.80      2132\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.85      0.71      0.77      2112\n",
      "   not_recom       0.83      0.11      0.20       167\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.63      0.50      0.51      6480\n",
      "weighted avg       0.78      0.77      0.77      6480\n",
      "\n",
      "accuracy:  0.7737654320987655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.78      0.77      0.78      2208\n",
      "    priority       0.77      0.82      0.79      2135\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.79      0.80      0.80      1974\n",
      "   not_recom       0.90      0.16      0.28       160\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.65      0.51      0.53      6480\n",
      "weighted avg       0.78      0.78      0.78      6480\n",
      "\n",
      "accuracy:  0.7807098765432099\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.71      0.82      0.76      2123\n",
      "    priority       0.82      0.76      0.79      2178\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.80      0.78      0.79      2006\n",
      "   not_recom       0.87      0.20      0.32       168\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.64      0.51      0.53      6480\n",
      "weighted avg       0.78      0.77      0.77      6480\n",
      "\n",
      "accuracy:  0.7716049382716049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.75      0.78      0.77      2150\n",
      "    priority       0.74      0.83      0.78      2089\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.83      0.75      0.79      2080\n",
      "   not_recom       0.92      0.14      0.24       159\n",
      "\n",
      "   micro avg       0.77      0.77      0.77      6480\n",
      "   macro avg       0.65      0.50      0.52      6480\n",
      "weighted avg       0.78      0.77      0.77      6480\n",
      "\n",
      "accuracy:  0.7711419753086419\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.69      0.72      0.70      2161\n",
      "    priority       0.75      0.80      0.77      2120\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.78      0.77      0.77      2007\n",
      "   not_recom       0.00      0.00      0.00       187\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.44      0.46      0.45      6480\n",
      "weighted avg       0.72      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.7376543209876543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.64      0.77      0.70      2112\n",
      "    priority       0.79      0.74      0.76      2147\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.79      0.73      0.76      2079\n",
      "   not_recom       0.88      0.15      0.26       140\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.62      0.48      0.50      6480\n",
      "weighted avg       0.74      0.73      0.73      6480\n",
      "\n",
      "accuracy:  0.7334876543209876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.63      0.76      0.69      2105\n",
      "    priority       0.79      0.74      0.76      2134\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.79      0.73      0.76      2078\n",
      "   not_recom       0.75      0.11      0.20       158\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.59      0.47      0.48      6480\n",
      "weighted avg       0.73      0.73      0.72      6480\n",
      "\n",
      "accuracy:  0.7256172839506173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.70      0.73      0.71      2168\n",
      "    priority       0.76      0.79      0.77      2133\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.78      0.78      0.78      2008\n",
      "   not_recom       0.60      0.02      0.03       169\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.57      0.46      0.46      6480\n",
      "weighted avg       0.74      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7462962962962963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.68      0.71      0.70      2161\n",
      "    priority       0.77      0.77      0.77      2126\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.75      0.78      0.76      2016\n",
      "   not_recom       0.00      0.00      0.00       174\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.44      0.45      0.45      6480\n",
      "weighted avg       0.71      0.73      0.72      6480\n",
      "\n",
      "accuracy:  0.7316358024691358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.67      0.76      0.71      2112\n",
      "    priority       0.78      0.78      0.78      2141\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.81      0.74      0.78      2070\n",
      "   not_recom       0.74      0.09      0.16       153\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.60      0.48      0.49      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7470679012345679\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.65      0.76      0.70      2126\n",
      "    priority       0.77      0.78      0.77      2123\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.80      0.71      0.75      2052\n",
      "   not_recom       0.43      0.02      0.03       175\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.53      0.45      0.45      6480\n",
      "weighted avg       0.73      0.73      0.72      6480\n",
      "\n",
      "accuracy:  0.7294753086419753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.69      0.71      0.70      2147\n",
      "    priority       0.78      0.78      0.78      2144\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.77      0.79      0.78      2034\n",
      "   not_recom       0.82      0.09      0.17       152\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.61      0.47      0.48      6480\n",
      "weighted avg       0.75      0.74      0.74      6480\n",
      "\n",
      "accuracy:  0.7444444444444445\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.65      0.78      0.71      2122\n",
      "    priority       0.81      0.74      0.77      2151\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.78      0.75      0.77      2043\n",
      "   not_recom       0.84      0.13      0.23       161\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.62      0.48      0.49      6480\n",
      "weighted avg       0.75      0.74      0.74      6480\n",
      "\n",
      "accuracy:  0.7390432098765433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.70      0.70      0.70      2151\n",
      "    priority       0.73      0.81      0.76      2116\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.79      0.77      0.78      2043\n",
      "   not_recom       0.20      0.01      0.01       166\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.48      0.46      0.45      6480\n",
      "weighted avg       0.72      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.7367283950617284\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80320987654321, 0.8909413580246912, 0.8943055555555555, 0.8953395061728395, 0.8964197530864197, 0.8803240740740741, 0.8470216049382715, 0.7753086419753087, 0.737145061728395]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_minkowski\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
