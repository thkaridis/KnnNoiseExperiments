{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "poker = pd.read_csv(\"poker.csv\")\n",
    "poker.columns.values\n",
    "poker = poker.head(200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "j = 0\n",
    "features = ['S1', ' C1', ' S2', ' C2', ' S3', ' C3', ' S4', ' C4', ' S5', ' C5']\n",
    "for index, m in poker.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        poker.at[index,features[j]] = i + 1\n",
    "        j += 3\n",
    "        i+=10\n",
    "        if j >= 9:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for index, m in poker.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        if index < 299994:\n",
    "            poker.at[index+5,:] = poker.loc[j,:]\n",
    "        else:\n",
    "            poker.at[index+1,:] = poker.loc[j,:]\n",
    "        j += 120\n",
    "        if j >= 299994:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "poker.to_csv('/home/valia/Documents/AppliedDataScience/pokerNoise.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>S1</th>\n",
       "      <th>C1</th>\n",
       "      <th>S2</th>\n",
       "      <th>C2</th>\n",
       "      <th>S3</th>\n",
       "      <th>C3</th>\n",
       "      <th>S4</th>\n",
       "      <th>C4</th>\n",
       "      <th>S5</th>\n",
       "      <th>C5</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>101</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>161</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299970</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299971</th>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299972</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299973</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299974</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299975</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299976</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299977</th>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299978</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299979</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>10</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299980</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>150091</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299981</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299982</th>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299983</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299984</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299985</th>\n",
       "      <td>150041</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299986</th>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299987</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299988</th>\n",
       "      <td>2</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299989</th>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299990</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299991</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299992</th>\n",
       "      <td>4</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299993</th>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299994</th>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299995</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>12</td>\n",
       "      <td>2</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299996</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299997</th>\n",
       "      <td>2</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299998</th>\n",
       "      <td>4</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299999</th>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>300000 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            S1   C1   S2   C2   S3   C3      S4   C4   S5   C5  Class\n",
       "0          101   12    3    2    3   11       4    5    2    5      1\n",
       "1            1    9    4    6    1    4       3    2    3    9      1\n",
       "2            1    4    3   13    2   13       2    1    3    6      1\n",
       "3            3   10    2    7    1    2       2   11    4    9      0\n",
       "4            1    3    4    5    3    4       1   12    4    6      0\n",
       "5          101   12    3    2    3   11       4    5    2    5      1\n",
       "6            3    2    4    9    3    7       4    3    4    5      0\n",
       "7            4    4    3   13    1    8       3    9    3   10      0\n",
       "8            1    9    3    8    4    4       1    7    3    5      0\n",
       "9            4    7    3   12    1   13       1    9    2    6      0\n",
       "10           2   12    1    3    2   11       2    7    4    8      0\n",
       "11           4    2    2    9    2    7       1    5    3   11      0\n",
       "12           1   13    2    6    1    6       2   11    3    5      1\n",
       "13           3    8    2    7    1    9       3    6    2    3      0\n",
       "14           2   10    1   11    1    9       3    1    1   13      0\n",
       "15           4    2    4   12    2   12       2    7    3   10      1\n",
       "16           4    5    2    2    4    9       1    5    4    1      1\n",
       "17           2    3    3    9    2    1       2    6    4   10      0\n",
       "18           1    7    2   11    4    1       2    9    3   13      0\n",
       "19           4   12    1    6    3    1       2    2    1    8      0\n",
       "20           2    5    3  111    3   13       4   13    3    8      1\n",
       "21           1    3    4    8    2    1       1   12    3    5      0\n",
       "22           2    8    4    6    1   12       2   13    1    8      1\n",
       "23           1    7    4   13    4    9       1    9    1   10      1\n",
       "24           2   13    3    3    2   11       2    6    1    4      0\n",
       "25         161    1    3   11    3   10       1    1    1   10      2\n",
       "26           2    5    3    7    2   12       3    3    2   11      0\n",
       "27           3    4    2    1    3   10       1    8    4    1      1\n",
       "28           4   11    2   13    4    4       3    8    4    1      0\n",
       "29           4   10    2    5    4    8       1    6    2   13      0\n",
       "...        ...  ...  ...  ...  ...  ...     ...  ...  ...  ...    ...\n",
       "299970       4    3    2    5    3    7       3    2    2    2      1\n",
       "299971       3   10    3    7    4    1       4   13    2    6      0\n",
       "299972       1    1    1    6    1   12       4    2    1   10      0\n",
       "299973       3    3    2   11    3   11       3    9    3    4      1\n",
       "299974       3    1    3   12    1   13       1   11    1    2      0\n",
       "299975       4    3    2   10    2    5       3    9    3    2      0\n",
       "299976       2    5    2    3    4    6       3   11    4    1      0\n",
       "299977       3   12    4   10    1    9       1    6    1    3      0\n",
       "299978       1    9    4    5    1    2       3    9    1    3      1\n",
       "299979       4    8    2    8    4   10       3    6    4    7      1\n",
       "299980       4    2    2    1    2    3  150091    7    3    6      0\n",
       "299981       2    5    2    6    1    3       3    2    3    6      1\n",
       "299982       2    7    3   10    4    4       2    3    2    8      0\n",
       "299983       1    2    4    9    2    7       4    1    1   13      0\n",
       "299984       2    5    3    6    1    5       3    2    1    3      1\n",
       "299985  150041   13    1   13    4    1       1    5    4    3      1\n",
       "299986       4    3    3    6    4    7       3    8    1    8      1\n",
       "299987       4   13    1    8    3    4       2   12    2    9      0\n",
       "299988       2   12    1   10    2   13       4   12    4    3      1\n",
       "299989       4    8    3    9    3   11       1    1    4    6      0\n",
       "299990       4   11    3    8    2    4       3    6    2    6      1\n",
       "299991       4    2    1   11    3   12       1    2    1   12      2\n",
       "299992       4   13    4    7    1   10       2    9    2   11      0\n",
       "299993       2    9    3    1    1   13       3   11    3    7      0\n",
       "299994       2   13    4   11    3    4       4    4    2    4      3\n",
       "299995       3    5    4   12    2    8       4    8    2   13      1\n",
       "299996       1   10    1    5    1    4       4    9    3   13      0\n",
       "299997       2    6    3    4    2    3       4    2    2   13      0\n",
       "299998       4    6    4    1    3    4       1    8    4   11      0\n",
       "299999       1    7    1   13    3    9       1    5    3    3      0\n",
       "\n",
       "[300000 rows x 11 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "poker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = poker.iloc[:,0:8]\n",
    "labels = poker.iloc[:,8].apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True) #5 fores me 2 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean and k tuning on 10% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.29      0.29     37573\n",
      "           3       0.30      0.29      0.29     37637\n",
      "           2       0.29      0.29      0.29     37394\n",
      "           4       0.29      0.30      0.30     37396\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.2939733333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.29      0.30      0.30     37456\n",
      "           3       0.29      0.29      0.29     37377\n",
      "           2       0.29      0.29      0.29     37404\n",
      "           4       0.29      0.29      0.29     37763\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.2937666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.29      0.29     37468\n",
      "           3       0.30      0.29      0.29     37708\n",
      "           2       0.29      0.29      0.29     37236\n",
      "           4       0.29      0.29      0.29     37588\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.29372\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.30      0.30     37561\n",
      "           3       0.29      0.29      0.29     37306\n",
      "           2       0.29      0.29      0.29     37562\n",
      "           4       0.29      0.29      0.29     37571\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.29370666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.30      0.30     37429\n",
      "           3       0.29      0.29      0.29     37319\n",
      "           2       0.29      0.29      0.29     37639\n",
      "           4       0.29      0.30      0.30     37613\n",
      "\n",
      "   micro avg       0.30      0.30      0.30    150000\n",
      "   macro avg       0.30      0.30      0.30    150000\n",
      "weighted avg       0.30      0.30      0.30    150000\n",
      "\n",
      "accuracy:  0.29506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.30      0.30     37600\n",
      "           3       0.29      0.29      0.29     37695\n",
      "           2       0.29      0.29      0.29     37159\n",
      "           4       0.29      0.29      0.29     37546\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.29357333333333335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.30      0.30     37402\n",
      "           3       0.29      0.29      0.29     37483\n",
      "           2       0.29      0.29      0.29     37421\n",
      "           4       0.29      0.29      0.29     37694\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.2925466666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.30      0.30     37627\n",
      "           3       0.30      0.29      0.29     37531\n",
      "           2       0.29      0.29      0.29     37377\n",
      "           4       0.29      0.30      0.30     37465\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.29428666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.30      0.30     37518\n",
      "           3       0.30      0.29      0.29     37506\n",
      "           2       0.29      0.29      0.29     37490\n",
      "           4       0.29      0.30      0.29     37486\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.2949733333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.30      0.30      0.30     37511\n",
      "           3       0.29      0.29      0.29     37508\n",
      "           2       0.29      0.29      0.29     37308\n",
      "           4       0.29      0.29      0.29     37673\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.29    150000\n",
      "weighted avg       0.29      0.29      0.29    150000\n",
      "\n",
      "accuracy:  0.2936533333333333\n",
      "mean accuracy 0.293926\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.37      0.32     37486\n",
      "           3       0.28      0.31      0.30     37505\n",
      "           2       0.28      0.25      0.27     37273\n",
      "           4       0.30      0.20      0.24     37736\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.28      0.28      0.28    150000\n",
      "weighted avg       0.29      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.28336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.37      0.32     37543\n",
      "           3       0.28      0.31      0.29     37509\n",
      "           2       0.29      0.25      0.27     37525\n",
      "           4       0.29      0.20      0.24     37423\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.29      0.28      0.28    150000\n",
      "weighted avg       0.29      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.28396666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.37      0.32     37496\n",
      "           3       0.28      0.30      0.29     37550\n",
      "           2       0.28      0.26      0.27     37282\n",
      "           4       0.30      0.20      0.24     37672\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.29      0.28      0.28    150000\n",
      "weighted avg       0.29      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.37      0.32     37533\n",
      "           3       0.28      0.31      0.29     37464\n",
      "           2       0.29      0.25      0.27     37516\n",
      "           4       0.29      0.20      0.24     37487\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.28      0.28      0.28    150000\n",
      "weighted avg       0.28      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.28330666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.37      0.32     37740\n",
      "           3       0.28      0.31      0.29     37448\n",
      "           2       0.28      0.26      0.27     37254\n",
      "           4       0.30      0.20      0.24     37558\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.28      0.28      0.28    150000\n",
      "weighted avg       0.28      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.28312\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.37      0.32     37289\n",
      "           3       0.28      0.31      0.30     37566\n",
      "           2       0.29      0.25      0.27     37544\n",
      "           4       0.29      0.20      0.24     37601\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.29      0.28      0.28    150000\n",
      "weighted avg       0.29      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.28386\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.38      0.32     37423\n",
      "           3       0.28      0.31      0.29     37546\n",
      "           2       0.29      0.26      0.27     37408\n",
      "           4       0.30      0.20      0.24     37623\n",
      "\n",
      "   micro avg       0.29      0.29      0.29    150000\n",
      "   macro avg       0.29      0.29      0.28    150000\n",
      "weighted avg       0.29      0.29      0.28    150000\n",
      "\n",
      "accuracy:  0.28530666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.36      0.31     37606\n",
      "           3       0.28      0.31      0.30     37468\n",
      "           2       0.29      0.26      0.27     37390\n",
      "           4       0.30      0.20      0.24     37536\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.29      0.28      0.28    150000\n",
      "weighted avg       0.29      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.28330666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.37      0.32     37547\n",
      "           3       0.28      0.31      0.29     37596\n",
      "           2       0.28      0.25      0.27     37524\n",
      "           4       0.29      0.20      0.24     37333\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.28      0.28      0.28    150000\n",
      "weighted avg       0.28      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.28180666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.28      0.37      0.32     37482\n",
      "           3       0.28      0.31      0.29     37418\n",
      "           2       0.28      0.26      0.27     37274\n",
      "           4       0.29      0.20      0.24     37826\n",
      "\n",
      "   micro avg       0.28      0.28      0.28    150000\n",
      "   macro avg       0.28      0.28      0.28    150000\n",
      "weighted avg       0.28      0.28      0.28    150000\n",
      "\n",
      "accuracy:  0.2829333333333333\n",
      "mean accuracy 0.2834966666666666\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.36      0.30     37447\n",
      "           3       0.27      0.28      0.27     37647\n",
      "           2       0.27      0.23      0.25     37304\n",
      "           4       0.27      0.20      0.23     37602\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.26802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.35      0.30     37582\n",
      "           3       0.26      0.28      0.27     37367\n",
      "           2       0.27      0.23      0.25     37494\n",
      "           4       0.27      0.20      0.23     37557\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.2654666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.35      0.30     37578\n",
      "           3       0.27      0.28      0.27     37541\n",
      "           2       0.27      0.23      0.25     37304\n",
      "           4       0.27      0.21      0.23     37577\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.26776666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.36      0.30     37451\n",
      "           3       0.27      0.28      0.27     37473\n",
      "           2       0.27      0.23      0.25     37494\n",
      "           4       0.27      0.20      0.23     37582\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.27    150000\n",
      "weighted avg       0.27      0.27      0.27    150000\n",
      "\n",
      "accuracy:  0.26814666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.35      0.30     37506\n",
      "           3       0.27      0.28      0.27     37596\n",
      "           2       0.26      0.23      0.25     37377\n",
      "           4       0.27      0.20      0.23     37521\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.26572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.35      0.30     37523\n",
      "           3       0.27      0.28      0.28     37418\n",
      "           2       0.27      0.23      0.25     37421\n",
      "           4       0.27      0.20      0.23     37638\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.2673066666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.36      0.31     37342\n",
      "           3       0.27      0.28      0.27     37614\n",
      "           2       0.27      0.23      0.25     37342\n",
      "           4       0.27      0.20      0.23     37702\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.26765333333333335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.35      0.30     37687\n",
      "           3       0.27      0.28      0.27     37400\n",
      "           2       0.27      0.23      0.25     37456\n",
      "           4       0.27      0.20      0.23     37457\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.26638666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.36      0.30     37424\n",
      "           3       0.26      0.28      0.27     37403\n",
      "           2       0.27      0.23      0.25     37406\n",
      "           4       0.27      0.20      0.23     37767\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.26570666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.35      0.30     37605\n",
      "           3       0.27      0.28      0.27     37611\n",
      "           2       0.27      0.23      0.25     37392\n",
      "           4       0.27      0.21      0.23     37392\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.26743333333333336\n",
      "mean accuracy 0.2669606666666667\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.34      0.30     37420\n",
      "           3       0.26      0.27      0.26     37498\n",
      "           2       0.27      0.23      0.25     37393\n",
      "           4       0.26      0.22      0.24     37689\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26413333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.33      0.29     37609\n",
      "           3       0.27      0.27      0.27     37516\n",
      "           2       0.27      0.24      0.25     37405\n",
      "           4       0.26      0.22      0.24     37470\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.26544666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.34      0.30     37256\n",
      "           3       0.27      0.27      0.27     37644\n",
      "           2       0.27      0.23      0.25     37507\n",
      "           4       0.27      0.22      0.24     37593\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.2655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.33      0.30     37773\n",
      "           3       0.27      0.28      0.27     37370\n",
      "           2       0.27      0.24      0.25     37291\n",
      "           4       0.27      0.22      0.24     37566\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.27    150000\n",
      "\n",
      "accuracy:  0.26671333333333336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.33      0.29     37559\n",
      "           3       0.26      0.27      0.27     37370\n",
      "           2       0.27      0.23      0.25     37534\n",
      "           4       0.27      0.22      0.24     37537\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.2653933333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.34      0.30     37470\n",
      "           3       0.27      0.27      0.27     37644\n",
      "           2       0.27      0.24      0.25     37264\n",
      "           4       0.27      0.23      0.25     37622\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.27    150000\n",
      "weighted avg       0.27      0.27      0.27    150000\n",
      "\n",
      "accuracy:  0.2676\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.34      0.30     37270\n",
      "           3       0.27      0.27      0.27     37600\n",
      "           2       0.27      0.23      0.25     37588\n",
      "           4       0.27      0.22      0.24     37542\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.27      0.26      0.26    150000\n",
      "weighted avg       0.27      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26471333333333336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.33      0.29     37759\n",
      "           3       0.27      0.28      0.27     37414\n",
      "           2       0.26      0.23      0.25     37210\n",
      "           4       0.27      0.22      0.24     37617\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26377333333333336\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.34      0.30     37513\n",
      "           3       0.27      0.28      0.27     37361\n",
      "           2       0.27      0.23      0.25     37495\n",
      "           4       0.27      0.22      0.24     37631\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.2657466666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.33      0.29     37516\n",
      "           3       0.26      0.26      0.26     37653\n",
      "           2       0.26      0.23      0.25     37303\n",
      "           4       0.27      0.22      0.24     37528\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2633066666666667\n",
      "mean accuracy 0.2652326666666667\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37454\n",
      "           3       0.26      0.27      0.27     37378\n",
      "           2       0.26      0.24      0.25     37516\n",
      "           4       0.27      0.23      0.24     37652\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26306\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37575\n",
      "           3       0.26      0.26      0.26     37636\n",
      "           2       0.26      0.24      0.25     37282\n",
      "           4       0.26      0.23      0.25     37507\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26378666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37404\n",
      "           3       0.27      0.26      0.27     37727\n",
      "           2       0.26      0.24      0.25     37303\n",
      "           4       0.27      0.23      0.24     37566\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26370666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37625\n",
      "           3       0.26      0.27      0.27     37287\n",
      "           2       0.26      0.23      0.25     37495\n",
      "           4       0.26      0.22      0.24     37593\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2623466666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.33      0.29     37354\n",
      "           3       0.26      0.26      0.26     37571\n",
      "           2       0.26      0.23      0.25     37483\n",
      "           4       0.27      0.23      0.24     37592\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26272\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37675\n",
      "           3       0.26      0.26      0.26     37443\n",
      "           2       0.26      0.24      0.25     37315\n",
      "           4       0.26      0.23      0.24     37567\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26325333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.33      0.29     37385\n",
      "           3       0.27      0.27      0.27     37569\n",
      "           2       0.26      0.24      0.25     37389\n",
      "           4       0.27      0.23      0.25     37657\n",
      "\n",
      "   micro avg       0.27      0.27      0.27    150000\n",
      "   macro avg       0.27      0.27      0.26    150000\n",
      "weighted avg       0.27      0.27      0.26    150000\n",
      "\n",
      "accuracy:  0.2658933333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37644\n",
      "           3       0.26      0.27      0.26     37445\n",
      "           2       0.27      0.24      0.25     37409\n",
      "           4       0.26      0.23      0.24     37502\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26393333333333335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37438\n",
      "           3       0.26      0.27      0.26     37505\n",
      "           2       0.26      0.24      0.25     37373\n",
      "           4       0.27      0.22      0.24     37684\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26209333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.27      0.32      0.29     37591\n",
      "           3       0.26      0.27      0.27     37509\n",
      "           2       0.26      0.24      0.25     37425\n",
      "           4       0.27      0.23      0.25     37475\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26468\n",
      "mean accuracy 0.26354733333333336\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37468\n",
      "           3       0.26      0.26      0.26     37484\n",
      "           2       0.26      0.23      0.25     37495\n",
      "           4       0.27      0.23      0.25     37553\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26271333333333335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37561\n",
      "           3       0.26      0.26      0.26     37530\n",
      "           2       0.26      0.24      0.25     37303\n",
      "           4       0.26      0.23      0.25     37606\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26081333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37594\n",
      "           3       0.26      0.26      0.26     37512\n",
      "           2       0.26      0.24      0.25     37253\n",
      "           4       0.27      0.23      0.25     37641\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26194666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37435\n",
      "           3       0.26      0.26      0.26     37502\n",
      "           2       0.26      0.23      0.24     37545\n",
      "           4       0.27      0.23      0.25     37518\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26156666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37617\n",
      "           3       0.26      0.27      0.27     37272\n",
      "           2       0.26      0.24      0.25     37370\n",
      "           4       0.27      0.23      0.25     37741\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26416\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37412\n",
      "           3       0.27      0.26      0.26     37742\n",
      "           2       0.26      0.24      0.25     37428\n",
      "           4       0.27      0.24      0.25     37418\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2646733333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37568\n",
      "           3       0.26      0.26      0.26     37577\n",
      "           2       0.26      0.23      0.25     37442\n",
      "           4       0.27      0.24      0.25     37413\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2613666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37461\n",
      "           3       0.26      0.26      0.26     37437\n",
      "           2       0.26      0.24      0.25     37356\n",
      "           4       0.27      0.23      0.25     37746\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26203333333333334\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37661\n",
      "           3       0.26      0.26      0.26     37581\n",
      "           2       0.26      0.25      0.25     37054\n",
      "           4       0.27      0.23      0.25     37704\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26392\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.32      0.29     37368\n",
      "           3       0.26      0.26      0.26     37433\n",
      "           2       0.26      0.23      0.24     37744\n",
      "           4       0.26      0.23      0.25     37455\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.26182\n",
      "mean accuracy 0.26250133333333336\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37380\n",
      "           3       0.26      0.25      0.25     37511\n",
      "           2       0.26      0.23      0.24     37444\n",
      "           4       0.26      0.24      0.25     37665\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25732666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37649\n",
      "           3       0.26      0.26      0.26     37503\n",
      "           2       0.26      0.23      0.24     37354\n",
      "           4       0.26      0.24      0.25     37494\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25812\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37378\n",
      "           3       0.26      0.25      0.25     37723\n",
      "           2       0.25      0.24      0.25     37295\n",
      "           4       0.26      0.24      0.25     37604\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25884\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37651\n",
      "           3       0.26      0.27      0.26     37291\n",
      "           2       0.26      0.23      0.24     37503\n",
      "           4       0.26      0.24      0.25     37555\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2591133333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37685\n",
      "           3       0.26      0.25      0.26     37612\n",
      "           2       0.26      0.24      0.25     37330\n",
      "           4       0.26      0.24      0.25     37373\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25904666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37344\n",
      "           3       0.26      0.25      0.25     37402\n",
      "           2       0.26      0.23      0.24     37468\n",
      "           4       0.26      0.23      0.24     37786\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37560\n",
      "           3       0.26      0.25      0.26     37548\n",
      "           2       0.26      0.23      0.24     37593\n",
      "           4       0.26      0.25      0.25     37299\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25971333333333335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37469\n",
      "           3       0.26      0.25      0.26     37466\n",
      "           2       0.26      0.24      0.25     37205\n",
      "           4       0.26      0.23      0.25     37860\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25932666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37406\n",
      "           3       0.26      0.24      0.25     37612\n",
      "           2       0.26      0.24      0.25     37275\n",
      "           4       0.26      0.24      0.25     37707\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25886\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37623\n",
      "           3       0.26      0.26      0.26     37402\n",
      "           2       0.26      0.23      0.24     37523\n",
      "           4       0.26      0.25      0.25     37452\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25782666666666665\n",
      "mean accuracy 0.25852533333333333\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37620\n",
      "           3       0.26      0.25      0.25     37462\n",
      "           2       0.26      0.23      0.24     37339\n",
      "           4       0.25      0.26      0.26     37579\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25675333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37409\n",
      "           3       0.26      0.24      0.25     37552\n",
      "           2       0.25      0.23      0.24     37459\n",
      "           4       0.26      0.25      0.26     37580\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2560866666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.29      0.27     37667\n",
      "           3       0.26      0.25      0.25     37423\n",
      "           2       0.25      0.23      0.24     37352\n",
      "           4       0.26      0.25      0.26     37558\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.30      0.28     37362\n",
      "           3       0.26      0.24      0.25     37591\n",
      "           2       0.26      0.23      0.24     37446\n",
      "           4       0.26      0.26      0.26     37601\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.31      0.28     37301\n",
      "           3       0.26      0.23      0.24     37634\n",
      "           2       0.25      0.22      0.24     37611\n",
      "           4       0.26      0.26      0.26     37454\n",
      "\n",
      "   micro avg       0.25      0.25      0.25    150000\n",
      "   macro avg       0.25      0.25      0.25    150000\n",
      "weighted avg       0.25      0.25      0.25    150000\n",
      "\n",
      "accuracy:  0.25484666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37728\n",
      "           3       0.26      0.25      0.26     37380\n",
      "           2       0.25      0.24      0.24     37187\n",
      "           4       0.26      0.25      0.25     37705\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2574133333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37574\n",
      "           3       0.25      0.24      0.25     37364\n",
      "           2       0.25      0.22      0.24     37600\n",
      "           4       0.25      0.26      0.26     37462\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.25      0.26      0.25    150000\n",
      "weighted avg       0.25      0.26      0.25    150000\n",
      "\n",
      "accuracy:  0.25512666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37455\n",
      "           3       0.26      0.24      0.25     37650\n",
      "           2       0.25      0.23      0.24     37198\n",
      "           4       0.26      0.25      0.26     37697\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25678666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37432\n",
      "           3       0.26      0.25      0.25     37274\n",
      "           2       0.26      0.23      0.24     37503\n",
      "           4       0.26      0.25      0.25     37791\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2569666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37597\n",
      "           3       0.26      0.23      0.25     37740\n",
      "           2       0.25      0.24      0.24     37295\n",
      "           4       0.26      0.26      0.26     37368\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2561\n",
      "mean accuracy 0.25624399999999997\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37414\n",
      "           3       0.25      0.24      0.25     37495\n",
      "           2       0.25      0.23      0.24     37481\n",
      "           4       0.26      0.26      0.26     37610\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.25    150000\n",
      "weighted avg       0.26      0.26      0.25    150000\n",
      "\n",
      "accuracy:  0.25549333333333335\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37615\n",
      "           3       0.26      0.23      0.25     37519\n",
      "           2       0.25      0.23      0.24     37317\n",
      "           4       0.25      0.27      0.26     37549\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2562\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.30      0.28     37473\n",
      "           3       0.25      0.24      0.25     37473\n",
      "           2       0.25      0.22      0.23     37514\n",
      "           4       0.26      0.27      0.26     37540\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.2561333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37556\n",
      "           3       0.25      0.23      0.24     37541\n",
      "           2       0.25      0.23      0.24     37284\n",
      "           4       0.26      0.26      0.26     37619\n",
      "\n",
      "   micro avg       0.25      0.25      0.25    150000\n",
      "   macro avg       0.25      0.25      0.25    150000\n",
      "weighted avg       0.25      0.25      0.25    150000\n",
      "\n",
      "accuracy:  0.2545866666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37688\n",
      "           3       0.25      0.24      0.25     37266\n",
      "           2       0.26      0.22      0.24     37622\n",
      "           4       0.26      0.27      0.27     37424\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25610666666666665\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.31      0.28     37341\n",
      "           3       0.26      0.23      0.24     37748\n",
      "           2       0.25      0.24      0.24     37176\n",
      "           4       0.26      0.25      0.25     37735\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.25    150000\n",
      "weighted avg       0.26      0.26      0.25    150000\n",
      "\n",
      "accuracy:  0.25528666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37694\n",
      "           3       0.26      0.24      0.25     37495\n",
      "           2       0.25      0.23      0.24     37310\n",
      "           4       0.26      0.26      0.26     37501\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25626666666666664\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.31      0.28     37335\n",
      "           3       0.26      0.23      0.25     37519\n",
      "           2       0.25      0.22      0.24     37488\n",
      "           4       0.26      0.26      0.26     37658\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.26    150000\n",
      "weighted avg       0.26      0.26      0.26    150000\n",
      "\n",
      "accuracy:  0.25694\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.26      0.29      0.27     37707\n",
      "           3       0.25      0.23      0.24     37532\n",
      "           2       0.25      0.23      0.24     37316\n",
      "           4       0.26      0.27      0.26     37445\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.25    150000\n",
      "weighted avg       0.26      0.26      0.25    150000\n",
      "\n",
      "accuracy:  0.2551466666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.25      0.30      0.28     37322\n",
      "           3       0.26      0.24      0.24     37482\n",
      "           2       0.26      0.22      0.24     37482\n",
      "           4       0.26      0.26      0.26     37714\n",
      "\n",
      "   micro avg       0.26      0.26      0.26    150000\n",
      "   macro avg       0.26      0.26      0.25    150000\n",
      "weighted avg       0.26      0.26      0.25    150000\n",
      "\n",
      "accuracy:  0.2556733333333333\n",
      "mean accuracy 0.25578333333333336\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(poker):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.293926, 0.2834966666666666, 0.2669606666666667, 0.2652326666666667, 0.26354733333333336, 0.26250133333333336, 0.25852533333333333, 0.25624399999999997, 0.25578333333333336]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-fcb2653aa17c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mprint\u001b[0m \u001b[0mmean_accuracy_model_euclidean\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m150\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmean_accuracy_model_euclidean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "print mean_accuracy_model_euclidean\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski and k tuning on 10% dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_minkowski = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mean_accuracy_model_minkowski\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
