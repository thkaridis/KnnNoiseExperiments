{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sex', ' MaritalStatus', ' Age', ' Education', ' Occupation',\n",
       "       ' YearsInSf', ' DualIncome', ' HouseholdMembers', ' Under18',\n",
       "       ' HouseholdStatus', ' TypeOfHome', ' EthnicClass', ' Language',\n",
       "       'Unnamed: 13'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing = pd.read_csv(\"marketing_numerical.csv\")\n",
    "(marketing.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Cap-shape</th>\n",
       "      <th>Cap-surface</th>\n",
       "      <th>Cap-color</th>\n",
       "      <th>Bruises</th>\n",
       "      <th>Odor</th>\n",
       "      <th>Gill-attachment</th>\n",
       "      <th>Gill-spacing</th>\n",
       "      <th>Gill-size</th>\n",
       "      <th>Gill-color</th>\n",
       "      <th>Stalk-shape</th>\n",
       "      <th>...</th>\n",
       "      <th>Stalk-color-above-ring</th>\n",
       "      <th>Stalk-color-below-ring</th>\n",
       "      <th>Veil-type</th>\n",
       "      <th>Veil-color</th>\n",
       "      <th>Ring-number</th>\n",
       "      <th>Ring-type</th>\n",
       "      <th>Spore-print-color</th>\n",
       "      <th>Population</th>\n",
       "      <th>Habitat</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5614</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5615</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5616</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5617</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5618</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5619</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5620</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5621</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5622</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5623</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5624</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5625</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5626</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5627</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5628</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5629</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5630</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5631</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5632</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5633</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5634</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5635</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5636</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5637</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5638</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5639</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5640</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5641</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>e</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5642</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5643</th>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>3</td>\n",
       "      <td>p</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5644 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Cap-shape   Cap-surface   Cap-color   Bruises   Odor   Gill-attachment  \\\n",
       "0             0             0           0         1      0                 0   \n",
       "1             0             0           1         1      1                 0   \n",
       "2             1             0           2         1      2                 0   \n",
       "3             0             1           2         1      0                 0   \n",
       "4             0             0           3         0      3                 0   \n",
       "5             0             1           1         1      1                 0   \n",
       "6             1             0           2         1      1                 0   \n",
       "7             1             1           2         1      2                 0   \n",
       "8             0             1           2         1      0                 0   \n",
       "9             1             0           1         1      1                 0   \n",
       "10            0             1           1         1      2                 0   \n",
       "11            0             1           1         1      1                 0   \n",
       "12            1             0           1         1      1                 0   \n",
       "13            0             1           2         1      0                 0   \n",
       "14            0             2           0         0      3                 0   \n",
       "15            2             2           3         0      3                 0   \n",
       "16            3             2           2         0      3                 0   \n",
       "17            0             0           0         1      0                 0   \n",
       "18            0             1           2         1      0                 0   \n",
       "19            0             0           0         1      0                 0   \n",
       "20            1             0           1         1      1                 0   \n",
       "21            0             1           0         1      0                 0   \n",
       "22            1             1           1         1      2                 0   \n",
       "23            1             1           2         1      1                 0   \n",
       "24            1             0           2         1      2                 0   \n",
       "25            3             0           2         1      0                 0   \n",
       "26            0             1           1         1      1                 0   \n",
       "27            0             1           2         1      2                 0   \n",
       "28            3             2           0         0      3                 0   \n",
       "29            0             0           1         1      1                 0   \n",
       "...         ...           ...         ...       ...    ...               ...   \n",
       "5614          3             0           3         1      3                 0   \n",
       "5615          0             0           5         1      3                 0   \n",
       "5616          5             1           1         0      3                 0   \n",
       "5617          4             1           4         0      8                 1   \n",
       "5618          0             1           8         0      8                 0   \n",
       "5619          1             1           0         0      3                 0   \n",
       "5620          0             1           0         1      3                 0   \n",
       "5621          0             1           0         0      8                 1   \n",
       "5622          4             1           1         0      3                 0   \n",
       "5623          4             0           0         0      3                 0   \n",
       "5624          3             1           8         0      8                 1   \n",
       "5625          0             1           0         0      8                 0   \n",
       "5626          4             1           4         0      8                 0   \n",
       "5627          0             1           0         0      3                 0   \n",
       "5628          3             1           3         1      3                 0   \n",
       "5629          3             1           0         0      8                 1   \n",
       "5630          3             1           5         1      3                 0   \n",
       "5631          3             0           0         0      3                 0   \n",
       "5632          3             1           3         1      3                 0   \n",
       "5633          4             1           4         0      8                 1   \n",
       "5634          0             1           8         1      3                 0   \n",
       "5635          3             0           8         1      3                 0   \n",
       "5636          0             1           0         1      3                 0   \n",
       "5637          4             1           8         0      8                 1   \n",
       "5638          3             0           0         0      3                 0   \n",
       "5639          1             1           0         0      3                 0   \n",
       "5640          0             1           0         0      3                 0   \n",
       "5641          0             1           3         1      3                 0   \n",
       "5642          0             1           8         0      8                 0   \n",
       "5643          3             1           8         0      8                 1   \n",
       "\n",
       "       Gill-spacing   Gill-size   Gill-color   Stalk-shape  ...    \\\n",
       "0                 0           0            0             0  ...     \n",
       "1                 0           1            0             0  ...     \n",
       "2                 0           1            1             0  ...     \n",
       "3                 0           0            1             0  ...     \n",
       "4                 1           1            0             1  ...     \n",
       "5                 0           1            1             0  ...     \n",
       "6                 0           1            9             0  ...     \n",
       "7                 0           1            1             0  ...     \n",
       "8                 0           0           10             0  ...     \n",
       "9                 0           1            9             0  ...     \n",
       "10                0           1            9             0  ...     \n",
       "11                0           1            1             0  ...     \n",
       "12                0           1            2             0  ...     \n",
       "13                0           0            0             0  ...     \n",
       "14                1           1            1             1  ...     \n",
       "15                0           0            0             0  ...     \n",
       "16                1           1            0             1  ...     \n",
       "17                0           0            1             0  ...     \n",
       "18                0           0            1             0  ...     \n",
       "19                0           0            0             0  ...     \n",
       "20                0           1            0             0  ...     \n",
       "21                0           0            1             0  ...     \n",
       "22                0           1            0             0  ...     \n",
       "23                0           1            2             0  ...     \n",
       "24                0           1            9             0  ...     \n",
       "25                0           0            1             0  ...     \n",
       "26                0           1            1             0  ...     \n",
       "27                0           1            2             0  ...     \n",
       "28                0           0            0             0  ...     \n",
       "29                1           0            1             1  ...     \n",
       "...             ...         ...          ...           ...  ...     \n",
       "5614              0           1            2             0  ...     \n",
       "5615              0           1            2             0  ...     \n",
       "5616              1           0            2             0  ...     \n",
       "5617              0           1            7             0  ...     \n",
       "5618              0           1            2             0  ...     \n",
       "5619              0           1            2             0  ...     \n",
       "5620              0           1            2             0  ...     \n",
       "5621              0           1            7             0  ...     \n",
       "5622              1           0            2             0  ...     \n",
       "5623              0           1            2             0  ...     \n",
       "5624              0           1            2             0  ...     \n",
       "5625              0           1            2             0  ...     \n",
       "5626              0           1            2             0  ...     \n",
       "5627              0           1            2             0  ...     \n",
       "5628              0           1            2             0  ...     \n",
       "5629              0           1            7             0  ...     \n",
       "5630              0           1            2             0  ...     \n",
       "5631              0           1            2             0  ...     \n",
       "5632              0           1            2             0  ...     \n",
       "5633              0           1            2             0  ...     \n",
       "5634              0           1            2             0  ...     \n",
       "5635              0           1            2             0  ...     \n",
       "5636              0           1            2             0  ...     \n",
       "5637              0           1            7             0  ...     \n",
       "5638              0           1            2             0  ...     \n",
       "5639              0           1            2             0  ...     \n",
       "5640              0           1            2             0  ...     \n",
       "5641              0           1            2             0  ...     \n",
       "5642              0           1            7             0  ...     \n",
       "5643              0           1            7             0  ...     \n",
       "\n",
       "       Stalk-color-above-ring   Stalk-color-below-ring   Veil-type  \\\n",
       "0                           0                        0           1   \n",
       "1                           0                        0           1   \n",
       "2                           0                        0           1   \n",
       "3                           0                        0           1   \n",
       "4                           0                        0           1   \n",
       "5                           0                        0           1   \n",
       "6                           0                        0           1   \n",
       "7                           0                        0           1   \n",
       "8                           0                        0           1   \n",
       "9                           0                        0           1   \n",
       "10                          0                        0           1   \n",
       "11                          0                        0           1   \n",
       "12                          0                        0           1   \n",
       "13                          0                        0           1   \n",
       "14                          0                        0           1   \n",
       "15                          0                        0           1   \n",
       "16                          0                        0           1   \n",
       "17                          0                        0           1   \n",
       "18                          0                        0           1   \n",
       "19                          0                        0           1   \n",
       "20                          0                        0           1   \n",
       "21                          0                        0           1   \n",
       "22                          0                        0           1   \n",
       "23                          0                        0           1   \n",
       "24                          0                        0           1   \n",
       "25                          0                        0           1   \n",
       "26                          0                        0           1   \n",
       "27                          0                        0           1   \n",
       "28                          0                        0           1   \n",
       "29                          0                        0           1   \n",
       "...                       ...                      ...         ...   \n",
       "5614                        0                        0           1   \n",
       "5615                        0                        0           1   \n",
       "5616                        8                        8           1   \n",
       "5617                        7                        7           1   \n",
       "5618                        7                        7           1   \n",
       "5619                        3                        3           1   \n",
       "5620                        0                        0           1   \n",
       "5621                        7                        7           1   \n",
       "5622                        8                        8           1   \n",
       "5623                        3                        3           1   \n",
       "5624                        7                        7           1   \n",
       "5625                        7                        7           1   \n",
       "5626                        7                        7           1   \n",
       "5627                        3                        3           1   \n",
       "5628                        0                        0           1   \n",
       "5629                        7                        7           1   \n",
       "5630                        0                        0           1   \n",
       "5631                        3                        3           1   \n",
       "5632                        0                        0           1   \n",
       "5633                        7                        7           1   \n",
       "5634                        0                        0           1   \n",
       "5635                        0                        0           1   \n",
       "5636                        0                        0           1   \n",
       "5637                        7                        7           1   \n",
       "5638                        3                        3           1   \n",
       "5639                        3                        3           1   \n",
       "5640                        3                        3           1   \n",
       "5641                        0                        0           1   \n",
       "5642                        7                        7           1   \n",
       "5643                        7                        7           1   \n",
       "\n",
       "       Veil-color   Ring-number   Ring-type   Spore-print-color   Population  \\\n",
       "0               0             2           0                   0            0   \n",
       "1               0             2           0                   1            1   \n",
       "2               0             2           0                   1            1   \n",
       "3               0             2           0                   0            0   \n",
       "4               0             2           1                   1            2   \n",
       "5               0             2           0                   0            1   \n",
       "6               0             2           0                   0            1   \n",
       "7               0             2           0                   1            0   \n",
       "8               0             2           0                   0            3   \n",
       "9               0             2           0                   0            0   \n",
       "10              0             2           0                   1            1   \n",
       "11              0             2           0                   0            0   \n",
       "12              0             2           0                   1            0   \n",
       "13              0             2           0                   1            3   \n",
       "14              0             2           1                   0            2   \n",
       "15              0             2           0                   1            4   \n",
       "16              0             2           1                   1            2   \n",
       "17              0             2           0                   0            0   \n",
       "18              0             2           0                   1            0   \n",
       "19              0             2           0                   1            0   \n",
       "20              0             2           0                   1            0   \n",
       "21              0             2           0                   1            3   \n",
       "22              0             2           0                   1            0   \n",
       "23              0             2           0                   1            1   \n",
       "24              0             2           0                   0            0   \n",
       "25              0             2           0                   1            3   \n",
       "26              0             2           0                   1            1   \n",
       "27              0             2           0                   1            1   \n",
       "28              0             2           0                   0            4   \n",
       "29              0             2           0                   1            3   \n",
       "...           ...           ...         ...                 ...          ...   \n",
       "5614            0             0           0                   2            4   \n",
       "5615            0             0           0                   2            3   \n",
       "5616            3             2           1                   2            5   \n",
       "5617            0             1           4                   2            5   \n",
       "5618            0             1           4                   2            5   \n",
       "5619            0             0           0                   2            4   \n",
       "5620            0             0           0                   2            3   \n",
       "5621            0             1           4                   2            5   \n",
       "5622            3             2           1                   2            5   \n",
       "5623            0             0           0                   2            4   \n",
       "5624            0             1           4                   2            5   \n",
       "5625            0             1           4                   2            5   \n",
       "5626            0             1           4                   2            5   \n",
       "5627            0             0           0                   2            4   \n",
       "5628            0             0           0                   2            3   \n",
       "5629            0             1           4                   2            5   \n",
       "5630            0             0           0                   2            3   \n",
       "5631            0             0           0                   2            4   \n",
       "5632            0             0           0                   2            4   \n",
       "5633            0             1           4                   2            5   \n",
       "5634            0             0           0                   2            3   \n",
       "5635            0             0           0                   2            3   \n",
       "5636            0             0           0                   2            4   \n",
       "5637            0             1           4                   2            5   \n",
       "5638            0             0           0                   2            4   \n",
       "5639            0             0           0                   2            4   \n",
       "5640            0             0           0                   2            4   \n",
       "5641            0             0           0                   2            4   \n",
       "5642            0             1           4                   2            5   \n",
       "5643            0             1           4                   2            5   \n",
       "\n",
       "       Habitat  Class  \n",
       "0            0      p  \n",
       "1            1      e  \n",
       "2            2      e  \n",
       "3            0      p  \n",
       "4            1      e  \n",
       "5            1      e  \n",
       "6            2      e  \n",
       "7            2      e  \n",
       "8            1      p  \n",
       "9            2      e  \n",
       "10           1      e  \n",
       "11           2      e  \n",
       "12           1      e  \n",
       "13           0      p  \n",
       "14           1      e  \n",
       "15           0      e  \n",
       "16           1      e  \n",
       "17           1      p  \n",
       "18           0      p  \n",
       "19           0      p  \n",
       "20           2      e  \n",
       "21           1      p  \n",
       "22           2      e  \n",
       "23           2      e  \n",
       "24           2      e  \n",
       "25           1      p  \n",
       "26           2      e  \n",
       "27           2      e  \n",
       "28           0      e  \n",
       "29           3      e  \n",
       "...        ...    ...  \n",
       "5614         4      e  \n",
       "5615         4      e  \n",
       "5616         6      p  \n",
       "5617         3      p  \n",
       "5618         3      p  \n",
       "5619         3      e  \n",
       "5620         4      e  \n",
       "5621         3      p  \n",
       "5622         6      p  \n",
       "5623         3      e  \n",
       "5624         3      p  \n",
       "5625         3      p  \n",
       "5626         3      p  \n",
       "5627         3      e  \n",
       "5628         4      e  \n",
       "5629         3      p  \n",
       "5630         4      e  \n",
       "5631         4      e  \n",
       "5632         4      e  \n",
       "5633         3      p  \n",
       "5634         4      e  \n",
       "5635         4      e  \n",
       "5636         4      e  \n",
       "5637         3      p  \n",
       "5638         3      e  \n",
       "5639         4      e  \n",
       "5640         4      e  \n",
       "5641         4      e  \n",
       "5642         3      p  \n",
       "5643         3      p  \n",
       "\n",
       "[5644 rows x 23 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mushroom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "j = 0\n",
    "features = ['Cap-shape', ' Cap-surface', ' Cap-color', ' Bruises', ' Odor',\n",
    "       ' Gill-attachment', ' Gill-spacing', ' Gill-size', ' Gill-color',\n",
    "       ' Stalk-shape', ' Stalk-root', ' Stalk-surface-above-ring',\n",
    "       ' Stalk-surface-below-ring', ' Stalk-color-above-ring',\n",
    "       ' Stalk-color-below-ring', ' Veil-type', ' Veil-color',\n",
    "       ' Ring-number', ' Ring-type', ' Spore-print-color', ' Population',\n",
    "       ' Habitat']\n",
    "for index, m in mushroom.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        mushroom.at[index+2,features[j]] = i + 1\n",
    "        j += 3\n",
    "        i += 10\n",
    "        if j >= 22:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for index, m in mushroom.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        if index < 5637:\n",
    "            mushroom.at[index+5,:] = mushroom.loc[j,:]\n",
    "        else:\n",
    "            mushroom.at[index+1,:] = mushroom.loc[j,:]\n",
    "        j += 120\n",
    "        if j >= 5637:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "mushroom.to_csv('/home/valia/Documents/AppliedDataScience/mushroomNoise.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['p', 'e'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = mushroom.iloc[:,0:22]\n",
    "labels = mushroom.iloc[:,22]\n",
    "labels.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True) #5 times with 2 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean and k tuning on 10% noise datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1755\n",
      "           e       0.99      0.99      0.99      1067\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9904323175053154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1749\n",
      "           e       0.99      0.99      0.99      1073\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9911410347271439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1742\n",
      "           e       0.99      0.99      0.99      1080\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9900779588944011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1762\n",
      "           e       0.98      0.99      0.98      1060\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9886605244507441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       1.00      0.99      0.99      1758\n",
      "           e       0.98      0.99      0.99      1064\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9897236002834869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1746\n",
      "           e       0.99      0.98      0.98      1076\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9883061658398299\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1802\n",
      "           e       0.99      0.98      0.99      1020\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9893692416725727\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1702\n",
      "           e       0.99      0.99      0.99      1120\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9911410347271439\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       1.00      0.99      0.99      1782\n",
      "           e       0.98      0.99      0.99      1040\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9897236002834869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1722\n",
      "           e       0.99      0.99      0.99      1100\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9893692416725727\n",
      "mean accuracy 0.9897944720056697\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1693\n",
      "           e       0.99      0.98      0.98      1129\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9875974486180015\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1811\n",
      "           e       0.99      0.99      0.99      1011\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9904323175053154\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1765\n",
      "           e       0.99      0.99      0.99      1057\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9897236002834869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1739\n",
      "           e       0.99      0.98      0.99      1083\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9886605244507441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      1.00      0.99      1770\n",
      "           e       0.99      0.98      0.99      1052\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9918497519489724\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1734\n",
      "           e       0.98      0.99      0.99      1088\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9886605244507441\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1762\n",
      "           e       0.98      0.99      0.99      1060\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9900779588944011\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      1.00      0.99      1742\n",
      "           e       0.99      0.97      0.98      1080\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.98      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9865343727852587\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1763\n",
      "           e       0.99      0.98      0.98      1059\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9879518072289156\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1741\n",
      "           e       0.99      0.99      0.99      1081\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9918497519489724\n",
      "mean accuracy 0.9893338058114812\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model2 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.99      1733\n",
      "           e       1.00      0.96      0.98      1089\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9815733522324592\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1771\n",
      "           e       0.99      0.97      0.98      1051\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9858256555634302\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1724\n",
      "           e       0.99      0.98      0.98      1098\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9872430900070872\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1780\n",
      "           e       0.99      0.95      0.97      1042\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9801559177888023\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      1.00      0.99      1714\n",
      "           e       1.00      0.97      0.98      1108\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9847625797306875\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      1.00      0.99      1790\n",
      "           e       0.99      0.96      0.98      1032\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9836995038979447\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1720\n",
      "           e       0.99      0.95      0.97      1102\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9787384833451452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      1.00      0.99      1784\n",
      "           e       0.99      0.96      0.98      1038\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9844082211197732\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1760\n",
      "           e       0.99      0.98      0.99      1062\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9890148830616584\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1744\n",
      "           e       0.99      0.96      0.97      1078\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9805102763997166\n",
      "mean accuracy 0.9835931963146705\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model3 = sum(acc)/10 \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1762\n",
      "           e       0.99      0.95      0.97      1060\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9776754075124026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1742\n",
      "           e       1.00      0.95      0.97      1080\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9794472005669738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1757\n",
      "           e       0.99      0.95      0.97      1065\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9787384833451452\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1747\n",
      "           e       0.99      0.95      0.97      1075\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9759036144578314\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1746\n",
      "           e       1.00      0.93      0.96      1076\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.98      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.97413182140326\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.98      0.98      1758\n",
      "           e       0.97      0.97      0.97      1064\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9794472005669738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1732\n",
      "           e       0.99      0.94      0.97      1090\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9755492558469171\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.98      1772\n",
      "           e       0.99      0.96      0.97      1050\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9794472005669738\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1736\n",
      "           e       0.99      0.94      0.97      1086\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9737774627923459\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.98      1768\n",
      "           e       0.99      0.96      0.97      1054\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.979801559177888\n",
      "mean accuracy 0.9773919206236712\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model4 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1780\n",
      "           e       1.00      0.94      0.97      1042\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9780297661233168\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1724\n",
      "           e       0.99      0.92      0.95      1098\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.96      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9659815733522324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1765\n",
      "           e       0.99      0.93      0.96      1057\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9709425939050319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1739\n",
      "           e       0.99      0.95      0.97      1083\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9766123316796598\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.98      0.98      1786\n",
      "           e       0.97      0.96      0.97      1036\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.976966690290574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      1.00      0.97      1718\n",
      "           e       1.00      0.92      0.96      1104\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9673990077958894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      1.00      0.97      1720\n",
      "           e       1.00      0.92      0.96      1102\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9673990077958894\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1784\n",
      "           e       0.98      0.96      0.97      1038\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.976966690290574\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1748\n",
      "           e       0.99      0.93      0.96      1074\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9723600283486888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1756\n",
      "           e       0.99      0.94      0.96      1066\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9730687455705174\n",
      "mean accuracy 0.9725726435152373\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model5 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1770\n",
      "           e       0.98      0.94      0.96      1052\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9709425939050319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1734\n",
      "           e       0.99      0.92      0.95      1088\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.97      0.96      0.96      2822\n",
      "weighted avg       0.97      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.9649184975194898\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1748\n",
      "           e       0.98      0.94      0.96      1074\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9709425939050319\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1756\n",
      "           e       0.99      0.92      0.95      1066\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.96      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9659815733522324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1789\n",
      "           e       0.99      0.94      0.97      1033\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9751948972360028\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1715\n",
      "           e       0.98      0.92      0.95      1107\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.97      0.95      0.96      2822\n",
      "weighted avg       0.96      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.9617292700212615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1750\n",
      "           e       0.98      0.93      0.96      1072\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9691708008504607\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1754\n",
      "           e       0.99      0.91      0.95      1068\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.97      0.95      0.96      2822\n",
      "weighted avg       0.96      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.9635010630758327\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1739\n",
      "           e       0.99      0.94      0.96      1083\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9723600283486888\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.97      1765\n",
      "           e       0.99      0.92      0.95      1057\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.96      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9663359319631467\n",
      "mean accuracy 0.9681077250177179\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model6 = sum(acc)/10\n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1739\n",
      "           e       0.99      0.88      0.93      1083\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9510985116938342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1765\n",
      "           e       0.99      0.88      0.93      1057\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9507441530829199\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1742\n",
      "           e       0.99      0.87      0.93      1080\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.93      0.94      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9472005669737774\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.94      0.99      0.97      1762\n",
      "           e       0.98      0.90      0.94      1060\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.96      0.95      0.95      2822\n",
      "weighted avg       0.96      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.9567682494684621\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.94      0.99      0.97      1740\n",
      "           e       0.98      0.90      0.94      1082\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.96      0.95      0.95      2822\n",
      "weighted avg       0.96      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.9571226080793763\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1764\n",
      "           e       0.99      0.88      0.93      1058\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9514528703047485\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1713\n",
      "           e       0.99      0.88      0.93      1109\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9510985116938342\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.94      0.99      0.97      1791\n",
      "           e       0.98      0.90      0.94      1031\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.96      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.9564138908575478\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1762\n",
      "           e       0.98      0.88      0.93      1060\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9500354358610914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.94      0.99      0.96      1742\n",
      "           e       0.99      0.89      0.94      1080\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.96      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9542877391920623\n",
      "mean accuracy 0.9526222537207655\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model7 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1776\n",
      "           e       0.98      0.76      0.86      1046\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2822\n",
      "   macro avg       0.93      0.88      0.89      2822\n",
      "weighted avg       0.91      0.90      0.90      2822\n",
      "\n",
      "accuracy:  0.904677533664068\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      1.00      0.94      1728\n",
      "           e       0.99      0.80      0.88      1094\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2822\n",
      "   macro avg       0.94      0.90      0.91      2822\n",
      "weighted avg       0.93      0.92      0.92      2822\n",
      "\n",
      "accuracy:  0.9177888022678952\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.89      0.99      0.94      1743\n",
      "           e       0.98      0.80      0.88      1079\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2822\n",
      "   macro avg       0.94      0.89      0.91      2822\n",
      "weighted avg       0.92      0.92      0.91      2822\n",
      "\n",
      "accuracy:  0.9167257264351524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.87      0.99      0.93      1761\n",
      "           e       0.98      0.76      0.86      1061\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2822\n",
      "   macro avg       0.93      0.88      0.89      2822\n",
      "weighted avg       0.91      0.90      0.90      2822\n",
      "\n",
      "accuracy:  0.9043231750531537\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1744\n",
      "           e       0.98      0.79      0.88      1078\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.89      0.91      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9142452161587526\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1760\n",
      "           e       0.99      0.78      0.87      1062\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.89      0.90      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9124734231041814\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.87      0.99      0.92      1727\n",
      "           e       0.98      0.76      0.85      1095\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      2822\n",
      "   macro avg       0.92      0.87      0.89      2822\n",
      "weighted avg       0.91      0.90      0.90      2822\n",
      "\n",
      "accuracy:  0.8997165131112687\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.90      0.99      0.94      1777\n",
      "           e       0.99      0.80      0.89      1045\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2822\n",
      "   macro avg       0.94      0.90      0.91      2822\n",
      "weighted avg       0.93      0.92      0.92      2822\n",
      "\n",
      "accuracy:  0.923458540042523\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.89      0.99      0.94      1790\n",
      "           e       0.98      0.79      0.87      1032\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2822\n",
      "   macro avg       0.94      0.89      0.91      2822\n",
      "weighted avg       0.92      0.92      0.91      2822\n",
      "\n",
      "accuracy:  0.9167257264351524\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.87      0.99      0.93      1714\n",
      "           e       0.99      0.77      0.86      1108\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.88      0.90      2822\n",
      "weighted avg       0.92      0.91      0.90      2822\n",
      "\n",
      "accuracy:  0.9053862508858965\n",
      "mean accuracy 0.9115520907158043\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model8 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1748\n",
      "           e       0.99      0.69      0.81      1074\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.90      0.88      0.87      2822\n",
      "\n",
      "accuracy:  0.8784549964564139\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.85      0.99      0.91      1756\n",
      "           e       0.97      0.71      0.82      1066\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.85      0.87      2822\n",
      "weighted avg       0.90      0.88      0.88      2822\n",
      "\n",
      "accuracy:  0.8827072997873848\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.85      0.99      0.92      1753\n",
      "           e       0.99      0.71      0.83      1069\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2822\n",
      "   macro avg       0.92      0.85      0.87      2822\n",
      "weighted avg       0.90      0.89      0.88      2822\n",
      "\n",
      "accuracy:  0.8869596031183558\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1751\n",
      "           e       0.98      0.69      0.81      1071\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.89      0.87      0.87      2822\n",
      "\n",
      "accuracy:  0.8749114103472715\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1737\n",
      "           e       0.98      0.69      0.81      1085\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.89      0.88      0.87      2822\n",
      "\n",
      "accuracy:  0.8759744861800142\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.85      0.99      0.91      1767\n",
      "           e       0.98      0.70      0.82      1055\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.92      0.85      0.87      2822\n",
      "weighted avg       0.90      0.88      0.88      2822\n",
      "\n",
      "accuracy:  0.8837703756201276\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1757\n",
      "           e       0.98      0.68      0.80      1065\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.89      0.87      0.87      2822\n",
      "\n",
      "accuracy:  0.8745570517363572\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.85      0.99      0.91      1747\n",
      "           e       0.98      0.71      0.83      1075\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2822\n",
      "   macro avg       0.91      0.85      0.87      2822\n",
      "weighted avg       0.90      0.89      0.88      2822\n",
      "\n",
      "accuracy:  0.8855421686746988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1786\n",
      "           e       0.98      0.68      0.80      1036\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.83      0.86      2822\n",
      "weighted avg       0.89      0.88      0.87      2822\n",
      "\n",
      "accuracy:  0.8766832034018427\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1718\n",
      "           e       0.98      0.72      0.83      1104\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.85      0.87      2822\n",
      "weighted avg       0.90      0.88      0.88      2822\n",
      "\n",
      "accuracy:  0.8834160170092134\n",
      "mean accuracy 0.880297661233168\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model9 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9897944720056697, 0.9893338058114812, 0.9835931963146705, 0.9773919206236712, 0.9725726435152373, 0.9681077250177179, 0.9526222537207655, 0.9115520907158043, 0.880297661233168]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_euclidean\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski and k tuning on 10% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1732\n",
      "           e       0.99      0.99      0.99      1090\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9904323175053154\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1772\n",
      "           e       0.99      0.99      0.99      1050\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9904323175053154\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1762\n",
      "           e       0.98      0.99      0.99      1060\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9893692416725727\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1742\n",
      "           e       0.98      0.99      0.99      1080\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9897236002834869\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1735\n",
      "           e       0.98      0.99      0.98      1087\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9879518072289156\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1769\n",
      "           e       0.99      0.99      0.99      1053\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9893692416725727\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.98      0.98      1762\n",
      "           e       0.97      0.97      0.97      1060\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9808646350106307\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1742\n",
      "           e       0.99      0.98      0.99      1080\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9893692416725727\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1722\n",
      "           e       0.98      0.99      0.99      1100\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9890148830616584\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1782\n",
      "           e       0.99      0.98      0.98      1040\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9875974486180015\n",
      "mean accuracy 0.988412473423104\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_minkowski = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       1.00      0.99      0.99      1748\n",
      "           e       0.99      0.99      0.99      1074\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9936215450035436\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1756\n",
      "           e       0.99      0.97      0.98      1066\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9854712969525159\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1770\n",
      "           e       0.99      0.98      0.98      1052\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9883061658398299\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1734\n",
      "           e       0.99      0.98      0.99      1088\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9897236002834869\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1762\n",
      "           e       0.99      0.99      0.99      1060\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9907866761162296\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1742\n",
      "           e       0.99      0.97      0.98      1080\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.98      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.986888731396173\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      0.99      0.99      1770\n",
      "           e       0.98      0.98      0.98      1052\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9883061658398299\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1734\n",
      "           e       0.99      0.97      0.98      1088\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.98      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9865343727852587\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.99      1.00      0.99      1739\n",
      "           e       0.99      0.98      0.99      1083\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9904323175053154\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       1.00      0.99      0.99      1765\n",
      "           e       0.98      0.99      0.99      1057\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.99      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9911410347271439\n",
      "mean accuracy 0.9891211906449326\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model11 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      1.00      0.99      1728\n",
      "           e       1.00      0.96      0.98      1094\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9836995038979447\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1776\n",
      "           e       0.99      0.97      0.98      1046\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      2822\n",
      "   macro avg       0.99      0.98      0.99      2822\n",
      "weighted avg       0.99      0.99      0.99      2822\n",
      "\n",
      "accuracy:  0.9861800141743444\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1740\n",
      "           e       0.99      0.97      0.98      1082\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9844082211197732\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1764\n",
      "           e       1.00      0.94      0.97      1058\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9783841247342311\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1754\n",
      "           e       0.99      0.96      0.98      1068\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9822820694542878\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      1.00      0.99      1750\n",
      "           e       0.99      0.96      0.98      1072\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9822820694542878\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.99      1754\n",
      "           e       0.99      0.97      0.98      1068\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9833451452870304\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.99      1750\n",
      "           e       1.00      0.96      0.98      1072\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9822820694542878\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      1.00      0.99      1743\n",
      "           e       1.00      0.96      0.98      1079\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.99      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9844082211197732\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.98      1761\n",
      "           e       0.98      0.96      0.97      1061\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9794472005669738\n",
      "mean accuracy 0.9826718639262934\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model12 = sum(acc)/10 \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1741\n",
      "           e       0.99      0.94      0.97      1081\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9751948972360028\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1763\n",
      "           e       0.99      0.96      0.97      1059\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9801559177888023\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1751\n",
      "           e       0.99      0.94      0.96      1071\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9734231041814316\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.98      0.99      0.98      1753\n",
      "           e       0.98      0.96      0.97      1069\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9766123316796598\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1759\n",
      "           e       1.00      0.94      0.97      1063\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.976966690290574\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1745\n",
      "           e       0.99      0.95      0.97      1077\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9780297661233168\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1744\n",
      "           e       1.00      0.94      0.97      1078\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9766123316796598\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1760\n",
      "           e       0.99      0.96      0.97      1062\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.98      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.979801559177888\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1759\n",
      "           e       0.98      0.94      0.96      1063\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9727143869596031\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1745\n",
      "           e       1.00      0.95      0.97      1077\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.979801559177888\n",
      "mean accuracy 0.9769312544294827\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model11 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1721\n",
      "           e       1.00      0.94      0.97      1101\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.98      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9748405386250886\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1783\n",
      "           e       0.99      0.94      0.96      1039\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9737774627923459\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1764\n",
      "           e       0.99      0.94      0.96      1058\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9734231041814316\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1740\n",
      "           e       1.00      0.93      0.96      1082\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9705882352941176\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1748\n",
      "           e       0.98      0.94      0.96      1074\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9720056697377746\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1756\n",
      "           e       1.00      0.93      0.96      1066\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9727143869596031\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1728\n",
      "           e       0.99      0.93      0.96      1094\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9691708008504607\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      1.00      0.98      1776\n",
      "           e       1.00      0.94      0.97      1046\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      2822\n",
      "   macro avg       0.98      0.97      0.98      2822\n",
      "weighted avg       0.98      0.98      0.98      2822\n",
      "\n",
      "accuracy:  0.9776754075124026\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.97      0.99      0.98      1754\n",
      "           e       0.98      0.94      0.96      1068\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.97      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9712969525159462\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1750\n",
      "           e       1.00      0.93      0.96      1072\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9712969525159462\n",
      "mean accuracy 0.9726789510985115\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model13 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1766\n",
      "           e       0.98      0.93      0.96      1056\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9691708008504607\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.97      1738\n",
      "           e       0.99      0.93      0.96      1084\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9681077250177179\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1735\n",
      "           e       0.99      0.92      0.95      1087\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.97      0.95      0.96      2822\n",
      "weighted avg       0.96      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.96243798724309\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1769\n",
      "           e       0.99      0.93      0.96      1053\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9720056697377746\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1739\n",
      "           e       0.99      0.92      0.95      1083\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.96      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9652728561304039\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1765\n",
      "           e       0.99      0.93      0.96      1057\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9684620836286322\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1792\n",
      "           e       0.99      0.93      0.96      1030\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9688164422395464\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1712\n",
      "           e       0.99      0.92      0.95      1110\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.96      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9652728561304039\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      1.00      0.98      1757\n",
      "           e       0.99      0.93      0.96      1065\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.98      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9712969525159462\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.96      0.99      0.98      1747\n",
      "           e       0.98      0.94      0.96      1075\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      2822\n",
      "   macro avg       0.97      0.96      0.97      2822\n",
      "weighted avg       0.97      0.97      0.97      2822\n",
      "\n",
      "accuracy:  0.9691708008504607\n",
      "mean accuracy 0.9680014174344436\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model14 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.94      0.99      0.97      1758\n",
      "           e       0.99      0.89      0.94      1064\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.96      0.96      0.95      2822\n",
      "\n",
      "accuracy:  0.9553508150248051\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1746\n",
      "           e       0.98      0.88      0.93      1076\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.93      0.94      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9475549255846917\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1781\n",
      "           e       0.98      0.87      0.92      1041\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.93      0.94      2822\n",
      "weighted avg       0.95      0.95      0.94      2822\n",
      "\n",
      "accuracy:  0.9454287739192062\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.94      0.99      0.97      1723\n",
      "           e       0.99      0.90      0.94      1099\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.96      0.95      0.95      2822\n",
      "weighted avg       0.96      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.9574769666902906\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.94      0.99      0.96      1739\n",
      "           e       0.98      0.89      0.94      1083\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9532246633593197\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1765\n",
      "           e       0.99      0.88      0.93      1057\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9507441530829199\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.95      0.99      0.97      1775\n",
      "           e       0.99      0.91      0.95      1047\n",
      "\n",
      "   micro avg       0.96      0.96      0.96      2822\n",
      "   macro avg       0.97      0.95      0.96      2822\n",
      "weighted avg       0.96      0.96      0.96      2822\n",
      "\n",
      "accuracy:  0.9627923458540043\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.91      0.99      0.95      1729\n",
      "           e       0.99      0.85      0.92      1093\n",
      "\n",
      "   micro avg       0.94      0.94      0.94      2822\n",
      "   macro avg       0.95      0.92      0.93      2822\n",
      "weighted avg       0.94      0.94      0.94      2822\n",
      "\n",
      "accuracy:  0.9390503189227498\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1740\n",
      "           e       0.99      0.89      0.93      1082\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9518072289156626\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.93      0.99      0.96      1764\n",
      "           e       0.99      0.88      0.93      1058\n",
      "\n",
      "   micro avg       0.95      0.95      0.95      2822\n",
      "   macro avg       0.96      0.94      0.95      2822\n",
      "weighted avg       0.95      0.95      0.95      2822\n",
      "\n",
      "accuracy:  0.9528703047484054\n",
      "mean accuracy 0.9516300496102055\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model15 = sum(acc)/10 \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1801\n",
      "           e       0.98      0.76      0.86      1021\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.88      0.90      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9092841956059532\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1703\n",
      "           e       0.99      0.79      0.88      1119\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.89      0.91      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9142452161587526\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1755\n",
      "           e       0.98      0.78      0.87      1067\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.88      0.90      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.910347271438696\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.87      0.99      0.93      1749\n",
      "           e       0.99      0.77      0.86      1073\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.88      0.90      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9078667611622963\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.87      0.99      0.93      1745\n",
      "           e       0.99      0.76      0.86      1077\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.88      0.90      2822\n",
      "weighted avg       0.92      0.91      0.90      2822\n",
      "\n",
      "accuracy:  0.9068036853295535\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.89      0.99      0.94      1759\n",
      "           e       0.98      0.79      0.88      1063\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      2822\n",
      "   macro avg       0.93      0.89      0.91      2822\n",
      "weighted avg       0.92      0.92      0.91      2822\n",
      "\n",
      "accuracy:  0.9160170092133239\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1776\n",
      "           e       0.99      0.78      0.87      1046\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.88      0.90      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9128277817150957\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1728\n",
      "           e       0.98      0.79      0.87      1094\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.89      0.90      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9110559886605244\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1744\n",
      "           e       0.98      0.77      0.87      1078\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.88      0.90      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9089298369950389\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.88      0.99      0.93      1760\n",
      "           e       0.98      0.79      0.87      1062\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      2822\n",
      "   macro avg       0.93      0.89      0.90      2822\n",
      "weighted avg       0.92      0.91      0.91      2822\n",
      "\n",
      "accuracy:  0.9138908575478384\n",
      "mean accuracy 0.9111268603827071\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model16 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.85      0.99      0.91      1760\n",
      "           e       0.98      0.71      0.82      1062\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2822\n",
      "   macro avg       0.91      0.85      0.87      2822\n",
      "weighted avg       0.90      0.89      0.88      2822\n",
      "\n",
      "accuracy:  0.8851878100637846\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1744\n",
      "           e       0.99      0.69      0.81      1078\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.89      0.88      0.87      2822\n",
      "\n",
      "accuracy:  0.8763288447909284\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1751\n",
      "           e       0.98      0.69      0.81      1071\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.89      0.88      0.87      2822\n",
      "\n",
      "accuracy:  0.8773919206236711\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.85      0.99      0.92      1753\n",
      "           e       0.98      0.72      0.83      1069\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2822\n",
      "   macro avg       0.92      0.85      0.87      2822\n",
      "weighted avg       0.90      0.89      0.88      2822\n",
      "\n",
      "accuracy:  0.8876683203401843\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1745\n",
      "           e       0.98      0.69      0.81      1077\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.89      0.88      0.87      2822\n",
      "\n",
      "accuracy:  0.8781006378454996\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.85      0.99      0.91      1759\n",
      "           e       0.98      0.70      0.82      1063\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.85      0.86      2822\n",
      "weighted avg       0.90      0.88      0.88      2822\n",
      "\n",
      "accuracy:  0.8816442239546421\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.86      0.99      0.92      1774\n",
      "           e       0.98      0.72      0.83      1048\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2822\n",
      "   macro avg       0.92      0.86      0.87      2822\n",
      "weighted avg       0.90      0.89      0.89      2822\n",
      "\n",
      "accuracy:  0.890148830616584\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.83      0.99      0.91      1730\n",
      "           e       0.99      0.68      0.81      1092\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.89      0.87      0.87      2822\n",
      "\n",
      "accuracy:  0.8731396172927002\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.85      0.99      0.91      1727\n",
      "           e       0.98      0.72      0.83      1095\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      2822\n",
      "   macro avg       0.91      0.86      0.87      2822\n",
      "weighted avg       0.90      0.89      0.88      2822\n",
      "\n",
      "accuracy:  0.8858965272856131\n",
      "set(['p', 'e'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           p       0.84      0.99      0.91      1777\n",
      "           e       0.98      0.68      0.81      1045\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      2822\n",
      "   macro avg       0.91      0.84      0.86      2822\n",
      "weighted avg       0.89      0.88      0.87      2822\n",
      "\n",
      "accuracy:  0.8781006378454996\n",
      "mean accuracy 0.8813607370659107\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(mushroom):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model17 = sum(acc)/10\n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.988412473423104, 0.9891211906449326, 0.9826718639262934, 0.9769312544294827, 0.9726789510985115, 0.9680014174344436, 0.9516300496102055, 0.9111268603827071, 0.8813607370659107]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_minkowski\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
