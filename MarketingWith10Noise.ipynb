{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sex', ' MaritalStatus', ' Age', ' Education', ' Occupation',\n",
       "       ' YearsInSf', ' DualIncome', ' HouseholdMembers', ' Under18',\n",
       "       ' HouseholdStatus', ' TypeOfHome', ' EthnicClass', ' Language',\n",
       "       'Class'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing = pd.read_csv(\"marketing_numerical.csv\")\n",
    "(marketing.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>YearsInSf</th>\n",
       "      <th>DualIncome</th>\n",
       "      <th>HouseholdMembers</th>\n",
       "      <th>Under18</th>\n",
       "      <th>HouseholdStatus</th>\n",
       "      <th>TypeOfHome</th>\n",
       "      <th>EthnicClass</th>\n",
       "      <th>Language</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6846</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6847</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6848</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6849</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6855</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6856</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6857</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6863</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6865</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6866</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6867</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6869</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6871</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6873</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6874</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6875</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6876 rows Ã— 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   MaritalStatus   Age   Education   Occupation   YearsInSf  \\\n",
       "0       1             1.0     5         5.0          5.0         5.0   \n",
       "1       2             1.0     3         5.0          1.0         5.0   \n",
       "2       2             5.0     1         2.0          6.0         5.0   \n",
       "3       2             5.0     1         2.0          6.0         3.0   \n",
       "4       1             1.0     6         4.0          8.0         5.0   \n",
       "5       1             5.0     2         3.0          9.0         4.0   \n",
       "6       1             3.0     3         4.0          3.0         5.0   \n",
       "7       1             1.0     6         3.0          8.0         5.0   \n",
       "8       1             1.0     7         4.0          8.0         4.0   \n",
       "9       1             5.0     2         4.0          9.0         5.0   \n",
       "10      2             2.0     2         3.0          2.0         5.0   \n",
       "11      2             1.0     3         6.0          6.0         2.0   \n",
       "12      2             1.0     5         3.0          5.0         5.0   \n",
       "13      2             1.0     5         4.0          1.0         5.0   \n",
       "14      2             3.0     3         3.0          2.0         2.0   \n",
       "15      2             1.0     5         3.0          5.0         1.0   \n",
       "16      1             3.0     4         2.0          3.0         4.0   \n",
       "17      2             1.0     4         4.0          2.0         5.0   \n",
       "18      1             1.0     4         4.0          1.0         5.0   \n",
       "19      2             3.0     5         4.0          2.0         3.0   \n",
       "20      2             5.0     1         2.0          6.0         5.0   \n",
       "21      1             1.0     3         5.0          1.0         5.0   \n",
       "22      1             1.0     4         4.0          1.0         5.0   \n",
       "23      1             1.0     4         4.0          1.0         5.0   \n",
       "24      2             3.0     7         3.0          8.0         5.0   \n",
       "25      2             3.0     4         5.0          1.0         4.0   \n",
       "26      2             1.0     4         4.0          1.0         4.0   \n",
       "27      2             5.0     1         2.0          6.0         5.0   \n",
       "28      2             5.0     1         2.0          6.0         5.0   \n",
       "29      1             5.0     1         2.0          6.0         5.0   \n",
       "...   ...             ...   ...         ...          ...         ...   \n",
       "6846    1             5.0     2         4.0          2.0         5.0   \n",
       "6847    1             1.0     3         4.0          1.0         5.0   \n",
       "6848    1             1.0     3         4.0          2.0         5.0   \n",
       "6849    1             5.0     3         5.0          2.0         5.0   \n",
       "6850    2             1.0     6         5.0          1.0         5.0   \n",
       "6851    1             3.0     3         5.0          1.0         5.0   \n",
       "6852    1             5.0     1         2.0          6.0         5.0   \n",
       "6853    2             5.0     1         2.0          6.0         5.0   \n",
       "6854    1             5.0     3         4.0          4.0         5.0   \n",
       "6855    2             5.0     2         4.0          4.0         5.0   \n",
       "6856    1             1.0     7         4.0          8.0         5.0   \n",
       "6857    1             4.0     3         4.0          1.0         5.0   \n",
       "6858    2             5.0     2         3.0          6.0         2.0   \n",
       "6859    1             5.0     2         4.0          4.0         5.0   \n",
       "6860    2             3.0     4         4.0          1.0         5.0   \n",
       "6861    1             5.0     3         5.0          2.0         5.0   \n",
       "6862    1             5.0     2         3.0          3.0         5.0   \n",
       "6863    2             1.0     4         6.0          5.0         5.0   \n",
       "6864    2             5.0     1         2.0          6.0         5.0   \n",
       "6865    1             5.0     2         3.0          4.0         5.0   \n",
       "6866    1             5.0     3         4.0          6.0         5.0   \n",
       "6867    1             5.0     2         4.0          4.0         2.0   \n",
       "6868    2             5.0     2         4.0          4.0         2.0   \n",
       "6869    1             5.0     3         4.0          6.0         2.0   \n",
       "6870    1             1.0     3         6.0          1.0         5.0   \n",
       "6871    2             5.0     1         1.0          2.0         5.0   \n",
       "6872    1             5.0     2         4.0          1.0         5.0   \n",
       "6873    2             5.0     1         2.0          1.0         5.0   \n",
       "6874    1             1.0     6         4.0          3.0         5.0   \n",
       "6875    1             5.0     3         4.0          1.0         5.0   \n",
       "\n",
       "       DualIncome   HouseholdMembers   Under18   HouseholdStatus   TypeOfHome  \\\n",
       "0               3                5.0         2               1.0          1.0   \n",
       "1               2                3.0         1               2.0          3.0   \n",
       "2               1                4.0         2               3.0          1.0   \n",
       "3               1                4.0         2               3.0          1.0   \n",
       "4               3                2.0         0               1.0          1.0   \n",
       "5               1                3.0         1               2.0          3.0   \n",
       "6               1                1.0         0               2.0          3.0   \n",
       "7               3                3.0         0               2.0          3.0   \n",
       "8               3                2.0         0               2.0          3.0   \n",
       "9               1                1.0         0               2.0          3.0   \n",
       "10              1                2.0         0               1.0          1.0   \n",
       "11              2                4.0         2               1.0          1.0   \n",
       "12              3                4.0         0               2.0          1.0   \n",
       "13              2                2.0         2               1.0          1.0   \n",
       "14              1                2.0         1               2.0          3.0   \n",
       "15              3                2.0         0               2.0          3.0   \n",
       "16              1                2.0         0               2.0          3.0   \n",
       "17              3                5.0         3               1.0          1.0   \n",
       "18              3                5.0         3               1.0          1.0   \n",
       "19              1                3.0         0               2.0          5.0   \n",
       "20              1                4.0         1               3.0          1.0   \n",
       "21              2                3.0         1               2.0          3.0   \n",
       "22              2                3.0         0               1.0          1.0   \n",
       "23              2                3.0         1               1.0          1.0   \n",
       "24              1                1.0         0               1.0          5.0   \n",
       "25              1                5.0         3               1.0          1.0   \n",
       "26              2                4.0         2               2.0          1.0   \n",
       "27              1                4.0         2               3.0          1.0   \n",
       "28              1                3.0         1               3.0          1.0   \n",
       "29              1                6.0         4               3.0          1.0   \n",
       "...           ...                ...       ...               ...          ...   \n",
       "6846            1                3.0         0               3.0          1.0   \n",
       "6847            3                4.0         2               1.0          1.0   \n",
       "6848            2                5.0         3               1.0          1.0   \n",
       "6849            1                1.0         0               1.0          2.0   \n",
       "6850            2                2.0         0               1.0          1.0   \n",
       "6851            1                2.0         0               1.0          1.0   \n",
       "6852            1                3.0         1               3.0          1.0   \n",
       "6853            1                4.0         2               3.0          1.0   \n",
       "6854            1                2.0         0               2.0          3.0   \n",
       "6855            1                3.0         0               2.0          1.0   \n",
       "6856            3                2.0         0               1.0          1.0   \n",
       "6857            1                3.0         0               2.0          1.0   \n",
       "6858            1                4.0         2               3.0          3.0   \n",
       "6859            1                1.0         0               2.0          2.0   \n",
       "6860            1                1.0         0               1.0          2.0   \n",
       "6861            1                1.0         0               2.0          3.0   \n",
       "6862            1                1.0         0               2.0          1.0   \n",
       "6863            3                5.0         3               1.0          1.0   \n",
       "6864            1                4.0         1               3.0          1.0   \n",
       "6865            1                4.0         0               3.0          1.0   \n",
       "6866            1                2.0         0               2.0          5.0   \n",
       "6867            1                3.0         0               2.0          5.0   \n",
       "6868            1                4.0         0               2.0          1.0   \n",
       "6869            1                3.0         0               2.0          3.0   \n",
       "6870            2                2.0         0               1.0          1.0   \n",
       "6871            1                3.0         2               3.0          1.0   \n",
       "6872            1                4.0         0               3.0          1.0   \n",
       "6873            1                3.0         2               3.0          1.0   \n",
       "6874            2                3.0         1               2.0          3.0   \n",
       "6875            1                1.0         0               2.0          3.0   \n",
       "\n",
       "       EthnicClass   Language  Class  \n",
       "0              7.0        1.0      9  \n",
       "1              7.0        1.0      9  \n",
       "2              7.0        1.0      1  \n",
       "3              7.0        1.0      1  \n",
       "4              7.0        1.0      8  \n",
       "5              7.0        1.0      1  \n",
       "6              7.0        1.0      6  \n",
       "7              7.0        1.0      2  \n",
       "8              7.0        1.0      4  \n",
       "9              7.0        1.0      1  \n",
       "10             5.0        1.0      4  \n",
       "11             7.0        1.0      8  \n",
       "12             7.0        1.0      7  \n",
       "13             7.0        1.0      7  \n",
       "14             7.0        1.0      1  \n",
       "15             7.0        1.0      8  \n",
       "16             7.0        1.0      2  \n",
       "17             5.0        2.0      9  \n",
       "18             7.0        1.0      8  \n",
       "19             7.0        1.0      4  \n",
       "20             7.0        1.0      1  \n",
       "21             7.0        1.0      9  \n",
       "22             7.0        1.0      7  \n",
       "23             7.0        1.0      9  \n",
       "24             7.0        1.0      4  \n",
       "25             7.0        1.0      6  \n",
       "26             7.0        1.0      8  \n",
       "27             7.0        1.0      1  \n",
       "28             7.0        1.0      1  \n",
       "29             7.0        1.0      1  \n",
       "...            ...        ...    ...  \n",
       "6846           7.0        1.0      3  \n",
       "6847           2.0        1.0      9  \n",
       "6848           5.0        1.0      9  \n",
       "6849           7.0        1.0      6  \n",
       "6850           7.0        1.0      8  \n",
       "6851           7.0        1.0      7  \n",
       "6852           7.0        1.0      1  \n",
       "6853           5.0        1.0      1  \n",
       "6854           7.0        1.0      2  \n",
       "6855           7.0        1.0      4  \n",
       "6856           7.0        1.0      9  \n",
       "6857           7.0        1.0      4  \n",
       "6858           7.0        1.0      1  \n",
       "6859           7.0        1.0      6  \n",
       "6860           7.0        1.0      7  \n",
       "6861           7.0        1.0      5  \n",
       "6862           2.0        1.0      3  \n",
       "6863           5.0        1.0      7  \n",
       "6864           7.0        1.0      1  \n",
       "6865           7.0        1.0      6  \n",
       "6866           7.0        1.0      5  \n",
       "6867           8.0        3.0      1  \n",
       "6868           7.0        1.0      3  \n",
       "6869           8.0        3.0      2  \n",
       "6870           7.0        1.0      9  \n",
       "6871           7.0        1.0      1  \n",
       "6872           7.0        1.0      2  \n",
       "6873           7.0        1.0      1  \n",
       "6874           7.0        1.0      4  \n",
       "6875           5.0        1.0      6  \n",
       "\n",
       "[6876 rows x 14 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "j = 0\n",
    "features = ['Sex', ' MaritalStatus', ' Age', ' Education', ' Occupation',\n",
    "       ' YearsInSf', ' DualIncome', ' HouseholdMembers', ' Under18',\n",
    "       ' HouseholdStatus', ' TypeOfHome', ' EthnicClass', ' Language']\n",
    "for index, m in marketing.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        marketing.at[index+2,features[j]] = i + 1\n",
    "        j += 3\n",
    "        i += 10\n",
    "        if j >= 12:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for index, m in marketing.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        if index < 6870:\n",
    "            marketing.at[index+5,:] = marketing.loc[j,:]\n",
    "        else:\n",
    "            marketing.at[index+1,:] = marketing.loc[j,:]\n",
    "        j += 120\n",
    "        if j >= 6500:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing.to_csv('marketingNoise.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9.0', '1.0', '8.0', '6.0', '2.0', '4.0', '7.0', '5.0', '3.0'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = marketing.iloc[:,0:13]\n",
    "labels = marketing.iloc[:,13].apply(str)\n",
    "labels.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = data.values.astype(np.float32) #returns a numpy array\n",
    "min_max_scaler = MinMaxScaler()\n",
    "x_scaled = min_max_scaler.fit_transform(x)\n",
    "df = pd.DataFrame(x_scaled)\n",
    "marketing = pd.concat([df, labels], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True) #5 times with 2 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean and k tuning on 10% noise datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.60      0.59      0.59       629\n",
      "        8.0       0.19      0.17      0.18       260\n",
      "        1.0       0.18      0.17      0.18       249\n",
      "        4.0       0.20      0.19      0.19       299\n",
      "        7.0       0.16      0.20      0.18       232\n",
      "        3.0       0.28      0.26      0.27       433\n",
      "        6.0       0.27      0.25      0.26       409\n",
      "        2.0       0.34      0.35      0.34       567\n",
      "        9.0       0.34      0.39      0.37       360\n",
      "\n",
      "avg / total       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.62      0.60      0.61       615\n",
      "        8.0       0.21      0.15      0.17       253\n",
      "        1.0       0.18      0.20      0.19       237\n",
      "        4.0       0.16      0.18      0.17       297\n",
      "        7.0       0.26      0.20      0.23       297\n",
      "        3.0       0.24      0.24      0.24       436\n",
      "        6.0       0.24      0.25      0.24       390\n",
      "        2.0       0.29      0.36      0.33       530\n",
      "        9.0       0.39      0.35      0.37       383\n",
      "\n",
      "avg / total       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.31849912739965097\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.61      0.57      0.59       632\n",
      "        8.0       0.17      0.17      0.17       242\n",
      "        1.0       0.12      0.12      0.12       236\n",
      "        4.0       0.16      0.17      0.16       295\n",
      "        7.0       0.19      0.20      0.19       269\n",
      "        3.0       0.26      0.30      0.28       417\n",
      "        6.0       0.26      0.27      0.27       411\n",
      "        2.0       0.35      0.34      0.35       570\n",
      "        9.0       0.33      0.32      0.32       366\n",
      "\n",
      "avg / total       0.32      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3126817917393834\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.57      0.62      0.59       612\n",
      "        8.0       0.17      0.14      0.15       271\n",
      "        1.0       0.11      0.10      0.11       250\n",
      "        4.0       0.14      0.13      0.13       301\n",
      "        7.0       0.19      0.18      0.18       260\n",
      "        3.0       0.27      0.23      0.25       452\n",
      "        6.0       0.22      0.24      0.23       388\n",
      "        2.0       0.32      0.39      0.35       527\n",
      "        9.0       0.35      0.33      0.34       377\n",
      "\n",
      "avg / total       0.30      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.30599185573007565\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.62      0.62      0.62       624\n",
      "        8.0       0.19      0.19      0.19       249\n",
      "        1.0       0.13      0.11      0.12       243\n",
      "        4.0       0.17      0.15      0.16       306\n",
      "        7.0       0.20      0.20      0.20       261\n",
      "        3.0       0.24      0.26      0.25       414\n",
      "        6.0       0.23      0.23      0.23       410\n",
      "        2.0       0.30      0.35      0.33       552\n",
      "        9.0       0.36      0.32      0.34       379\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3138452588714369\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.61      0.63      0.62       620\n",
      "        8.0       0.15      0.16      0.15       264\n",
      "        1.0       0.17      0.14      0.16       243\n",
      "        4.0       0.17      0.20      0.18       290\n",
      "        7.0       0.20      0.21      0.20       268\n",
      "        3.0       0.26      0.23      0.25       455\n",
      "        6.0       0.24      0.22      0.23       389\n",
      "        2.0       0.34      0.36      0.35       545\n",
      "        9.0       0.30      0.32      0.31       364\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3138452588714369\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.61      0.59      0.60       653\n",
      "        8.0       0.19      0.18      0.19       244\n",
      "        1.0       0.17      0.15      0.16       257\n",
      "        4.0       0.13      0.16      0.15       256\n",
      "        7.0       0.20      0.23      0.22       264\n",
      "        3.0       0.26      0.24      0.25       447\n",
      "        6.0       0.23      0.21      0.22       406\n",
      "        2.0       0.32      0.34      0.33       549\n",
      "        9.0       0.30      0.33      0.31       362\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3112274578243165\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.55      0.59      0.57       591\n",
      "        8.0       0.17      0.15      0.16       269\n",
      "        1.0       0.16      0.16      0.16       229\n",
      "        4.0       0.18      0.14      0.16       340\n",
      "        7.0       0.20      0.19      0.20       265\n",
      "        3.0       0.23      0.25      0.24       422\n",
      "        6.0       0.24      0.25      0.25       393\n",
      "        2.0       0.31      0.35      0.33       548\n",
      "        9.0       0.37      0.34      0.35       381\n",
      "\n",
      "avg / total       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.3048283885980221\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.60      0.61      0.60       630\n",
      "        8.0       0.22      0.18      0.19       256\n",
      "        1.0       0.16      0.16      0.16       240\n",
      "        4.0       0.19      0.19      0.19       311\n",
      "        7.0       0.22      0.22      0.22       252\n",
      "        3.0       0.22      0.22      0.22       433\n",
      "        6.0       0.21      0.23      0.22       395\n",
      "        2.0       0.32      0.32      0.32       553\n",
      "        9.0       0.30      0.31      0.31       368\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.30715532286212915\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.61      0.57      0.59       614\n",
      "        8.0       0.19      0.16      0.18       257\n",
      "        1.0       0.14      0.15      0.15       246\n",
      "        4.0       0.17      0.18      0.17       285\n",
      "        7.0       0.20      0.17      0.19       277\n",
      "        3.0       0.26      0.27      0.27       436\n",
      "        6.0       0.20      0.17      0.19       404\n",
      "        2.0       0.33      0.35      0.34       544\n",
      "        9.0       0.32      0.40      0.36       375\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.30744618964514253\n",
      "mean accuracy 0.31198371146015125\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.70      0.59       608\n",
      "        8.0       0.18      0.19      0.18       257\n",
      "        1.0       0.15      0.16      0.15       241\n",
      "        4.0       0.17      0.13      0.15       301\n",
      "        7.0       0.21      0.15      0.17       260\n",
      "        3.0       0.25      0.25      0.25       423\n",
      "        6.0       0.26      0.19      0.22       407\n",
      "        2.0       0.32      0.35      0.33       557\n",
      "        9.0       0.41      0.34      0.38       384\n",
      "\n",
      "avg / total       0.30      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.3205351948807446\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.74      0.60       636\n",
      "        8.0       0.16      0.16      0.16       256\n",
      "        1.0       0.16      0.14      0.15       245\n",
      "        4.0       0.16      0.14      0.14       295\n",
      "        7.0       0.16      0.12      0.14       269\n",
      "        3.0       0.27      0.24      0.25       446\n",
      "        6.0       0.25      0.22      0.24       392\n",
      "        2.0       0.33      0.36      0.34       540\n",
      "        9.0       0.48      0.31      0.38       359\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.74      0.60       625\n",
      "        8.0       0.14      0.13      0.14       251\n",
      "        1.0       0.14      0.14      0.14       235\n",
      "        4.0       0.19      0.20      0.20       278\n",
      "        7.0       0.19      0.14      0.16       274\n",
      "        3.0       0.27      0.23      0.25       455\n",
      "        6.0       0.24      0.20      0.22       422\n",
      "        2.0       0.30      0.32      0.31       521\n",
      "        9.0       0.41      0.30      0.35       377\n",
      "\n",
      "avg / total       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3170447934845841\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.73      0.60       619\n",
      "        8.0       0.17      0.18      0.18       262\n",
      "        1.0       0.15      0.13      0.14       251\n",
      "        4.0       0.20      0.13      0.16       318\n",
      "        7.0       0.17      0.16      0.17       255\n",
      "        3.0       0.23      0.24      0.24       414\n",
      "        6.0       0.24      0.23      0.23       377\n",
      "        2.0       0.39      0.37      0.38       576\n",
      "        9.0       0.40      0.30      0.34       366\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3266433973240256\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.71      0.60       623\n",
      "        8.0       0.18      0.21      0.19       228\n",
      "        1.0       0.14      0.11      0.13       247\n",
      "        4.0       0.16      0.16      0.16       300\n",
      "        7.0       0.25      0.18      0.21       280\n",
      "        3.0       0.29      0.26      0.27       439\n",
      "        6.0       0.25      0.19      0.22       407\n",
      "        2.0       0.34      0.35      0.35       562\n",
      "        9.0       0.39      0.36      0.37       352\n",
      "\n",
      "avg / total       0.31      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.76      0.61       621\n",
      "        8.0       0.21      0.20      0.21       285\n",
      "        1.0       0.13      0.11      0.12       239\n",
      "        4.0       0.16      0.14      0.15       296\n",
      "        7.0       0.17      0.14      0.16       249\n",
      "        3.0       0.27      0.26      0.26       430\n",
      "        6.0       0.23      0.19      0.21       392\n",
      "        2.0       0.35      0.38      0.36       535\n",
      "        9.0       0.48      0.29      0.36       391\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3312972658522397\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.73      0.60       625\n",
      "        8.0       0.22      0.19      0.20       277\n",
      "        1.0       0.17      0.16      0.16       260\n",
      "        4.0       0.18      0.18      0.18       284\n",
      "        7.0       0.18      0.15      0.16       268\n",
      "        3.0       0.28      0.25      0.27       444\n",
      "        6.0       0.25      0.24      0.24       381\n",
      "        2.0       0.37      0.38      0.37       532\n",
      "        9.0       0.44      0.29      0.35       367\n",
      "\n",
      "avg / total       0.32      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.3344968004653869\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.73      0.61       619\n",
      "        8.0       0.17      0.19      0.18       236\n",
      "        1.0       0.15      0.12      0.14       226\n",
      "        4.0       0.18      0.17      0.18       312\n",
      "        7.0       0.20      0.16      0.18       261\n",
      "        3.0       0.23      0.23      0.23       425\n",
      "        6.0       0.29      0.22      0.25       418\n",
      "        2.0       0.36      0.39      0.37       565\n",
      "        9.0       0.45      0.32      0.38       376\n",
      "\n",
      "avg / total       0.32      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.33391506689936007\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.77      0.59       589\n",
      "        8.0       0.14      0.14      0.14       255\n",
      "        1.0       0.19      0.14      0.16       255\n",
      "        4.0       0.18      0.17      0.17       293\n",
      "        7.0       0.19      0.17      0.18       254\n",
      "        3.0       0.30      0.22      0.26       456\n",
      "        6.0       0.24      0.24      0.24       397\n",
      "        2.0       0.34      0.34      0.34       558\n",
      "        9.0       0.43      0.31      0.36       381\n",
      "\n",
      "avg / total       0.31      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.32460732984293195\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.56      0.71      0.62       655\n",
      "        8.0       0.16      0.17      0.16       258\n",
      "        1.0       0.14      0.16      0.15       231\n",
      "        4.0       0.16      0.13      0.14       303\n",
      "        7.0       0.23      0.16      0.19       275\n",
      "        3.0       0.23      0.25      0.24       413\n",
      "        6.0       0.24      0.17      0.20       402\n",
      "        2.0       0.35      0.37      0.36       539\n",
      "        9.0       0.42      0.37      0.39       362\n",
      "\n",
      "avg / total       0.31      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.3298429319371728\n",
      "mean accuracy 0.3273414776032577\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model2 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.78      0.61       601\n",
      "        8.0       0.17      0.14      0.16       249\n",
      "        1.0       0.20      0.12      0.15       251\n",
      "        4.0       0.18      0.16      0.17       310\n",
      "        7.0       0.20      0.16      0.18       286\n",
      "        3.0       0.25      0.26      0.25       433\n",
      "        6.0       0.23      0.17      0.20       391\n",
      "        2.0       0.38      0.38      0.38       571\n",
      "        9.0       0.38      0.37      0.38       346\n",
      "\n",
      "avg / total       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3356602675974404\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.72      0.59       643\n",
      "        8.0       0.18      0.15      0.16       264\n",
      "        1.0       0.18      0.14      0.16       235\n",
      "        4.0       0.15      0.12      0.13       286\n",
      "        7.0       0.20      0.19      0.19       243\n",
      "        3.0       0.23      0.25      0.24       436\n",
      "        6.0       0.26      0.19      0.22       408\n",
      "        2.0       0.32      0.39      0.35       526\n",
      "        9.0       0.47      0.33      0.39       397\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.76      0.61       625\n",
      "        8.0       0.16      0.15      0.16       255\n",
      "        1.0       0.17      0.11      0.14       244\n",
      "        4.0       0.22      0.16      0.19       293\n",
      "        7.0       0.18      0.13      0.15       262\n",
      "        3.0       0.23      0.26      0.24       413\n",
      "        6.0       0.23      0.16      0.19       408\n",
      "        2.0       0.34      0.39      0.36       558\n",
      "        9.0       0.36      0.29      0.32       380\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3266433973240256\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.70      0.59       619\n",
      "        8.0       0.21      0.18      0.20       258\n",
      "        1.0       0.12      0.10      0.11       242\n",
      "        4.0       0.18      0.15      0.17       303\n",
      "        7.0       0.19      0.13      0.16       267\n",
      "        3.0       0.25      0.27      0.26       456\n",
      "        6.0       0.18      0.15      0.16       391\n",
      "        2.0       0.31      0.39      0.35       539\n",
      "        9.0       0.41      0.31      0.35       363\n",
      "\n",
      "avg / total       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.31529959278650377\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.78      0.60       626\n",
      "        8.0       0.17      0.14      0.15       279\n",
      "        1.0       0.17      0.08      0.10       253\n",
      "        4.0       0.17      0.17      0.17       290\n",
      "        7.0       0.14      0.12      0.13       236\n",
      "        3.0       0.24      0.25      0.24       428\n",
      "        6.0       0.25      0.14      0.18       425\n",
      "        2.0       0.35      0.42      0.39       544\n",
      "        9.0       0.42      0.31      0.36       357\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3286794648051193\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.57      0.72      0.64       618\n",
      "        8.0       0.19      0.21      0.20       234\n",
      "        1.0       0.17      0.14      0.15       233\n",
      "        4.0       0.22      0.18      0.20       306\n",
      "        7.0       0.26      0.14      0.18       293\n",
      "        3.0       0.25      0.29      0.27       441\n",
      "        6.0       0.22      0.17      0.19       374\n",
      "        2.0       0.32      0.38      0.35       553\n",
      "        9.0       0.40      0.35      0.37       386\n",
      "\n",
      "avg / total       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.3368237347294939\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.54      0.75      0.63       627\n",
      "        8.0       0.18      0.17      0.18       235\n",
      "        1.0       0.18      0.13      0.15       241\n",
      "        4.0       0.21      0.14      0.17       300\n",
      "        7.0       0.19      0.14      0.16       261\n",
      "        3.0       0.25      0.28      0.26       441\n",
      "        6.0       0.20      0.18      0.19       400\n",
      "        2.0       0.34      0.36      0.35       560\n",
      "        9.0       0.40      0.34      0.36       373\n",
      "\n",
      "avg / total       0.31      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.72      0.59       617\n",
      "        8.0       0.18      0.14      0.16       278\n",
      "        1.0       0.17      0.14      0.15       245\n",
      "        4.0       0.18      0.18      0.18       296\n",
      "        7.0       0.18      0.13      0.15       268\n",
      "        3.0       0.25      0.25      0.25       428\n",
      "        6.0       0.25      0.19      0.22       399\n",
      "        2.0       0.34      0.40      0.37       537\n",
      "        9.0       0.42      0.32      0.37       370\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3263525305410122\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.74      0.59       602\n",
      "        8.0       0.15      0.14      0.15       243\n",
      "        1.0       0.15      0.08      0.11       271\n",
      "        4.0       0.18      0.20      0.19       284\n",
      "        7.0       0.20      0.15      0.17       264\n",
      "        3.0       0.20      0.24      0.22       420\n",
      "        6.0       0.24      0.15      0.19       423\n",
      "        2.0       0.34      0.36      0.35       552\n",
      "        9.0       0.43      0.32      0.37       379\n",
      "\n",
      "avg / total       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3150087260034904\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.54      0.76      0.63       642\n",
      "        8.0       0.20      0.16      0.18       270\n",
      "        1.0       0.14      0.15      0.14       215\n",
      "        4.0       0.19      0.15      0.17       312\n",
      "        7.0       0.20      0.11      0.14       265\n",
      "        3.0       0.26      0.28      0.27       449\n",
      "        6.0       0.25      0.20      0.22       376\n",
      "        2.0       0.33      0.37      0.35       545\n",
      "        9.0       0.40      0.34      0.37       364\n",
      "\n",
      "avg / total       0.31      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.3379872018615474\n",
      "mean accuracy 0.3285049447353113\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model3 = sum(acc)/10 \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.75      0.62       636\n",
      "        8.0       0.17      0.17      0.17       242\n",
      "        1.0       0.20      0.13      0.16       231\n",
      "        4.0       0.19      0.14      0.16       308\n",
      "        7.0       0.15      0.10      0.12       273\n",
      "        3.0       0.21      0.26      0.23       418\n",
      "        6.0       0.23      0.17      0.20       407\n",
      "        2.0       0.33      0.38      0.35       554\n",
      "        9.0       0.41      0.33      0.36       369\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.73      0.60       608\n",
      "        8.0       0.20      0.12      0.15       271\n",
      "        1.0       0.20      0.14      0.16       255\n",
      "        4.0       0.20      0.22      0.21       288\n",
      "        7.0       0.18      0.14      0.16       256\n",
      "        3.0       0.25      0.25      0.25       451\n",
      "        6.0       0.22      0.15      0.18       392\n",
      "        2.0       0.32      0.44      0.37       543\n",
      "        9.0       0.42      0.30      0.35       374\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3278068644560791\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.79      0.60       583\n",
      "        8.0       0.22      0.17      0.19       249\n",
      "        1.0       0.19      0.11      0.14       241\n",
      "        4.0       0.21      0.17      0.19       299\n",
      "        7.0       0.20      0.09      0.12       271\n",
      "        3.0       0.25      0.26      0.26       434\n",
      "        6.0       0.25      0.17      0.20       418\n",
      "        2.0       0.33      0.39      0.36       562\n",
      "        9.0       0.39      0.41      0.40       381\n",
      "\n",
      "avg / total       0.30      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3379872018615474\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.54      0.74      0.62       661\n",
      "        8.0       0.17      0.12      0.14       264\n",
      "        1.0       0.12      0.08      0.09       245\n",
      "        4.0       0.19      0.15      0.17       297\n",
      "        7.0       0.16      0.12      0.13       258\n",
      "        3.0       0.22      0.26      0.24       435\n",
      "        6.0       0.24      0.19      0.21       381\n",
      "        2.0       0.35      0.45      0.39       535\n",
      "        9.0       0.43      0.31      0.36       362\n",
      "\n",
      "avg / total       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.335369400814427\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.79      0.61       611\n",
      "        8.0       0.23      0.17      0.19       251\n",
      "        1.0       0.14      0.07      0.09       242\n",
      "        4.0       0.19      0.13      0.15       320\n",
      "        7.0       0.16      0.09      0.12       265\n",
      "        3.0       0.21      0.28      0.24       413\n",
      "        6.0       0.20      0.14      0.16       398\n",
      "        2.0       0.36      0.41      0.38       570\n",
      "        9.0       0.37      0.32      0.34       368\n",
      "\n",
      "avg / total       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.75      0.62       633\n",
      "        8.0       0.15      0.10      0.12       262\n",
      "        1.0       0.12      0.09      0.11       244\n",
      "        4.0       0.14      0.16      0.15       276\n",
      "        7.0       0.17      0.12      0.14       264\n",
      "        3.0       0.28      0.24      0.26       456\n",
      "        6.0       0.21      0.13      0.17       401\n",
      "        2.0       0.30      0.44      0.36       527\n",
      "        9.0       0.43      0.34      0.38       375\n",
      "\n",
      "avg / total       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.72      0.61       613\n",
      "        8.0       0.21      0.17      0.19       264\n",
      "        1.0       0.23      0.10      0.14       257\n",
      "        4.0       0.20      0.16      0.18       308\n",
      "        7.0       0.17      0.12      0.14       251\n",
      "        3.0       0.24      0.31      0.27       429\n",
      "        6.0       0.24      0.13      0.17       411\n",
      "        2.0       0.29      0.44      0.35       528\n",
      "        9.0       0.43      0.32      0.36       377\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.78      0.61       631\n",
      "        8.0       0.20      0.17      0.18       249\n",
      "        1.0       0.15      0.11      0.13       229\n",
      "        4.0       0.21      0.18      0.19       288\n",
      "        7.0       0.22      0.09      0.12       278\n",
      "        3.0       0.22      0.23      0.22       440\n",
      "        6.0       0.20      0.18      0.19       388\n",
      "        2.0       0.35      0.40      0.37       569\n",
      "        9.0       0.40      0.31      0.35       366\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3321698662012798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.77      0.60       596\n",
      "        8.0       0.17      0.10      0.13       274\n",
      "        1.0       0.19      0.10      0.13       254\n",
      "        4.0       0.18      0.18      0.18       295\n",
      "        7.0       0.22      0.12      0.16       265\n",
      "        3.0       0.19      0.21      0.20       431\n",
      "        6.0       0.22      0.18      0.20       383\n",
      "        2.0       0.35      0.41      0.38       559\n",
      "        9.0       0.39      0.34      0.36       381\n",
      "\n",
      "avg / total       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3240255962769052\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.55      0.76      0.64       648\n",
      "        8.0       0.17      0.14      0.15       239\n",
      "        1.0       0.17      0.14      0.15       232\n",
      "        4.0       0.21      0.17      0.19       301\n",
      "        7.0       0.21      0.14      0.17       264\n",
      "        3.0       0.24      0.25      0.25       438\n",
      "        6.0       0.17      0.12      0.14       416\n",
      "        2.0       0.31      0.38      0.34       538\n",
      "        9.0       0.38      0.35      0.36       362\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3312972658522397\n",
      "mean accuracy 0.32993019197207685\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model4 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.77      0.62       637\n",
      "        8.0       0.17      0.09      0.12       274\n",
      "        1.0       0.17      0.11      0.13       239\n",
      "        4.0       0.18      0.11      0.13       316\n",
      "        7.0       0.19      0.15      0.16       260\n",
      "        3.0       0.24      0.25      0.24       433\n",
      "        6.0       0.18      0.16      0.17       378\n",
      "        2.0       0.32      0.46      0.38       525\n",
      "        9.0       0.46      0.33      0.38       376\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33304246655031994\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.73      0.58       607\n",
      "        8.0       0.17      0.11      0.13       239\n",
      "        1.0       0.18      0.09      0.12       247\n",
      "        4.0       0.20      0.18      0.19       280\n",
      "        7.0       0.22      0.12      0.15       269\n",
      "        3.0       0.22      0.27      0.24       436\n",
      "        6.0       0.20      0.12      0.15       421\n",
      "        2.0       0.31      0.40      0.35       572\n",
      "        9.0       0.41      0.35      0.38       367\n",
      "\n",
      "avg / total       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3190808609656777\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.76      0.62       630\n",
      "        8.0       0.21      0.14      0.17       257\n",
      "        1.0       0.22      0.09      0.13       251\n",
      "        4.0       0.24      0.18      0.20       288\n",
      "        7.0       0.21      0.11      0.15       264\n",
      "        3.0       0.25      0.30      0.27       433\n",
      "        6.0       0.25      0.17      0.20       393\n",
      "        2.0       0.33      0.45      0.38       559\n",
      "        9.0       0.37      0.35      0.36       363\n",
      "\n",
      "avg / total       0.32      0.35      0.32      3438\n",
      "\n",
      "accuracy:  0.34642233856893545\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.77      0.61       614\n",
      "        8.0       0.21      0.16      0.18       256\n",
      "        1.0       0.12      0.09      0.10       235\n",
      "        4.0       0.22      0.17      0.19       308\n",
      "        7.0       0.14      0.08      0.10       265\n",
      "        3.0       0.25      0.28      0.27       436\n",
      "        6.0       0.22      0.13      0.17       406\n",
      "        2.0       0.32      0.45      0.37       538\n",
      "        9.0       0.46      0.35      0.40       380\n",
      "\n",
      "avg / total       0.30      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.33595113438045376\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.76      0.60       601\n",
      "        8.0       0.25      0.11      0.15       284\n",
      "        1.0       0.20      0.14      0.17       246\n",
      "        4.0       0.25      0.11      0.15       321\n",
      "        7.0       0.16      0.08      0.11       265\n",
      "        3.0       0.22      0.34      0.27       402\n",
      "        6.0       0.22      0.14      0.17       401\n",
      "        2.0       0.32      0.43      0.37       546\n",
      "        9.0       0.41      0.37      0.39       372\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33275159976730656\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.77      0.61       643\n",
      "        8.0       0.18      0.17      0.18       229\n",
      "        1.0       0.14      0.07      0.09       240\n",
      "        4.0       0.16      0.18      0.17       275\n",
      "        7.0       0.25      0.10      0.15       264\n",
      "        3.0       0.27      0.22      0.24       467\n",
      "        6.0       0.22      0.17      0.19       398\n",
      "        2.0       0.33      0.43      0.37       551\n",
      "        9.0       0.41      0.32      0.36       371\n",
      "\n",
      "avg / total       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3365328679464805\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.80      0.60       594\n",
      "        8.0       0.13      0.08      0.10       255\n",
      "        1.0       0.16      0.13      0.15       235\n",
      "        4.0       0.16      0.12      0.14       298\n",
      "        7.0       0.19      0.12      0.15       278\n",
      "        3.0       0.25      0.24      0.25       453\n",
      "        6.0       0.26      0.18      0.21       383\n",
      "        2.0       0.34      0.43      0.38       564\n",
      "        9.0       0.39      0.31      0.34       378\n",
      "\n",
      "avg / total       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.71      0.60       650\n",
      "        8.0       0.20      0.15      0.17       258\n",
      "        1.0       0.20      0.10      0.13       251\n",
      "        4.0       0.19      0.17      0.18       298\n",
      "        7.0       0.12      0.07      0.09       251\n",
      "        3.0       0.23      0.28      0.25       416\n",
      "        6.0       0.21      0.14      0.17       416\n",
      "        2.0       0.32      0.43      0.37       533\n",
      "        9.0       0.41      0.33      0.37       365\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3254799301919721\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.79      0.62       628\n",
      "        8.0       0.24      0.12      0.16       283\n",
      "        1.0       0.19      0.09      0.12       248\n",
      "        4.0       0.20      0.18      0.19       297\n",
      "        7.0       0.13      0.06      0.08       272\n",
      "        3.0       0.23      0.30      0.26       405\n",
      "        6.0       0.21      0.13      0.17       401\n",
      "        2.0       0.30      0.41      0.35       537\n",
      "        9.0       0.42      0.35      0.38       367\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.77      0.63       616\n",
      "        8.0       0.19      0.20      0.20       230\n",
      "        1.0       0.17      0.11      0.13       238\n",
      "        4.0       0.19      0.13      0.15       299\n",
      "        7.0       0.14      0.10      0.12       257\n",
      "        3.0       0.26      0.27      0.27       464\n",
      "        6.0       0.22      0.14      0.17       398\n",
      "        2.0       0.34      0.44      0.38       560\n",
      "        9.0       0.41      0.33      0.36       376\n",
      "\n",
      "avg / total       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.337696335078534\n",
      "mean accuracy 0.33295520651541594\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model5 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.80      0.62       624\n",
      "        8.0       0.18      0.14      0.16       234\n",
      "        1.0       0.17      0.10      0.13       242\n",
      "        4.0       0.22      0.15      0.18       300\n",
      "        7.0       0.11      0.04      0.06       259\n",
      "        3.0       0.23      0.28      0.25       414\n",
      "        6.0       0.22      0.13      0.17       414\n",
      "        2.0       0.31      0.43      0.36       576\n",
      "        9.0       0.40      0.31      0.35       375\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33420593368237345\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.77      0.61       620\n",
      "        8.0       0.13      0.06      0.09       279\n",
      "        1.0       0.18      0.07      0.10       244\n",
      "        4.0       0.21      0.17      0.19       296\n",
      "        7.0       0.16      0.11      0.13       270\n",
      "        3.0       0.22      0.26      0.24       455\n",
      "        6.0       0.23      0.14      0.17       385\n",
      "        2.0       0.29      0.42      0.35       521\n",
      "        9.0       0.39      0.32      0.35       368\n",
      "\n",
      "avg / total       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.31995346131471786\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.71      0.60       633\n",
      "        8.0       0.18      0.09      0.12       259\n",
      "        1.0       0.17      0.16      0.17       218\n",
      "        4.0       0.23      0.14      0.17       308\n",
      "        7.0       0.14      0.07      0.09       276\n",
      "        3.0       0.21      0.28      0.24       416\n",
      "        6.0       0.19      0.11      0.14       393\n",
      "        2.0       0.30      0.48      0.37       555\n",
      "        9.0       0.44      0.28      0.34       380\n",
      "\n",
      "avg / total       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32140779522978474\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.47      0.79      0.59       611\n",
      "        8.0       0.17      0.13      0.15       254\n",
      "        1.0       0.23      0.07      0.11       268\n",
      "        4.0       0.16      0.11      0.13       288\n",
      "        7.0       0.11      0.06      0.08       253\n",
      "        3.0       0.21      0.24      0.22       453\n",
      "        6.0       0.25      0.14      0.18       406\n",
      "        2.0       0.32      0.43      0.37       542\n",
      "        9.0       0.37      0.33      0.35       363\n",
      "\n",
      "avg / total       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3179173938336242\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.80      0.61       640\n",
      "        8.0       0.26      0.12      0.16       267\n",
      "        1.0       0.21      0.10      0.14       246\n",
      "        4.0       0.19      0.18      0.19       289\n",
      "        7.0       0.12      0.08      0.10       246\n",
      "        3.0       0.23      0.23      0.23       456\n",
      "        6.0       0.19      0.12      0.14       394\n",
      "        2.0       0.32      0.42      0.36       548\n",
      "        9.0       0.37      0.34      0.35       352\n",
      "\n",
      "avg / total       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3312972658522397\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.78      0.62       604\n",
      "        8.0       0.20      0.15      0.17       246\n",
      "        1.0       0.25      0.12      0.16       240\n",
      "        4.0       0.20      0.15      0.17       307\n",
      "        7.0       0.15      0.04      0.06       283\n",
      "        3.0       0.21      0.29      0.24       413\n",
      "        6.0       0.21      0.13      0.16       405\n",
      "        2.0       0.29      0.46      0.35       549\n",
      "        9.0       0.40      0.24      0.30       391\n",
      "\n",
      "avg / total       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3228621291448517\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.80      0.61       614\n",
      "        8.0       0.19      0.14      0.16       249\n",
      "        1.0       0.23      0.10      0.14       248\n",
      "        4.0       0.23      0.16      0.19       298\n",
      "        7.0       0.11      0.05      0.07       249\n",
      "        3.0       0.24      0.33      0.28       415\n",
      "        6.0       0.25      0.10      0.14       425\n",
      "        2.0       0.32      0.45      0.37       562\n",
      "        9.0       0.44      0.31      0.36       378\n",
      "\n",
      "avg / total       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.3379872018615474\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.80      0.62       630\n",
      "        8.0       0.19      0.10      0.13       264\n",
      "        1.0       0.12      0.07      0.09       238\n",
      "        4.0       0.19      0.13      0.15       298\n",
      "        7.0       0.16      0.05      0.07       280\n",
      "        3.0       0.23      0.23      0.23       454\n",
      "        6.0       0.19      0.16      0.17       374\n",
      "        2.0       0.33      0.47      0.39       535\n",
      "        9.0       0.42      0.36      0.39       365\n",
      "\n",
      "avg / total       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33304246655031994\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.78      0.61       620\n",
      "        8.0       0.20      0.11      0.15       254\n",
      "        1.0       0.19      0.12      0.15       231\n",
      "        4.0       0.21      0.14      0.17       306\n",
      "        7.0       0.17      0.08      0.11       257\n",
      "        3.0       0.24      0.25      0.25       455\n",
      "        6.0       0.23      0.12      0.16       404\n",
      "        2.0       0.27      0.46      0.34       533\n",
      "        9.0       0.41      0.32      0.36       378\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.79      0.61       624\n",
      "        8.0       0.21      0.14      0.17       259\n",
      "        1.0       0.20      0.08      0.12       255\n",
      "        4.0       0.25      0.19      0.22       290\n",
      "        7.0       0.12      0.05      0.07       272\n",
      "        3.0       0.20      0.28      0.24       414\n",
      "        6.0       0.21      0.14      0.16       395\n",
      "        2.0       0.36      0.42      0.39       564\n",
      "        9.0       0.42      0.36      0.39       365\n",
      "\n",
      "avg / total       0.30      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.337696335078534\n",
      "mean accuracy 0.3285922047702153\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model6 = sum(acc)/10\n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.83      0.61       601\n",
      "        8.0       0.19      0.10      0.13       254\n",
      "        1.0       0.16      0.09      0.12       222\n",
      "        4.0       0.24      0.12      0.16       309\n",
      "        7.0       0.13      0.04      0.06       255\n",
      "        3.0       0.22      0.29      0.25       441\n",
      "        6.0       0.17      0.08      0.11       413\n",
      "        2.0       0.31      0.47      0.38       562\n",
      "        9.0       0.40      0.31      0.35       381\n",
      "\n",
      "avg / total       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3298429319371728\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.78      0.62       643\n",
      "        8.0       0.12      0.04      0.06       259\n",
      "        1.0       0.23      0.08      0.12       264\n",
      "        4.0       0.20      0.19      0.20       287\n",
      "        7.0       0.20      0.07      0.10       274\n",
      "        3.0       0.22      0.25      0.23       428\n",
      "        6.0       0.18      0.13      0.15       386\n",
      "        2.0       0.30      0.46      0.36       535\n",
      "        9.0       0.40      0.34      0.37       362\n",
      "\n",
      "avg / total       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33042466550319954\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.80      0.62       620\n",
      "        8.0       0.17      0.11      0.13       257\n",
      "        1.0       0.15      0.10      0.12       238\n",
      "        4.0       0.21      0.18      0.19       290\n",
      "        7.0       0.16      0.07      0.10       268\n",
      "        3.0       0.23      0.23      0.23       459\n",
      "        6.0       0.17      0.08      0.11       397\n",
      "        2.0       0.30      0.50      0.37       536\n",
      "        9.0       0.40      0.27      0.32       373\n",
      "\n",
      "avg / total       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3251890634089587\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.80      0.61       624\n",
      "        8.0       0.18      0.05      0.07       256\n",
      "        1.0       0.27      0.08      0.12       248\n",
      "        4.0       0.24      0.17      0.20       306\n",
      "        7.0       0.15      0.04      0.06       261\n",
      "        3.0       0.22      0.34      0.27       410\n",
      "        6.0       0.16      0.08      0.11       402\n",
      "        2.0       0.34      0.47      0.39       561\n",
      "        9.0       0.39      0.39      0.39       370\n",
      "\n",
      "avg / total       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.34060500290866785\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.78      0.63       643\n",
      "        8.0       0.16      0.14      0.15       240\n",
      "        1.0       0.18      0.11      0.13       252\n",
      "        4.0       0.19      0.12      0.15       293\n",
      "        7.0       0.20      0.06      0.09       278\n",
      "        3.0       0.21      0.24      0.22       428\n",
      "        6.0       0.16      0.10      0.12       388\n",
      "        2.0       0.32      0.50      0.39       550\n",
      "        9.0       0.40      0.31      0.35       366\n",
      "\n",
      "avg / total       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3324607329842932\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.85      0.61       601\n",
      "        8.0       0.21      0.07      0.10       273\n",
      "        1.0       0.20      0.07      0.11       234\n",
      "        4.0       0.21      0.15      0.18       303\n",
      "        7.0       0.20      0.07      0.10       251\n",
      "        3.0       0.22      0.30      0.26       441\n",
      "        6.0       0.19      0.08      0.12       411\n",
      "        2.0       0.31      0.46      0.37       547\n",
      "        9.0       0.38      0.32      0.34       377\n",
      "\n",
      "avg / total       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.79      0.62       628\n",
      "        8.0       0.15      0.08      0.11       244\n",
      "        1.0       0.20      0.11      0.14       238\n",
      "        4.0       0.24      0.19      0.21       293\n",
      "        7.0       0.17      0.04      0.06       278\n",
      "        3.0       0.25      0.24      0.24       454\n",
      "        6.0       0.18      0.11      0.14       390\n",
      "        2.0       0.31      0.53      0.39       545\n",
      "        9.0       0.41      0.32      0.36       368\n",
      "\n",
      "avg / total       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.33973240255962767\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.80      0.60       616\n",
      "        8.0       0.16      0.08      0.11       269\n",
      "        1.0       0.23      0.07      0.11       248\n",
      "        4.0       0.25      0.14      0.18       303\n",
      "        7.0       0.14      0.07      0.09       251\n",
      "        3.0       0.21      0.30      0.25       415\n",
      "        6.0       0.14      0.06      0.09       409\n",
      "        2.0       0.30      0.43      0.35       552\n",
      "        9.0       0.37      0.35      0.36       375\n",
      "\n",
      "avg / total       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3219895287958115\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.80      0.61       617\n",
      "        8.0       0.18      0.06      0.09       267\n",
      "        1.0       0.14      0.09      0.11       233\n",
      "        4.0       0.22      0.12      0.15       310\n",
      "        7.0       0.17      0.07      0.10       249\n",
      "        3.0       0.23      0.30      0.26       441\n",
      "        6.0       0.16      0.09      0.11       392\n",
      "        2.0       0.31      0.47      0.37       546\n",
      "        9.0       0.44      0.35      0.39       383\n",
      "\n",
      "avg / total       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3321698662012798\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.82      0.61       627\n",
      "        8.0       0.13      0.07      0.09       246\n",
      "        1.0       0.25      0.06      0.10       253\n",
      "        4.0       0.18      0.17      0.18       286\n",
      "        7.0       0.13      0.03      0.04       280\n",
      "        3.0       0.22      0.26      0.24       428\n",
      "        6.0       0.19      0.10      0.13       407\n",
      "        2.0       0.32      0.50      0.39       551\n",
      "        9.0       0.41      0.35      0.38       360\n",
      "\n",
      "avg / total       0.29      0.34      0.29      3438\n",
      "\n",
      "accuracy:  0.33507853403141363\n",
      "mean accuracy 0.3320826061663758\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model7 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.81      0.63       634\n",
      "        8.0       0.14      0.04      0.06       262\n",
      "        1.0       0.20      0.08      0.12       231\n",
      "        4.0       0.19      0.10      0.13       312\n",
      "        7.0       0.07      0.04      0.05       255\n",
      "        3.0       0.20      0.34      0.25       420\n",
      "        6.0       0.18      0.03      0.06       420\n",
      "        2.0       0.30      0.50      0.37       538\n",
      "        9.0       0.41      0.32      0.36       366\n",
      "\n",
      "avg / total       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.47      0.82      0.60       610\n",
      "        8.0       0.17      0.04      0.06       251\n",
      "        1.0       0.32      0.03      0.05       255\n",
      "        4.0       0.21      0.22      0.22       284\n",
      "        7.0       0.17      0.02      0.04       274\n",
      "        3.0       0.24      0.20      0.22       449\n",
      "        6.0       0.13      0.10      0.11       379\n",
      "        2.0       0.30      0.53      0.38       559\n",
      "        9.0       0.40      0.34      0.37       377\n",
      "\n",
      "avg / total       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.33042466550319954\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.81      0.63       640\n",
      "        8.0       0.24      0.05      0.08       259\n",
      "        1.0       0.18      0.03      0.06       259\n",
      "        4.0       0.18      0.15      0.17       288\n",
      "        7.0       0.07      0.00      0.01       264\n",
      "        3.0       0.19      0.24      0.21       432\n",
      "        6.0       0.13      0.09      0.11       384\n",
      "        2.0       0.30      0.52      0.38       555\n",
      "        9.0       0.37      0.32      0.34       357\n",
      "\n",
      "avg / total       0.27      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.84      0.61       604\n",
      "        8.0       0.14      0.02      0.03       254\n",
      "        1.0       0.17      0.08      0.11       227\n",
      "        4.0       0.23      0.15      0.18       308\n",
      "        7.0       0.19      0.06      0.09       265\n",
      "        3.0       0.22      0.29      0.25       437\n",
      "        6.0       0.15      0.03      0.06       415\n",
      "        2.0       0.28      0.50      0.36       542\n",
      "        9.0       0.40      0.31      0.35       386\n",
      "\n",
      "avg / total       0.28      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32693426410703896\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.84      0.61       603\n",
      "        8.0       0.09      0.03      0.05       252\n",
      "        1.0       0.21      0.04      0.07       233\n",
      "        4.0       0.21      0.18      0.19       293\n",
      "        7.0       0.11      0.01      0.01       273\n",
      "        3.0       0.19      0.28      0.22       420\n",
      "        6.0       0.20      0.02      0.04       446\n",
      "        2.0       0.29      0.56      0.38       542\n",
      "        9.0       0.43      0.32      0.36       376\n",
      "\n",
      "avg / total       0.27      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3275159976730657\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.81      0.62       641\n",
      "        8.0       0.16      0.02      0.04       261\n",
      "        1.0       0.18      0.05      0.08       253\n",
      "        4.0       0.22      0.14      0.17       303\n",
      "        7.0       0.12      0.04      0.06       256\n",
      "        3.0       0.21      0.23      0.22       449\n",
      "        6.0       0.14      0.15      0.15       353\n",
      "        2.0       0.29      0.44      0.35       555\n",
      "        9.0       0.38      0.32      0.35       367\n",
      "\n",
      "avg / total       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3228621291448517\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.82      0.63       621\n",
      "        8.0       0.14      0.08      0.10       217\n",
      "        1.0       0.25      0.03      0.06       262\n",
      "        4.0       0.22      0.14      0.17       300\n",
      "        7.0       0.10      0.01      0.03       267\n",
      "        3.0       0.20      0.27      0.23       429\n",
      "        6.0       0.15      0.06      0.08       404\n",
      "        2.0       0.28      0.54      0.37       554\n",
      "        9.0       0.40      0.28      0.33       384\n",
      "\n",
      "avg / total       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.47      0.82      0.60       623\n",
      "        8.0       0.05      0.00      0.01       296\n",
      "        1.0       0.13      0.05      0.07       224\n",
      "        4.0       0.23      0.15      0.18       296\n",
      "        7.0       0.11      0.03      0.05       262\n",
      "        3.0       0.21      0.28      0.24       440\n",
      "        6.0       0.15      0.07      0.09       395\n",
      "        2.0       0.31      0.51      0.38       543\n",
      "        9.0       0.40      0.35      0.37       359\n",
      "\n",
      "avg / total       0.26      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3275159976730657\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.80      0.61       629\n",
      "        8.0       0.14      0.02      0.03       265\n",
      "        1.0       0.19      0.09      0.12       227\n",
      "        4.0       0.18      0.12      0.14       295\n",
      "        7.0       0.11      0.02      0.04       249\n",
      "        3.0       0.22      0.25      0.24       450\n",
      "        6.0       0.19      0.11      0.14       391\n",
      "        2.0       0.29      0.53      0.37       547\n",
      "        9.0       0.40      0.30      0.34       385\n",
      "\n",
      "avg / total       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.84      0.62       615\n",
      "        8.0       0.18      0.06      0.09       248\n",
      "        1.0       0.16      0.03      0.06       259\n",
      "        4.0       0.26      0.17      0.21       301\n",
      "        7.0       0.06      0.01      0.01       280\n",
      "        3.0       0.19      0.27      0.22       419\n",
      "        6.0       0.14      0.04      0.07       408\n",
      "        2.0       0.28      0.49      0.36       550\n",
      "        9.0       0.35      0.32      0.33       358\n",
      "\n",
      "avg / total       0.26      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.32257126236183825\n",
      "mean accuracy 0.32675974403723096\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model8 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.83      0.63       635\n",
      "        8.0       0.15      0.03      0.05       258\n",
      "        1.0       0.18      0.02      0.04       253\n",
      "        4.0       0.17      0.25      0.20       285\n",
      "        7.0       0.09      0.01      0.02       243\n",
      "        3.0       0.21      0.18      0.20       438\n",
      "        6.0       0.14      0.05      0.08       409\n",
      "        2.0       0.27      0.49      0.35       557\n",
      "        9.0       0.36      0.36      0.36       360\n",
      "\n",
      "avg / total       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.47      0.80      0.59       609\n",
      "        8.0       0.19      0.01      0.02       255\n",
      "        1.0       0.31      0.03      0.06       233\n",
      "        4.0       0.23      0.09      0.13       311\n",
      "        7.0       0.14      0.02      0.04       286\n",
      "        3.0       0.20      0.32      0.25       431\n",
      "        6.0       0.22      0.07      0.11       390\n",
      "        2.0       0.28      0.60      0.38       540\n",
      "        9.0       0.41      0.23      0.29       383\n",
      "\n",
      "avg / total       0.29      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3228621291448517\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.81      0.61       617\n",
      "        8.0       0.31      0.06      0.10       258\n",
      "        1.0       0.31      0.03      0.06       229\n",
      "        4.0       0.18      0.18      0.18       305\n",
      "        7.0       0.17      0.05      0.08       250\n",
      "        3.0       0.17      0.14      0.15       458\n",
      "        6.0       0.14      0.09      0.11       382\n",
      "        2.0       0.27      0.49      0.35       569\n",
      "        9.0       0.38      0.33      0.35       370\n",
      "\n",
      "avg / total       0.28      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3164630599185573\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.83      0.62       627\n",
      "        8.0       0.17      0.02      0.03       255\n",
      "        1.0       0.27      0.04      0.06       257\n",
      "        4.0       0.21      0.13      0.16       291\n",
      "        7.0       0.00      0.00      0.00       279\n",
      "        3.0       0.19      0.34      0.25       411\n",
      "        6.0       0.22      0.02      0.03       417\n",
      "        2.0       0.27      0.57      0.36       528\n",
      "        9.0       0.40      0.26      0.31       373\n",
      "\n",
      "avg / total       0.27      0.33      0.26      3438\n",
      "\n",
      "accuracy:  0.3251890634089587\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.81      0.60       609\n",
      "        8.0       0.08      0.01      0.02       250\n",
      "        1.0       0.14      0.03      0.05       237\n",
      "        4.0       0.19      0.15      0.17       306\n",
      "        7.0       0.12      0.00      0.01       263\n",
      "        3.0       0.20      0.25      0.22       445\n",
      "        6.0       0.14      0.06      0.08       410\n",
      "        2.0       0.29      0.57      0.38       546\n",
      "        9.0       0.41      0.31      0.35       372\n",
      "\n",
      "avg / total       0.26      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3222803955788249\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.83      0.63       635\n",
      "        8.0       0.10      0.03      0.05       263\n",
      "        1.0       0.29      0.02      0.03       249\n",
      "        4.0       0.20      0.13      0.16       290\n",
      "        7.0       0.18      0.02      0.04       266\n",
      "        3.0       0.21      0.28      0.24       424\n",
      "        6.0       0.17      0.05      0.08       389\n",
      "        2.0       0.26      0.53      0.35       551\n",
      "        9.0       0.37      0.30      0.33       371\n",
      "\n",
      "avg / total       0.28      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.82      0.61       604\n",
      "        8.0       0.15      0.04      0.06       250\n",
      "        1.0       0.19      0.06      0.09       238\n",
      "        4.0       0.21      0.14      0.16       302\n",
      "        7.0       0.16      0.01      0.03       275\n",
      "        3.0       0.23      0.32      0.27       438\n",
      "        6.0       0.22      0.09      0.12       383\n",
      "        2.0       0.29      0.54      0.38       561\n",
      "        9.0       0.38      0.26      0.31       387\n",
      "\n",
      "avg / total       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.33187899941826643\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.81      0.61       640\n",
      "        8.0       0.19      0.01      0.02       263\n",
      "        1.0       0.40      0.03      0.06       248\n",
      "        4.0       0.19      0.12      0.14       294\n",
      "        7.0       0.21      0.03      0.05       254\n",
      "        3.0       0.20      0.26      0.23       431\n",
      "        6.0       0.17      0.05      0.07       416\n",
      "        2.0       0.27      0.56      0.36       536\n",
      "        9.0       0.37      0.33      0.35       356\n",
      "\n",
      "avg / total       0.29      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.81      0.61       639\n",
      "        8.0       0.19      0.03      0.05       258\n",
      "        1.0       0.15      0.02      0.04       239\n",
      "        4.0       0.21      0.18      0.19       276\n",
      "        7.0       0.07      0.00      0.01       268\n",
      "        3.0       0.23      0.31      0.27       435\n",
      "        6.0       0.13      0.05      0.07       388\n",
      "        2.0       0.29      0.53      0.37       555\n",
      "        9.0       0.36      0.30      0.33       380\n",
      "\n",
      "avg / total       0.27      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.82      0.60       605\n",
      "        8.0       0.17      0.02      0.04       255\n",
      "        1.0       0.39      0.04      0.07       247\n",
      "        4.0       0.26      0.13      0.17       320\n",
      "        7.0       0.16      0.04      0.07       261\n",
      "        3.0       0.20      0.28      0.24       434\n",
      "        6.0       0.15      0.04      0.06       411\n",
      "        2.0       0.27      0.56      0.36       542\n",
      "        9.0       0.43      0.33      0.37       363\n",
      "\n",
      "avg / total       0.29      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32693426410703896\n",
      "mean accuracy 0.32536358347876676\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model9 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31198371146015125, 0.3273414776032577, 0.3285049447353113, 0.32993019197207685, 0.33295520651541594, 0.3285922047702153, 0.3320826061663758, 0.32675974403723096, 0.32536358347876676]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_euclidean\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski and k tuning on 10% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.61      0.56      0.58       658\n",
      "        8.0       0.15      0.18      0.17       241\n",
      "        1.0       0.18      0.17      0.17       240\n",
      "        4.0       0.15      0.16      0.16       281\n",
      "        7.0       0.20      0.19      0.19       256\n",
      "        3.0       0.27      0.27      0.27       430\n",
      "        6.0       0.24      0.23      0.23       386\n",
      "        2.0       0.35      0.36      0.36       577\n",
      "        9.0       0.30      0.33      0.32       369\n",
      "\n",
      "avg / total       0.32      0.31      0.32      3438\n",
      "\n",
      "accuracy:  0.3126817917393834\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.59      0.62      0.60       586\n",
      "        8.0       0.19      0.16      0.17       272\n",
      "        1.0       0.16      0.17      0.16       246\n",
      "        4.0       0.14      0.11      0.12       315\n",
      "        7.0       0.21      0.24      0.23       273\n",
      "        3.0       0.23      0.23      0.23       439\n",
      "        6.0       0.24      0.23      0.24       413\n",
      "        2.0       0.31      0.35      0.33       520\n",
      "        9.0       0.35      0.31      0.33       374\n",
      "\n",
      "avg / total       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.3036649214659686\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.59      0.60      0.60       626\n",
      "        8.0       0.17      0.16      0.17       263\n",
      "        1.0       0.13      0.12      0.12       257\n",
      "        4.0       0.16      0.16      0.16       295\n",
      "        7.0       0.20      0.18      0.19       271\n",
      "        3.0       0.23      0.24      0.24       415\n",
      "        6.0       0.23      0.23      0.23       406\n",
      "        2.0       0.32      0.33      0.33       542\n",
      "        9.0       0.37      0.39      0.38       363\n",
      "\n",
      "avg / total       0.30      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3080279232111693\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.62      0.61      0.62       618\n",
      "        8.0       0.17      0.14      0.16       250\n",
      "        1.0       0.14      0.14      0.14       229\n",
      "        4.0       0.16      0.16      0.16       301\n",
      "        7.0       0.18      0.19      0.19       258\n",
      "        3.0       0.27      0.26      0.27       454\n",
      "        6.0       0.23      0.25      0.24       393\n",
      "        2.0       0.33      0.37      0.35       555\n",
      "        9.0       0.40      0.33      0.37       380\n",
      "\n",
      "avg / total       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3182082606166376\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.60      0.57      0.59       622\n",
      "        8.0       0.18      0.17      0.17       259\n",
      "        1.0       0.14      0.11      0.12       255\n",
      "        4.0       0.16      0.15      0.16       306\n",
      "        7.0       0.18      0.21      0.19       236\n",
      "        3.0       0.25      0.24      0.24       428\n",
      "        6.0       0.22      0.23      0.22       383\n",
      "        2.0       0.33      0.36      0.34       546\n",
      "        9.0       0.33      0.36      0.34       403\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3065735892961024\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.61      0.62      0.62       622\n",
      "        8.0       0.19      0.18      0.19       254\n",
      "        1.0       0.15      0.16      0.16       231\n",
      "        4.0       0.16      0.19      0.17       290\n",
      "        7.0       0.24      0.20      0.22       293\n",
      "        3.0       0.25      0.26      0.26       441\n",
      "        6.0       0.23      0.20      0.21       416\n",
      "        2.0       0.33      0.35      0.34       551\n",
      "        9.0       0.34      0.33      0.33       340\n",
      "\n",
      "avg / total       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3164630599185573\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.59      0.61      0.60       604\n",
      "        8.0       0.17      0.16      0.17       258\n",
      "        1.0       0.18      0.16      0.17       243\n",
      "        4.0       0.17      0.16      0.16       295\n",
      "        7.0       0.19      0.21      0.20       253\n",
      "        3.0       0.26      0.24      0.25       448\n",
      "        6.0       0.19      0.21      0.20       387\n",
      "        2.0       0.34      0.36      0.35       558\n",
      "        9.0       0.38      0.35      0.37       392\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.31326352530541013\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.63      0.60      0.62       640\n",
      "        8.0       0.19      0.16      0.18       255\n",
      "        1.0       0.13      0.10      0.11       243\n",
      "        4.0       0.17      0.18      0.17       301\n",
      "        7.0       0.23      0.22      0.22       276\n",
      "        3.0       0.24      0.24      0.24       421\n",
      "        6.0       0.24      0.27      0.25       412\n",
      "        2.0       0.30      0.31      0.31       539\n",
      "        9.0       0.31      0.35      0.33       351\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3112274578243165\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.61      0.58      0.60       630\n",
      "        8.0       0.20      0.19      0.20       250\n",
      "        1.0       0.15      0.15      0.15       226\n",
      "        4.0       0.18      0.17      0.17       302\n",
      "        7.0       0.20      0.19      0.19       290\n",
      "        3.0       0.27      0.27      0.27       443\n",
      "        6.0       0.24      0.24      0.24       379\n",
      "        2.0       0.30      0.35      0.32       534\n",
      "        9.0       0.36      0.34      0.35       384\n",
      "\n",
      "avg / total       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3150087260034904\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.60      0.64      0.62       614\n",
      "        8.0       0.21      0.18      0.19       263\n",
      "        1.0       0.14      0.14      0.14       260\n",
      "        4.0       0.14      0.15      0.15       294\n",
      "        7.0       0.19      0.19      0.19       239\n",
      "        3.0       0.26      0.25      0.25       426\n",
      "        6.0       0.26      0.24      0.25       420\n",
      "        2.0       0.30      0.31      0.30       563\n",
      "        9.0       0.33      0.33      0.33       359\n",
      "\n",
      "avg / total       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3106457242582897\n",
      "mean accuracy 0.31157649796393255\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_minkowski = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.73      0.61       639\n",
      "        8.0       0.19      0.18      0.19       256\n",
      "        1.0       0.18      0.19      0.18       244\n",
      "        4.0       0.17      0.13      0.15       318\n",
      "        7.0       0.21      0.18      0.20       266\n",
      "        3.0       0.27      0.25      0.26       436\n",
      "        6.0       0.24      0.22      0.23       380\n",
      "        2.0       0.33      0.39      0.36       528\n",
      "        9.0       0.46      0.26      0.33       371\n",
      "\n",
      "avg / total       0.32      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.72      0.59       605\n",
      "        8.0       0.16      0.18      0.17       257\n",
      "        1.0       0.14      0.12      0.13       242\n",
      "        4.0       0.14      0.13      0.14       278\n",
      "        7.0       0.24      0.15      0.19       263\n",
      "        3.0       0.22      0.21      0.21       433\n",
      "        6.0       0.25      0.18      0.21       419\n",
      "        2.0       0.38      0.39      0.38       569\n",
      "        9.0       0.45      0.38      0.41       372\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3254799301919721\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.73      0.61       657\n",
      "        8.0       0.23      0.22      0.23       282\n",
      "        1.0       0.16      0.16      0.16       225\n",
      "        4.0       0.19      0.16      0.17       291\n",
      "        7.0       0.19      0.17      0.18       270\n",
      "        3.0       0.28      0.25      0.26       427\n",
      "        6.0       0.24      0.21      0.23       376\n",
      "        2.0       0.36      0.35      0.36       561\n",
      "        9.0       0.38      0.33      0.35       349\n",
      "\n",
      "avg / total       0.32      0.34      0.33      3438\n",
      "\n",
      "accuracy:  0.33885980221058754\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.72      0.59       587\n",
      "        8.0       0.17      0.19      0.18       231\n",
      "        1.0       0.16      0.12      0.14       261\n",
      "        4.0       0.18      0.16      0.17       305\n",
      "        7.0       0.19      0.14      0.16       259\n",
      "        3.0       0.24      0.24      0.24       442\n",
      "        6.0       0.27      0.20      0.23       423\n",
      "        2.0       0.32      0.43      0.37       536\n",
      "        9.0       0.47      0.29      0.36       394\n",
      "\n",
      "avg / total       0.31      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.69      0.60       616\n",
      "        8.0       0.16      0.20      0.18       236\n",
      "        1.0       0.15      0.16      0.15       236\n",
      "        4.0       0.16      0.16      0.16       288\n",
      "        7.0       0.23      0.18      0.20       260\n",
      "        3.0       0.27      0.24      0.25       446\n",
      "        6.0       0.26      0.22      0.24       396\n",
      "        2.0       0.36      0.36      0.36       577\n",
      "        9.0       0.42      0.30      0.35       383\n",
      "\n",
      "avg / total       0.32      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.76      0.61       628\n",
      "        8.0       0.18      0.16      0.17       277\n",
      "        1.0       0.15      0.12      0.13       250\n",
      "        4.0       0.18      0.14      0.16       308\n",
      "        7.0       0.19      0.13      0.15       269\n",
      "        3.0       0.25      0.26      0.26       423\n",
      "        6.0       0.25      0.20      0.23       403\n",
      "        2.0       0.33      0.36      0.34       520\n",
      "        9.0       0.37      0.29      0.32       360\n",
      "\n",
      "avg / total       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3240255962769052\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.54      0.71      0.61       654\n",
      "        8.0       0.20      0.19      0.19       251\n",
      "        1.0       0.14      0.12      0.13       242\n",
      "        4.0       0.18      0.16      0.17       294\n",
      "        7.0       0.18      0.18      0.18       251\n",
      "        3.0       0.26      0.24      0.25       431\n",
      "        6.0       0.22      0.18      0.20       388\n",
      "        2.0       0.35      0.37      0.36       559\n",
      "        9.0       0.42      0.35      0.38       368\n",
      "\n",
      "avg / total       0.31      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.33275159976730656\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.47      0.74      0.58       590\n",
      "        8.0       0.17      0.16      0.17       262\n",
      "        1.0       0.14      0.12      0.13       244\n",
      "        4.0       0.20      0.17      0.18       302\n",
      "        7.0       0.20      0.15      0.17       278\n",
      "        3.0       0.26      0.26      0.26       438\n",
      "        6.0       0.29      0.20      0.23       411\n",
      "        2.0       0.33      0.36      0.34       538\n",
      "        9.0       0.42      0.33      0.37       375\n",
      "\n",
      "avg / total       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3216986620127981\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.74      0.61       631\n",
      "        8.0       0.13      0.14      0.14       242\n",
      "        1.0       0.19      0.15      0.17       239\n",
      "        4.0       0.19      0.15      0.17       312\n",
      "        7.0       0.19      0.16      0.17       259\n",
      "        3.0       0.23      0.24      0.23       436\n",
      "        6.0       0.26      0.18      0.21       417\n",
      "        2.0       0.34      0.41      0.37       532\n",
      "        9.0       0.45      0.29      0.36       370\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.73      0.60       613\n",
      "        8.0       0.19      0.15      0.17       271\n",
      "        1.0       0.13      0.12      0.13       247\n",
      "        4.0       0.19      0.20      0.20       284\n",
      "        7.0       0.21      0.16      0.18       270\n",
      "        3.0       0.28      0.26      0.27       433\n",
      "        6.0       0.27      0.23      0.25       382\n",
      "        2.0       0.40      0.40      0.40       565\n",
      "        9.0       0.40      0.33      0.36       373\n",
      "\n",
      "avg / total       0.32      0.34      0.33      3438\n",
      "\n",
      "accuracy:  0.34002326934264104\n",
      "mean accuracy 0.3295520651541594\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model11 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.75      0.61       629\n",
      "        8.0       0.16      0.10      0.13       263\n",
      "        1.0       0.13      0.12      0.13       218\n",
      "        4.0       0.15      0.13      0.14       290\n",
      "        7.0       0.15      0.10      0.12       268\n",
      "        3.0       0.23      0.27      0.25       431\n",
      "        6.0       0.22      0.17      0.19       396\n",
      "        2.0       0.34      0.39      0.36       560\n",
      "        9.0       0.43      0.32      0.37       383\n",
      "\n",
      "avg / total       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32344386271087844\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.75      0.61       615\n",
      "        8.0       0.17      0.16      0.17       250\n",
      "        1.0       0.20      0.11      0.14       268\n",
      "        4.0       0.20      0.19      0.20       306\n",
      "        7.0       0.20      0.16      0.18       261\n",
      "        3.0       0.24      0.25      0.24       438\n",
      "        6.0       0.22      0.15      0.18       403\n",
      "        2.0       0.33      0.41      0.36       537\n",
      "        9.0       0.41      0.33      0.36       360\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.73      0.60       638\n",
      "        8.0       0.21      0.16      0.18       253\n",
      "        1.0       0.15      0.12      0.13       240\n",
      "        4.0       0.17      0.14      0.15       309\n",
      "        7.0       0.18      0.12      0.14       278\n",
      "        3.0       0.23      0.24      0.23       431\n",
      "        6.0       0.24      0.16      0.19       398\n",
      "        2.0       0.33      0.40      0.36       543\n",
      "        9.0       0.35      0.33      0.34       348\n",
      "\n",
      "avg / total       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.74      0.60       606\n",
      "        8.0       0.22      0.20      0.21       260\n",
      "        1.0       0.12      0.09      0.10       246\n",
      "        4.0       0.15      0.16      0.15       287\n",
      "        7.0       0.17      0.13      0.15       251\n",
      "        3.0       0.25      0.27      0.26       438\n",
      "        6.0       0.24      0.18      0.21       401\n",
      "        2.0       0.36      0.41      0.38       554\n",
      "        9.0       0.51      0.29      0.37       395\n",
      "\n",
      "avg / total       0.32      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32897033158813266\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.74      0.61       632\n",
      "        8.0       0.24      0.19      0.21       257\n",
      "        1.0       0.20      0.15      0.17       251\n",
      "        4.0       0.17      0.12      0.14       319\n",
      "        7.0       0.22      0.15      0.18       268\n",
      "        3.0       0.21      0.29      0.24       401\n",
      "        6.0       0.22      0.16      0.19       383\n",
      "        2.0       0.35      0.41      0.38       553\n",
      "        9.0       0.40      0.28      0.33       374\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33304246655031994\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.76      0.61       612\n",
      "        8.0       0.18      0.14      0.16       256\n",
      "        1.0       0.13      0.11      0.12       235\n",
      "        4.0       0.20      0.19      0.20       277\n",
      "        7.0       0.21      0.15      0.17       261\n",
      "        3.0       0.27      0.24      0.25       468\n",
      "        6.0       0.23      0.16      0.19       416\n",
      "        2.0       0.31      0.38      0.34       544\n",
      "        9.0       0.36      0.30      0.33       369\n",
      "\n",
      "avg / total       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.74      0.61       619\n",
      "        8.0       0.20      0.15      0.17       256\n",
      "        1.0       0.15      0.10      0.12       242\n",
      "        4.0       0.17      0.15      0.16       289\n",
      "        7.0       0.20      0.11      0.15       272\n",
      "        3.0       0.25      0.27      0.26       448\n",
      "        6.0       0.24      0.18      0.21       400\n",
      "        2.0       0.32      0.40      0.36       555\n",
      "        9.0       0.40      0.35      0.37       357\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3298429319371728\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.76      0.62       625\n",
      "        8.0       0.23      0.19      0.21       257\n",
      "        1.0       0.10      0.08      0.09       244\n",
      "        4.0       0.19      0.16      0.17       307\n",
      "        7.0       0.18      0.16      0.17       257\n",
      "        3.0       0.25      0.27      0.26       421\n",
      "        6.0       0.30      0.20      0.24       399\n",
      "        2.0       0.35      0.42      0.38       542\n",
      "        9.0       0.45      0.32      0.38       386\n",
      "\n",
      "avg / total       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.341477603257708\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.77      0.59       600\n",
      "        8.0       0.22      0.18      0.20       252\n",
      "        1.0       0.23      0.12      0.16       249\n",
      "        4.0       0.18      0.16      0.17       301\n",
      "        7.0       0.19      0.17      0.18       259\n",
      "        3.0       0.26      0.21      0.23       458\n",
      "        6.0       0.25      0.20      0.23       392\n",
      "        2.0       0.33      0.40      0.36       545\n",
      "        9.0       0.43      0.31      0.36       382\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33187899941826643\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.57      0.74      0.64       644\n",
      "        8.0       0.19      0.15      0.17       261\n",
      "        1.0       0.17      0.14      0.15       237\n",
      "        4.0       0.20      0.17      0.19       295\n",
      "        7.0       0.18      0.11      0.14       270\n",
      "        3.0       0.22      0.29      0.25       411\n",
      "        6.0       0.24      0.18      0.20       407\n",
      "        2.0       0.35      0.39      0.37       552\n",
      "        9.0       0.37      0.31      0.34       361\n",
      "\n",
      "avg / total       0.31      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.33478766724840026\n",
      "mean accuracy 0.3301628853984876\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model12 = sum(acc)/10 \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.74      0.59       609\n",
      "        8.0       0.16      0.12      0.13       268\n",
      "        1.0       0.12      0.08      0.10       243\n",
      "        4.0       0.17      0.16      0.17       293\n",
      "        7.0       0.19      0.10      0.13       260\n",
      "        3.0       0.22      0.26      0.24       432\n",
      "        6.0       0.21      0.11      0.15       409\n",
      "        2.0       0.34      0.41      0.37       556\n",
      "        9.0       0.36      0.32      0.34       368\n",
      "\n",
      "avg / total       0.28      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.31297265852239675\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.76      0.62       635\n",
      "        8.0       0.17      0.12      0.14       245\n",
      "        1.0       0.19      0.11      0.14       243\n",
      "        4.0       0.19      0.16      0.17       303\n",
      "        7.0       0.14      0.12      0.13       269\n",
      "        3.0       0.24      0.24      0.24       437\n",
      "        6.0       0.20      0.18      0.19       390\n",
      "        2.0       0.32      0.40      0.36       541\n",
      "        9.0       0.41      0.30      0.35       375\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.77      0.61       621\n",
      "        8.0       0.16      0.10      0.13       264\n",
      "        1.0       0.16      0.11      0.13       246\n",
      "        4.0       0.21      0.15      0.17       297\n",
      "        7.0       0.11      0.09      0.10       248\n",
      "        3.0       0.24      0.26      0.25       437\n",
      "        6.0       0.21      0.14      0.17       420\n",
      "        2.0       0.33      0.42      0.37       539\n",
      "        9.0       0.38      0.32      0.35       366\n",
      "\n",
      "avg / total       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32344386271087844\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.74      0.61       623\n",
      "        8.0       0.18      0.13      0.15       249\n",
      "        1.0       0.15      0.09      0.11       240\n",
      "        4.0       0.15      0.12      0.13       299\n",
      "        7.0       0.19      0.11      0.14       281\n",
      "        3.0       0.23      0.27      0.25       432\n",
      "        6.0       0.22      0.19      0.20       379\n",
      "        2.0       0.33      0.42      0.37       558\n",
      "        9.0       0.43      0.32      0.37       377\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.55      0.73      0.63       637\n",
      "        8.0       0.16      0.16      0.16       230\n",
      "        1.0       0.18      0.10      0.13       249\n",
      "        4.0       0.18      0.16      0.17       292\n",
      "        7.0       0.17      0.13      0.15       258\n",
      "        3.0       0.22      0.23      0.23       453\n",
      "        6.0       0.23      0.15      0.18       410\n",
      "        2.0       0.32      0.43      0.37       532\n",
      "        9.0       0.40      0.32      0.35       377\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32693426410703896\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.76      0.59       607\n",
      "        8.0       0.24      0.14      0.18       283\n",
      "        1.0       0.14      0.12      0.13       237\n",
      "        4.0       0.19      0.13      0.16       304\n",
      "        7.0       0.15      0.07      0.10       271\n",
      "        3.0       0.22      0.26      0.24       416\n",
      "        6.0       0.21      0.17      0.19       389\n",
      "        2.0       0.35      0.38      0.37       565\n",
      "        9.0       0.37      0.32      0.34       366\n",
      "\n",
      "avg / total       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3205351948807446\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.76      0.62       636\n",
      "        8.0       0.18      0.12      0.14       256\n",
      "        1.0       0.19      0.09      0.12       258\n",
      "        4.0       0.18      0.15      0.16       282\n",
      "        7.0       0.18      0.16      0.17       250\n",
      "        3.0       0.23      0.28      0.25       429\n",
      "        6.0       0.25      0.12      0.16       424\n",
      "        2.0       0.34      0.43      0.38       534\n",
      "        9.0       0.42      0.37      0.39       369\n",
      "\n",
      "avg / total       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3379872018615474\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.72      0.59       608\n",
      "        8.0       0.18      0.16      0.17       257\n",
      "        1.0       0.16      0.11      0.13       228\n",
      "        4.0       0.21      0.16      0.18       314\n",
      "        7.0       0.22      0.14      0.17       279\n",
      "        3.0       0.25      0.26      0.26       440\n",
      "        6.0       0.21      0.19      0.20       375\n",
      "        2.0       0.34      0.42      0.38       563\n",
      "        9.0       0.42      0.34      0.38       374\n",
      "\n",
      "avg / total       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.55      0.74      0.63       633\n",
      "        8.0       0.20      0.14      0.16       263\n",
      "        1.0       0.16      0.08      0.11       248\n",
      "        4.0       0.18      0.17      0.17       296\n",
      "        7.0       0.16      0.09      0.11       266\n",
      "        3.0       0.23      0.32      0.27       421\n",
      "        6.0       0.27      0.19      0.22       405\n",
      "        2.0       0.34      0.44      0.38       547\n",
      "        9.0       0.41      0.35      0.37       359\n",
      "\n",
      "avg / total       0.31      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.3403141361256545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.76      0.60       611\n",
      "        8.0       0.22      0.17      0.19       250\n",
      "        1.0       0.14      0.12      0.13       238\n",
      "        4.0       0.22      0.17      0.19       300\n",
      "        7.0       0.16      0.11      0.13       263\n",
      "        3.0       0.24      0.24      0.24       448\n",
      "        6.0       0.20      0.16      0.18       394\n",
      "        2.0       0.32      0.37      0.34       550\n",
      "        9.0       0.41      0.33      0.36       384\n",
      "\n",
      "avg / total       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32460732984293195\n",
      "mean accuracy 0.3272542175683537\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model11 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.78      0.63       632\n",
      "        8.0       0.19      0.18      0.18       238\n",
      "        1.0       0.20      0.14      0.16       247\n",
      "        4.0       0.18      0.12      0.14       321\n",
      "        7.0       0.16      0.10      0.13       263\n",
      "        3.0       0.20      0.20      0.20       449\n",
      "        6.0       0.22      0.14      0.17       390\n",
      "        2.0       0.31      0.44      0.36       527\n",
      "        9.0       0.43      0.34      0.38       371\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3307155322862129\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.76      0.61       612\n",
      "        8.0       0.19      0.10      0.13       275\n",
      "        1.0       0.19      0.10      0.13       239\n",
      "        4.0       0.17      0.19      0.18       275\n",
      "        7.0       0.19      0.10      0.13       266\n",
      "        3.0       0.21      0.25      0.23       420\n",
      "        6.0       0.20      0.14      0.16       409\n",
      "        2.0       0.35      0.48      0.40       570\n",
      "        9.0       0.44      0.29      0.35       372\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3312972658522397\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.74      0.60       625\n",
      "        8.0       0.21      0.18      0.19       240\n",
      "        1.0       0.16      0.08      0.10       234\n",
      "        4.0       0.20      0.19      0.19       293\n",
      "        7.0       0.22      0.10      0.14       265\n",
      "        3.0       0.24      0.26      0.25       433\n",
      "        6.0       0.20      0.12      0.15       424\n",
      "        2.0       0.33      0.45      0.38       554\n",
      "        9.0       0.39      0.30      0.34       370\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.74      0.61       619\n",
      "        8.0       0.19      0.10      0.13       273\n",
      "        1.0       0.17      0.13      0.15       252\n",
      "        4.0       0.24      0.16      0.19       303\n",
      "        7.0       0.15      0.09      0.12       264\n",
      "        3.0       0.22      0.26      0.24       436\n",
      "        6.0       0.23      0.16      0.19       375\n",
      "        2.0       0.33      0.44      0.37       543\n",
      "        9.0       0.42      0.35      0.38       373\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33187899941826643\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.72      0.61       631\n",
      "        8.0       0.15      0.15      0.15       244\n",
      "        1.0       0.18      0.12      0.15       223\n",
      "        4.0       0.17      0.09      0.12       313\n",
      "        7.0       0.12      0.08      0.10       249\n",
      "        3.0       0.24      0.24      0.24       445\n",
      "        6.0       0.22      0.16      0.18       418\n",
      "        2.0       0.32      0.44      0.37       554\n",
      "        9.0       0.43      0.39      0.41       361\n",
      "\n",
      "avg / total       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3278068644560791\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.79      0.59       613\n",
      "        8.0       0.22      0.13      0.16       269\n",
      "        1.0       0.21      0.11      0.15       263\n",
      "        4.0       0.19      0.19      0.19       283\n",
      "        7.0       0.18      0.06      0.09       280\n",
      "        3.0       0.22      0.27      0.25       424\n",
      "        6.0       0.22      0.16      0.19       381\n",
      "        2.0       0.33      0.41      0.37       543\n",
      "        9.0       0.42      0.27      0.33       382\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3275159976730657\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.78      0.62       612\n",
      "        8.0       0.16      0.11      0.13       254\n",
      "        1.0       0.18      0.12      0.14       240\n",
      "        4.0       0.20      0.13      0.16       317\n",
      "        7.0       0.13      0.09      0.10       258\n",
      "        3.0       0.25      0.27      0.26       442\n",
      "        6.0       0.22      0.14      0.17       391\n",
      "        2.0       0.31      0.41      0.36       558\n",
      "        9.0       0.37      0.33      0.35       366\n",
      "\n",
      "avg / total       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.75      0.60       632\n",
      "        8.0       0.17      0.12      0.14       259\n",
      "        1.0       0.21      0.12      0.15       246\n",
      "        4.0       0.19      0.16      0.17       279\n",
      "        7.0       0.18      0.10      0.13       271\n",
      "        3.0       0.21      0.23      0.22       427\n",
      "        6.0       0.23      0.14      0.17       408\n",
      "        2.0       0.33      0.47      0.39       539\n",
      "        9.0       0.40      0.31      0.35       377\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.72      0.59       618\n",
      "        8.0       0.15      0.13      0.14       247\n",
      "        1.0       0.15      0.13      0.14       221\n",
      "        4.0       0.19      0.12      0.14       312\n",
      "        7.0       0.22      0.10      0.14       274\n",
      "        3.0       0.24      0.28      0.26       438\n",
      "        6.0       0.20      0.11      0.14       404\n",
      "        2.0       0.32      0.47      0.38       550\n",
      "        9.0       0.38      0.33      0.36       374\n",
      "\n",
      "avg / total       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.77      0.60       626\n",
      "        8.0       0.19      0.10      0.13       266\n",
      "        1.0       0.20      0.08      0.12       265\n",
      "        4.0       0.20      0.18      0.19       284\n",
      "        7.0       0.16      0.10      0.12       255\n",
      "        3.0       0.23      0.25      0.24       431\n",
      "        6.0       0.19      0.14      0.16       395\n",
      "        2.0       0.34      0.44      0.38       547\n",
      "        9.0       0.44      0.34      0.38       369\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33187899941826643\n",
      "mean accuracy 0.32908667830133803\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model13 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.77      0.62       617\n",
      "        8.0       0.22      0.16      0.19       244\n",
      "        1.0       0.21      0.08      0.12       243\n",
      "        4.0       0.20      0.12      0.15       311\n",
      "        7.0       0.16      0.06      0.08       284\n",
      "        3.0       0.21      0.29      0.25       428\n",
      "        6.0       0.21      0.14      0.17       402\n",
      "        2.0       0.31      0.45      0.37       543\n",
      "        9.0       0.41      0.35      0.38       366\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33187899941826643\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.79      0.61       627\n",
      "        8.0       0.14      0.06      0.09       269\n",
      "        1.0       0.18      0.13      0.15       243\n",
      "        4.0       0.19      0.18      0.18       285\n",
      "        7.0       0.13      0.09      0.10       245\n",
      "        3.0       0.23      0.22      0.22       441\n",
      "        6.0       0.21      0.10      0.14       397\n",
      "        2.0       0.31      0.45      0.36       554\n",
      "        9.0       0.39      0.30      0.34       377\n",
      "\n",
      "avg / total       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.77      0.60       610\n",
      "        8.0       0.15      0.12      0.14       255\n",
      "        1.0       0.20      0.09      0.12       252\n",
      "        4.0       0.17      0.10      0.12       298\n",
      "        7.0       0.14      0.07      0.09       250\n",
      "        3.0       0.25      0.29      0.27       443\n",
      "        6.0       0.18      0.13      0.15       383\n",
      "        2.0       0.30      0.44      0.36       553\n",
      "        9.0       0.43      0.32      0.37       394\n",
      "\n",
      "avg / total       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.79      0.62       634\n",
      "        8.0       0.18      0.08      0.11       258\n",
      "        1.0       0.17      0.07      0.10       234\n",
      "        4.0       0.25      0.25      0.25       298\n",
      "        7.0       0.17      0.06      0.09       279\n",
      "        3.0       0.23      0.28      0.25       426\n",
      "        6.0       0.21      0.14      0.17       416\n",
      "        2.0       0.31      0.43      0.36       544\n",
      "        9.0       0.37      0.35      0.36       349\n",
      "\n",
      "avg / total       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.337696335078534\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.80      0.60       608\n",
      "        8.0       0.22      0.12      0.16       259\n",
      "        1.0       0.23      0.05      0.09       255\n",
      "        4.0       0.21      0.20      0.20       291\n",
      "        7.0       0.16      0.07      0.10       267\n",
      "        3.0       0.23      0.26      0.24       446\n",
      "        6.0       0.19      0.16      0.18       386\n",
      "        2.0       0.31      0.41      0.35       552\n",
      "        9.0       0.41      0.32      0.36       374\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.77      0.62       636\n",
      "        8.0       0.23      0.15      0.18       254\n",
      "        1.0       0.17      0.15      0.16       231\n",
      "        4.0       0.23      0.15      0.18       305\n",
      "        7.0       0.20      0.07      0.11       262\n",
      "        3.0       0.23      0.27      0.25       423\n",
      "        6.0       0.23      0.10      0.14       413\n",
      "        2.0       0.31      0.46      0.37       545\n",
      "        9.0       0.36      0.32      0.34       369\n",
      "\n",
      "avg / total       0.30      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.33595113438045376\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.79      0.61       621\n",
      "        8.0       0.15      0.07      0.10       274\n",
      "        1.0       0.18      0.09      0.12       240\n",
      "        4.0       0.18      0.13      0.15       305\n",
      "        7.0       0.12      0.06      0.08       262\n",
      "        3.0       0.23      0.32      0.27       418\n",
      "        6.0       0.23      0.12      0.16       410\n",
      "        2.0       0.31      0.43      0.36       540\n",
      "        9.0       0.37      0.33      0.35       368\n",
      "\n",
      "avg / total       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3263525305410122\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.77      0.60       623\n",
      "        8.0       0.21      0.14      0.17       239\n",
      "        1.0       0.19      0.10      0.13       246\n",
      "        4.0       0.20      0.18      0.19       291\n",
      "        7.0       0.15      0.07      0.09       267\n",
      "        3.0       0.24      0.27      0.26       451\n",
      "        6.0       0.22      0.15      0.18       389\n",
      "        2.0       0.32      0.42      0.36       557\n",
      "        9.0       0.39      0.30      0.34       375\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3312972658522397\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.77      0.63       625\n",
      "        8.0       0.19      0.12      0.15       253\n",
      "        1.0       0.18      0.09      0.12       245\n",
      "        4.0       0.22      0.17      0.20       309\n",
      "        7.0       0.14      0.06      0.08       270\n",
      "        3.0       0.23      0.32      0.27       427\n",
      "        6.0       0.17      0.13      0.14       396\n",
      "        2.0       0.32      0.43      0.36       546\n",
      "        9.0       0.41      0.31      0.35       367\n",
      "\n",
      "avg / total       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3310063990692263\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.78      0.59       619\n",
      "        8.0       0.21      0.11      0.14       260\n",
      "        1.0       0.16      0.10      0.12       241\n",
      "        4.0       0.22      0.14      0.17       287\n",
      "        7.0       0.12      0.08      0.10       259\n",
      "        3.0       0.22      0.21      0.22       442\n",
      "        6.0       0.20      0.10      0.13       403\n",
      "        2.0       0.31      0.46      0.37       551\n",
      "        9.0       0.36      0.34      0.35       376\n",
      "\n",
      "avg / total       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3219895287958115\n",
      "mean accuracy 0.32934845840605004\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model14 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.80      0.60       615\n",
      "        8.0       0.18      0.10      0.13       232\n",
      "        1.0       0.20      0.08      0.12       238\n",
      "        4.0       0.21      0.14      0.17       300\n",
      "        7.0       0.12      0.06      0.08       259\n",
      "        3.0       0.21      0.24      0.23       446\n",
      "        6.0       0.18      0.10      0.13       415\n",
      "        2.0       0.32      0.47      0.39       559\n",
      "        9.0       0.43      0.35      0.39       374\n",
      "\n",
      "avg / total       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.83      0.64       629\n",
      "        8.0       0.14      0.04      0.06       281\n",
      "        1.0       0.21      0.10      0.14       248\n",
      "        4.0       0.21      0.15      0.17       296\n",
      "        7.0       0.09      0.02      0.04       270\n",
      "        3.0       0.21      0.29      0.25       423\n",
      "        6.0       0.14      0.06      0.08       384\n",
      "        2.0       0.28      0.48      0.36       538\n",
      "        9.0       0.38      0.31      0.34       369\n",
      "\n",
      "avg / total       0.27      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3278068644560791\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.53      0.77      0.62       658\n",
      "        8.0       0.17      0.14      0.15       240\n",
      "        1.0       0.19      0.09      0.13       246\n",
      "        4.0       0.22      0.12      0.16       299\n",
      "        7.0       0.12      0.02      0.03       267\n",
      "        3.0       0.23      0.30      0.26       436\n",
      "        6.0       0.21      0.16      0.18       394\n",
      "        2.0       0.32      0.46      0.37       546\n",
      "        9.0       0.40      0.31      0.35       352\n",
      "\n",
      "avg / total       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.33740546829552065\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.46      0.84      0.60       586\n",
      "        8.0       0.25      0.07      0.11       273\n",
      "        1.0       0.18      0.07      0.10       240\n",
      "        4.0       0.15      0.12      0.13       297\n",
      "        7.0       0.07      0.03      0.05       262\n",
      "        3.0       0.19      0.24      0.21       433\n",
      "        6.0       0.19      0.08      0.11       405\n",
      "        2.0       0.30      0.44      0.36       551\n",
      "        9.0       0.43      0.34      0.38       391\n",
      "\n",
      "avg / total       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3150087260034904\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.78      0.61       614\n",
      "        8.0       0.16      0.11      0.13       254\n",
      "        1.0       0.14      0.06      0.09       242\n",
      "        4.0       0.20      0.09      0.13       312\n",
      "        7.0       0.07      0.02      0.04       254\n",
      "        3.0       0.23      0.29      0.25       438\n",
      "        6.0       0.18      0.10      0.13       400\n",
      "        2.0       0.31      0.48      0.38       560\n",
      "        9.0       0.41      0.35      0.38       364\n",
      "\n",
      "avg / total       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3266433973240256\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.81      0.62       630\n",
      "        8.0       0.22      0.05      0.08       259\n",
      "        1.0       0.19      0.09      0.13       244\n",
      "        4.0       0.21      0.17      0.19       284\n",
      "        7.0       0.18      0.04      0.07       275\n",
      "        3.0       0.20      0.27      0.23       431\n",
      "        6.0       0.18      0.11      0.13       399\n",
      "        2.0       0.30      0.45      0.36       537\n",
      "        9.0       0.39      0.33      0.35       379\n",
      "\n",
      "avg / total       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3298429319371728\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.80      0.62       624\n",
      "        8.0       0.14      0.09      0.11       245\n",
      "        1.0       0.16      0.09      0.11       233\n",
      "        4.0       0.20      0.09      0.13       295\n",
      "        7.0       0.13      0.05      0.07       251\n",
      "        3.0       0.25      0.27      0.26       444\n",
      "        6.0       0.21      0.11      0.15       408\n",
      "        2.0       0.30      0.51      0.38       556\n",
      "        9.0       0.40      0.32      0.36       382\n",
      "\n",
      "avg / total       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33478766724840026\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.82      0.62       620\n",
      "        8.0       0.26      0.07      0.11       268\n",
      "        1.0       0.14      0.06      0.08       253\n",
      "        4.0       0.22      0.21      0.21       301\n",
      "        7.0       0.14      0.04      0.06       278\n",
      "        3.0       0.20      0.27      0.23       425\n",
      "        6.0       0.18      0.10      0.13       391\n",
      "        2.0       0.31      0.45      0.37       541\n",
      "        9.0       0.39      0.30      0.34       361\n",
      "\n",
      "avg / total       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3263525305410122\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.82      0.62       630\n",
      "        8.0       0.14      0.07      0.09       245\n",
      "        1.0       0.11      0.05      0.07       252\n",
      "        4.0       0.18      0.16      0.17       296\n",
      "        7.0       0.13      0.07      0.09       243\n",
      "        3.0       0.25      0.27      0.26       438\n",
      "        6.0       0.18      0.09      0.12       416\n",
      "        2.0       0.30      0.49      0.38       548\n",
      "        9.0       0.40      0.28      0.33       370\n",
      "\n",
      "avg / total       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.32897033158813266\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.83      0.62       614\n",
      "        8.0       0.18      0.08      0.11       268\n",
      "        1.0       0.11      0.06      0.08       234\n",
      "        4.0       0.19      0.09      0.12       300\n",
      "        7.0       0.22      0.05      0.08       286\n",
      "        3.0       0.21      0.31      0.25       431\n",
      "        6.0       0.16      0.08      0.10       383\n",
      "        2.0       0.31      0.46      0.37       549\n",
      "        9.0       0.37      0.32      0.34       373\n",
      "\n",
      "avg / total       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "mean accuracy 0.3281849912739965\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model15 = sum(acc)/10 \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.81      0.62       612\n",
      "        8.0       0.16      0.07      0.10       244\n",
      "        1.0       0.14      0.03      0.05       242\n",
      "        4.0       0.21      0.12      0.15       310\n",
      "        7.0       0.15      0.01      0.03       289\n",
      "        3.0       0.22      0.30      0.25       435\n",
      "        6.0       0.17      0.07      0.10       390\n",
      "        2.0       0.28      0.57      0.37       529\n",
      "        9.0       0.42      0.24      0.30       387\n",
      "\n",
      "avg / total       0.28      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.83      0.62       632\n",
      "        8.0       0.16      0.03      0.04       269\n",
      "        1.0       0.24      0.06      0.09       244\n",
      "        4.0       0.21      0.16      0.18       286\n",
      "        7.0       0.12      0.04      0.06       240\n",
      "        3.0       0.20      0.25      0.22       434\n",
      "        6.0       0.13      0.06      0.08       409\n",
      "        2.0       0.30      0.46      0.37       568\n",
      "        9.0       0.37      0.39      0.38       356\n",
      "\n",
      "avg / total       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3307155322862129\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.79      0.62       638\n",
      "        8.0       0.14      0.05      0.08       259\n",
      "        1.0       0.28      0.04      0.08       250\n",
      "        4.0       0.17      0.12      0.14       292\n",
      "        7.0       0.11      0.05      0.07       246\n",
      "        3.0       0.21      0.24      0.22       446\n",
      "        6.0       0.15      0.08      0.10       389\n",
      "        2.0       0.29      0.52      0.37       546\n",
      "        9.0       0.37      0.30      0.33       372\n",
      "\n",
      "avg / total       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3216986620127981\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.84      0.61       606\n",
      "        8.0       0.15      0.05      0.08       254\n",
      "        1.0       0.27      0.08      0.13       236\n",
      "        4.0       0.20      0.13      0.16       304\n",
      "        7.0       0.14      0.00      0.01       283\n",
      "        3.0       0.22      0.30      0.25       423\n",
      "        6.0       0.17      0.06      0.08       410\n",
      "        2.0       0.29      0.51      0.37       551\n",
      "        9.0       0.35      0.33      0.34       371\n",
      "\n",
      "avg / total       0.28      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3310063990692263\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.84      0.63       633\n",
      "        8.0       0.19      0.07      0.10       254\n",
      "        1.0       0.08      0.02      0.03       242\n",
      "        4.0       0.20      0.15      0.18       286\n",
      "        7.0       0.11      0.01      0.02       259\n",
      "        3.0       0.22      0.29      0.25       435\n",
      "        6.0       0.16      0.06      0.09       388\n",
      "        2.0       0.32      0.54      0.40       565\n",
      "        9.0       0.41      0.34      0.37       376\n",
      "\n",
      "avg / total       0.28      0.34      0.29      3438\n",
      "\n",
      "accuracy:  0.3435136707388016\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.81      0.61       611\n",
      "        8.0       0.14      0.03      0.05       259\n",
      "        1.0       0.24      0.09      0.13       244\n",
      "        4.0       0.19      0.13      0.16       310\n",
      "        7.0       0.08      0.02      0.04       270\n",
      "        3.0       0.21      0.28      0.24       434\n",
      "        6.0       0.14      0.05      0.07       411\n",
      "        2.0       0.27      0.50      0.35       532\n",
      "        9.0       0.37      0.29      0.32       367\n",
      "\n",
      "avg / total       0.26      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3161721931355439\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.83      0.61       634\n",
      "        8.0       0.22      0.04      0.07       249\n",
      "        1.0       0.29      0.04      0.07       245\n",
      "        4.0       0.20      0.13      0.16       300\n",
      "        7.0       0.21      0.03      0.06       274\n",
      "        3.0       0.20      0.27      0.23       423\n",
      "        6.0       0.14      0.06      0.08       386\n",
      "        2.0       0.30      0.57      0.39       555\n",
      "        9.0       0.49      0.26      0.34       372\n",
      "\n",
      "avg / total       0.30      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.79      0.62       610\n",
      "        8.0       0.14      0.06      0.08       264\n",
      "        1.0       0.23      0.07      0.10       241\n",
      "        4.0       0.19      0.16      0.18       296\n",
      "        7.0       0.12      0.06      0.08       255\n",
      "        3.0       0.21      0.20      0.21       446\n",
      "        6.0       0.19      0.07      0.10       413\n",
      "        2.0       0.29      0.51      0.37       542\n",
      "        9.0       0.37      0.40      0.38       371\n",
      "\n",
      "avg / total       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.82      0.61       615\n",
      "        8.0       0.16      0.05      0.08       244\n",
      "        1.0       0.29      0.04      0.07       268\n",
      "        4.0       0.22      0.13      0.16       298\n",
      "        7.0       0.14      0.04      0.06       262\n",
      "        3.0       0.20      0.26      0.23       425\n",
      "        6.0       0.14      0.08      0.10       396\n",
      "        2.0       0.29      0.51      0.37       546\n",
      "        9.0       0.43      0.35      0.38       384\n",
      "\n",
      "avg / total       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.51      0.81      0.63       629\n",
      "        8.0       0.10      0.01      0.03       269\n",
      "        1.0       0.11      0.08      0.09       218\n",
      "        4.0       0.22      0.18      0.20       298\n",
      "        7.0       0.12      0.01      0.01       267\n",
      "        3.0       0.20      0.23      0.22       444\n",
      "        6.0       0.15      0.09      0.11       403\n",
      "        2.0       0.30      0.48      0.37       551\n",
      "        9.0       0.36      0.32      0.34       359\n",
      "\n",
      "avg / total       0.26      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3222803955788249\n",
      "mean accuracy 0.3275741710296684\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model16 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.82      0.60       615\n",
      "        8.0       0.17      0.02      0.03       252\n",
      "        1.0       0.36      0.04      0.07       239\n",
      "        4.0       0.22      0.19      0.20       300\n",
      "        7.0       0.19      0.04      0.06       266\n",
      "        3.0       0.23      0.28      0.25       437\n",
      "        6.0       0.23      0.07      0.10       392\n",
      "        2.0       0.29      0.60      0.39       541\n",
      "        9.0       0.50      0.28      0.36       396\n",
      "\n",
      "avg / total       0.31      0.34      0.28      3438\n",
      "\n",
      "accuracy:  0.33973240255962767\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.82      0.62       629\n",
      "        8.0       0.03      0.00      0.01       261\n",
      "        1.0       0.14      0.02      0.04       247\n",
      "        4.0       0.25      0.13      0.17       296\n",
      "        7.0       0.10      0.01      0.01       263\n",
      "        3.0       0.22      0.31      0.26       432\n",
      "        6.0       0.15      0.04      0.06       407\n",
      "        2.0       0.26      0.50      0.34       556\n",
      "        9.0       0.34      0.33      0.34       347\n",
      "\n",
      "avg / total       0.25      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3228621291448517\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.81      0.62       639\n",
      "        8.0       0.10      0.01      0.02       247\n",
      "        1.0       0.20      0.05      0.08       229\n",
      "        4.0       0.20      0.17      0.18       291\n",
      "        7.0       0.11      0.00      0.01       287\n",
      "        3.0       0.22      0.25      0.23       438\n",
      "        6.0       0.10      0.02      0.03       408\n",
      "        2.0       0.26      0.55      0.35       540\n",
      "        9.0       0.36      0.34      0.35       359\n",
      "\n",
      "avg / total       0.26      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.32460732984293195\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.49      0.83      0.62       605\n",
      "        8.0       0.13      0.01      0.02       266\n",
      "        1.0       0.27      0.01      0.02       257\n",
      "        4.0       0.23      0.12      0.16       305\n",
      "        7.0       0.21      0.05      0.09       242\n",
      "        3.0       0.20      0.32      0.25       431\n",
      "        6.0       0.16      0.06      0.09       391\n",
      "        2.0       0.29      0.54      0.38       557\n",
      "        9.0       0.40      0.29      0.34       384\n",
      "\n",
      "avg / total       0.29      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3298429319371728\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.52      0.80      0.63       635\n",
      "        8.0       0.18      0.04      0.07       243\n",
      "        1.0       0.00      0.00      0.00       263\n",
      "        4.0       0.20      0.19      0.19       296\n",
      "        7.0       0.30      0.03      0.05       258\n",
      "        3.0       0.21      0.31      0.25       420\n",
      "        6.0       0.26      0.05      0.09       419\n",
      "        2.0       0.27      0.56      0.37       538\n",
      "        9.0       0.41      0.31      0.35       366\n",
      "\n",
      "avg / total       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.33478766724840026\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.47      0.82      0.60       609\n",
      "        8.0       0.09      0.01      0.01       270\n",
      "        1.0       0.20      0.11      0.14       223\n",
      "        4.0       0.21      0.09      0.13       300\n",
      "        7.0       0.12      0.00      0.01       271\n",
      "        3.0       0.22      0.27      0.24       449\n",
      "        6.0       0.16      0.08      0.10       380\n",
      "        2.0       0.28      0.52      0.37       559\n",
      "        9.0       0.36      0.28      0.32       377\n",
      "\n",
      "avg / total       0.26      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3222803955788249\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.80      0.61       632\n",
      "        8.0       0.11      0.03      0.05       244\n",
      "        1.0       0.26      0.03      0.06       239\n",
      "        4.0       0.22      0.14      0.17       316\n",
      "        7.0       0.19      0.01      0.03       268\n",
      "        3.0       0.21      0.35      0.26       427\n",
      "        6.0       0.15      0.09      0.12       382\n",
      "        2.0       0.31      0.50      0.38       555\n",
      "        9.0       0.39      0.28      0.33       375\n",
      "\n",
      "avg / total       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3298429319371728\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.83      0.60       612\n",
      "        8.0       0.15      0.02      0.03       269\n",
      "        1.0       0.09      0.01      0.01       247\n",
      "        4.0       0.22      0.15      0.18       280\n",
      "        7.0       0.12      0.03      0.05       261\n",
      "        3.0       0.20      0.23      0.21       442\n",
      "        6.0       0.10      0.01      0.02       417\n",
      "        2.0       0.25      0.56      0.35       542\n",
      "        9.0       0.38      0.32      0.35       368\n",
      "\n",
      "avg / total       0.25      0.32      0.25      3438\n",
      "\n",
      "accuracy:  0.3164630599185573\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.50      0.80      0.62       641\n",
      "        8.0       0.10      0.01      0.02       264\n",
      "        1.0       0.14      0.03      0.05       219\n",
      "        4.0       0.22      0.16      0.18       295\n",
      "        7.0       0.08      0.02      0.03       261\n",
      "        3.0       0.20      0.32      0.25       431\n",
      "        6.0       0.15      0.04      0.06       395\n",
      "        2.0       0.28      0.49      0.36       558\n",
      "        9.0       0.38      0.28      0.32       374\n",
      "\n",
      "avg / total       0.26      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.32315299592786506\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "        5.0       0.48      0.81      0.60       603\n",
      "        8.0       0.15      0.04      0.06       249\n",
      "        1.0       0.21      0.02      0.03       267\n",
      "        4.0       0.19      0.15      0.17       301\n",
      "        7.0       0.00      0.00      0.00       268\n",
      "        3.0       0.18      0.18      0.18       438\n",
      "        6.0       0.16      0.08      0.11       404\n",
      "        2.0       0.27      0.57      0.37       539\n",
      "        9.0       0.43      0.33      0.37       369\n",
      "\n",
      "avg / total       0.26      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.31733566026759746\n",
      "mean accuracy 0.3260907504363002\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model17 = sum(acc)/10\n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31157649796393255, 0.3295520651541594, 0.3301628853984876, 0.3272542175683537, 0.32908667830133803, 0.32934845840605004, 0.3281849912739965, 0.3275741710296684, 0.3260907504363002]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_minkowski\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
