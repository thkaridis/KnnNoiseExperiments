{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sex', ' MaritalStatus', ' Age', ' Education', ' Occupation',\n",
       "       ' YearsInSf', ' DualIncome', ' HouseholdMembers', ' Under18',\n",
       "       ' HouseholdStatus', ' TypeOfHome', ' EthnicClass', ' Language',\n",
       "       'Class'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing = pd.read_csv(\"marketing_numerical.csv\")\n",
    "(marketing.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>YearsInSf</th>\n",
       "      <th>DualIncome</th>\n",
       "      <th>HouseholdMembers</th>\n",
       "      <th>Under18</th>\n",
       "      <th>HouseholdStatus</th>\n",
       "      <th>TypeOfHome</th>\n",
       "      <th>EthnicClass</th>\n",
       "      <th>Language</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6846</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6847</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6848</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6849</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6855</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6856</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6857</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6863</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6865</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6866</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6867</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6869</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6871</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6873</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6874</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6875</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6876 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   MaritalStatus   Age   Education   Occupation   YearsInSf  \\\n",
       "0       1             1.0     5         5.0          5.0         5.0   \n",
       "1       2             1.0     3         5.0          1.0         5.0   \n",
       "2       2             5.0     1         2.0          6.0         5.0   \n",
       "3       2             5.0     1         2.0          6.0         3.0   \n",
       "4       1             1.0     6         4.0          8.0         5.0   \n",
       "5       1             5.0     2         3.0          9.0         4.0   \n",
       "6       1             3.0     3         4.0          3.0         5.0   \n",
       "7       1             1.0     6         3.0          8.0         5.0   \n",
       "8       1             1.0     7         4.0          8.0         4.0   \n",
       "9       1             5.0     2         4.0          9.0         5.0   \n",
       "10      2             2.0     2         3.0          2.0         5.0   \n",
       "11      2             1.0     3         6.0          6.0         2.0   \n",
       "12      2             1.0     5         3.0          5.0         5.0   \n",
       "13      2             1.0     5         4.0          1.0         5.0   \n",
       "14      2             3.0     3         3.0          2.0         2.0   \n",
       "15      2             1.0     5         3.0          5.0         1.0   \n",
       "16      1             3.0     4         2.0          3.0         4.0   \n",
       "17      2             1.0     4         4.0          2.0         5.0   \n",
       "18      1             1.0     4         4.0          1.0         5.0   \n",
       "19      2             3.0     5         4.0          2.0         3.0   \n",
       "20      2             5.0     1         2.0          6.0         5.0   \n",
       "21      1             1.0     3         5.0          1.0         5.0   \n",
       "22      1             1.0     4         4.0          1.0         5.0   \n",
       "23      1             1.0     4         4.0          1.0         5.0   \n",
       "24      2             3.0     7         3.0          8.0         5.0   \n",
       "25      2             3.0     4         5.0          1.0         4.0   \n",
       "26      2             1.0     4         4.0          1.0         4.0   \n",
       "27      2             5.0     1         2.0          6.0         5.0   \n",
       "28      2             5.0     1         2.0          6.0         5.0   \n",
       "29      1             5.0     1         2.0          6.0         5.0   \n",
       "...   ...             ...   ...         ...          ...         ...   \n",
       "6846    1             5.0     2         4.0          2.0         5.0   \n",
       "6847    1             1.0     3         4.0          1.0         5.0   \n",
       "6848    1             1.0     3         4.0          2.0         5.0   \n",
       "6849    1             5.0     3         5.0          2.0         5.0   \n",
       "6850    2             1.0     6         5.0          1.0         5.0   \n",
       "6851    1             3.0     3         5.0          1.0         5.0   \n",
       "6852    1             5.0     1         2.0          6.0         5.0   \n",
       "6853    2             5.0     1         2.0          6.0         5.0   \n",
       "6854    1             5.0     3         4.0          4.0         5.0   \n",
       "6855    2             5.0     2         4.0          4.0         5.0   \n",
       "6856    1             1.0     7         4.0          8.0         5.0   \n",
       "6857    1             4.0     3         4.0          1.0         5.0   \n",
       "6858    2             5.0     2         3.0          6.0         2.0   \n",
       "6859    1             5.0     2         4.0          4.0         5.0   \n",
       "6860    2             3.0     4         4.0          1.0         5.0   \n",
       "6861    1             5.0     3         5.0          2.0         5.0   \n",
       "6862    1             5.0     2         3.0          3.0         5.0   \n",
       "6863    2             1.0     4         6.0          5.0         5.0   \n",
       "6864    2             5.0     1         2.0          6.0         5.0   \n",
       "6865    1             5.0     2         3.0          4.0         5.0   \n",
       "6866    1             5.0     3         4.0          6.0         5.0   \n",
       "6867    1             5.0     2         4.0          4.0         2.0   \n",
       "6868    2             5.0     2         4.0          4.0         2.0   \n",
       "6869    1             5.0     3         4.0          6.0         2.0   \n",
       "6870    1             1.0     3         6.0          1.0         5.0   \n",
       "6871    2             5.0     1         1.0          2.0         5.0   \n",
       "6872    1             5.0     2         4.0          1.0         5.0   \n",
       "6873    2             5.0     1         2.0          1.0         5.0   \n",
       "6874    1             1.0     6         4.0          3.0         5.0   \n",
       "6875    1             5.0     3         4.0          1.0         5.0   \n",
       "\n",
       "       DualIncome   HouseholdMembers   Under18   HouseholdStatus   TypeOfHome  \\\n",
       "0               3                5.0         2               1.0          1.0   \n",
       "1               2                3.0         1               2.0          3.0   \n",
       "2               1                4.0         2               3.0          1.0   \n",
       "3               1                4.0         2               3.0          1.0   \n",
       "4               3                2.0         0               1.0          1.0   \n",
       "5               1                3.0         1               2.0          3.0   \n",
       "6               1                1.0         0               2.0          3.0   \n",
       "7               3                3.0         0               2.0          3.0   \n",
       "8               3                2.0         0               2.0          3.0   \n",
       "9               1                1.0         0               2.0          3.0   \n",
       "10              1                2.0         0               1.0          1.0   \n",
       "11              2                4.0         2               1.0          1.0   \n",
       "12              3                4.0         0               2.0          1.0   \n",
       "13              2                2.0         2               1.0          1.0   \n",
       "14              1                2.0         1               2.0          3.0   \n",
       "15              3                2.0         0               2.0          3.0   \n",
       "16              1                2.0         0               2.0          3.0   \n",
       "17              3                5.0         3               1.0          1.0   \n",
       "18              3                5.0         3               1.0          1.0   \n",
       "19              1                3.0         0               2.0          5.0   \n",
       "20              1                4.0         1               3.0          1.0   \n",
       "21              2                3.0         1               2.0          3.0   \n",
       "22              2                3.0         0               1.0          1.0   \n",
       "23              2                3.0         1               1.0          1.0   \n",
       "24              1                1.0         0               1.0          5.0   \n",
       "25              1                5.0         3               1.0          1.0   \n",
       "26              2                4.0         2               2.0          1.0   \n",
       "27              1                4.0         2               3.0          1.0   \n",
       "28              1                3.0         1               3.0          1.0   \n",
       "29              1                6.0         4               3.0          1.0   \n",
       "...           ...                ...       ...               ...          ...   \n",
       "6846            1                3.0         0               3.0          1.0   \n",
       "6847            3                4.0         2               1.0          1.0   \n",
       "6848            2                5.0         3               1.0          1.0   \n",
       "6849            1                1.0         0               1.0          2.0   \n",
       "6850            2                2.0         0               1.0          1.0   \n",
       "6851            1                2.0         0               1.0          1.0   \n",
       "6852            1                3.0         1               3.0          1.0   \n",
       "6853            1                4.0         2               3.0          1.0   \n",
       "6854            1                2.0         0               2.0          3.0   \n",
       "6855            1                3.0         0               2.0          1.0   \n",
       "6856            3                2.0         0               1.0          1.0   \n",
       "6857            1                3.0         0               2.0          1.0   \n",
       "6858            1                4.0         2               3.0          3.0   \n",
       "6859            1                1.0         0               2.0          2.0   \n",
       "6860            1                1.0         0               1.0          2.0   \n",
       "6861            1                1.0         0               2.0          3.0   \n",
       "6862            1                1.0         0               2.0          1.0   \n",
       "6863            3                5.0         3               1.0          1.0   \n",
       "6864            1                4.0         1               3.0          1.0   \n",
       "6865            1                4.0         0               3.0          1.0   \n",
       "6866            1                2.0         0               2.0          5.0   \n",
       "6867            1                3.0         0               2.0          5.0   \n",
       "6868            1                4.0         0               2.0          1.0   \n",
       "6869            1                3.0         0               2.0          3.0   \n",
       "6870            2                2.0         0               1.0          1.0   \n",
       "6871            1                3.0         2               3.0          1.0   \n",
       "6872            1                4.0         0               3.0          1.0   \n",
       "6873            1                3.0         2               3.0          1.0   \n",
       "6874            2                3.0         1               2.0          3.0   \n",
       "6875            1                1.0         0               2.0          3.0   \n",
       "\n",
       "       EthnicClass   Language  Class  \n",
       "0              7.0        1.0      9  \n",
       "1              7.0        1.0      9  \n",
       "2              7.0        1.0      1  \n",
       "3              7.0        1.0      1  \n",
       "4              7.0        1.0      8  \n",
       "5              7.0        1.0      1  \n",
       "6              7.0        1.0      6  \n",
       "7              7.0        1.0      2  \n",
       "8              7.0        1.0      4  \n",
       "9              7.0        1.0      1  \n",
       "10             5.0        1.0      4  \n",
       "11             7.0        1.0      8  \n",
       "12             7.0        1.0      7  \n",
       "13             7.0        1.0      7  \n",
       "14             7.0        1.0      1  \n",
       "15             7.0        1.0      8  \n",
       "16             7.0        1.0      2  \n",
       "17             5.0        2.0      9  \n",
       "18             7.0        1.0      8  \n",
       "19             7.0        1.0      4  \n",
       "20             7.0        1.0      1  \n",
       "21             7.0        1.0      9  \n",
       "22             7.0        1.0      7  \n",
       "23             7.0        1.0      9  \n",
       "24             7.0        1.0      4  \n",
       "25             7.0        1.0      6  \n",
       "26             7.0        1.0      8  \n",
       "27             7.0        1.0      1  \n",
       "28             7.0        1.0      1  \n",
       "29             7.0        1.0      1  \n",
       "...            ...        ...    ...  \n",
       "6846           7.0        1.0      3  \n",
       "6847           2.0        1.0      9  \n",
       "6848           5.0        1.0      9  \n",
       "6849           7.0        1.0      6  \n",
       "6850           7.0        1.0      8  \n",
       "6851           7.0        1.0      7  \n",
       "6852           7.0        1.0      1  \n",
       "6853           5.0        1.0      1  \n",
       "6854           7.0        1.0      2  \n",
       "6855           7.0        1.0      4  \n",
       "6856           7.0        1.0      9  \n",
       "6857           7.0        1.0      4  \n",
       "6858           7.0        1.0      1  \n",
       "6859           7.0        1.0      6  \n",
       "6860           7.0        1.0      7  \n",
       "6861           7.0        1.0      5  \n",
       "6862           2.0        1.0      3  \n",
       "6863           5.0        1.0      7  \n",
       "6864           7.0        1.0      1  \n",
       "6865           7.0        1.0      6  \n",
       "6866           7.0        1.0      5  \n",
       "6867           8.0        3.0      1  \n",
       "6868           7.0        1.0      3  \n",
       "6869           8.0        3.0      2  \n",
       "6870           7.0        1.0      9  \n",
       "6871           7.0        1.0      1  \n",
       "6872           7.0        1.0      2  \n",
       "6873           7.0        1.0      1  \n",
       "6874           7.0        1.0      4  \n",
       "6875           5.0        1.0      6  \n",
       "\n",
       "[6876 rows x 14 columns]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "j = 0\n",
    "features = ['Sex', ' MaritalStatus', ' Age', ' Education', ' Occupation',\n",
    "       ' YearsInSf', ' DualIncome', ' HouseholdMembers', ' Under18',\n",
    "       ' HouseholdStatus', ' TypeOfHome', ' EthnicClass', ' Language']\n",
    "for index, m in marketing.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        marketing.at[index+2,features[j]] = i + 1\n",
    "        j += 3\n",
    "        i += 10\n",
    "        if j >= 12:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for index, m in marketing.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        if index < 6870:\n",
    "            marketing.at[index+5,:] = marketing.loc[j,:]\n",
    "        else:\n",
    "            marketing.at[index+1,:] = marketing.loc[j,:]\n",
    "        j += 120\n",
    "        if j >= 6500:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing.to_csv('/home/valia/Documents/AppliedDataScience/marketingNoise.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9.0', '1.0', '8.0', '6.0', '2.0', '4.0', '7.0', '5.0', '3.0'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = marketing.iloc[:,0:13]\n",
    "labels = marketing.iloc[:,13].apply(str)\n",
    "labels.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True) #5 times with 2 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean and k tuning on 10% noise datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.62      0.61       606\n",
      "         8.0       0.17      0.16      0.16       253\n",
      "         1.0       0.19      0.16      0.17       242\n",
      "         4.0       0.14      0.15      0.14       287\n",
      "         7.0       0.18      0.17      0.18       276\n",
      "         3.0       0.29      0.29      0.29       451\n",
      "         6.0       0.22      0.22      0.22       405\n",
      "         2.0       0.30      0.32      0.31       541\n",
      "         9.0       0.35      0.36      0.36       377\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.31151832460732987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.66      0.57      0.61       638\n",
      "         8.0       0.18      0.19      0.19       260\n",
      "         1.0       0.15      0.15      0.15       244\n",
      "         4.0       0.17      0.18      0.17       309\n",
      "         7.0       0.19      0.23      0.20       253\n",
      "         3.0       0.22      0.22      0.22       418\n",
      "         6.0       0.24      0.24      0.24       394\n",
      "         2.0       0.33      0.34      0.34       556\n",
      "         9.0       0.31      0.30      0.31       366\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.32      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.30599185573007565\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.62      0.62      0.62       632\n",
      "         8.0       0.18      0.15      0.16       255\n",
      "         1.0       0.20      0.18      0.19       256\n",
      "         4.0       0.15      0.15      0.15       287\n",
      "         7.0       0.20      0.22      0.21       264\n",
      "         3.0       0.24      0.25      0.24       435\n",
      "         6.0       0.23      0.25      0.24       390\n",
      "         2.0       0.33      0.32      0.33       571\n",
      "         9.0       0.30      0.31      0.30       348\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3121000581733566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.61      0.61      0.61       612\n",
      "         8.0       0.17      0.16      0.16       258\n",
      "         1.0       0.16      0.18      0.17       230\n",
      "         4.0       0.19      0.19      0.19       309\n",
      "         7.0       0.20      0.20      0.20       265\n",
      "         3.0       0.24      0.24      0.24       434\n",
      "         6.0       0.25      0.23      0.24       409\n",
      "         2.0       0.32      0.35      0.34       526\n",
      "         9.0       0.38      0.34      0.36       395\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3164630599185573\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.60      0.59      0.60       625\n",
      "         8.0       0.20      0.20      0.20       246\n",
      "         1.0       0.16      0.13      0.15       253\n",
      "         4.0       0.17      0.16      0.17       299\n",
      "         7.0       0.21      0.22      0.21       264\n",
      "         3.0       0.28      0.31      0.29       424\n",
      "         6.0       0.26      0.26      0.26       407\n",
      "         2.0       0.31      0.30      0.30       568\n",
      "         9.0       0.30      0.35      0.32       352\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.31      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.3150087260034904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.58      0.58      0.58       619\n",
      "         8.0       0.20      0.19      0.19       267\n",
      "         1.0       0.15      0.17      0.16       233\n",
      "         4.0       0.17      0.17      0.17       297\n",
      "         7.0       0.22      0.25      0.23       265\n",
      "         3.0       0.28      0.24      0.26       445\n",
      "         6.0       0.24      0.24      0.24       392\n",
      "         2.0       0.30      0.33      0.32       529\n",
      "         9.0       0.38      0.33      0.35       391\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.31239092495637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.65      0.62      0.63       633\n",
      "         8.0       0.16      0.13      0.14       268\n",
      "         1.0       0.16      0.14      0.15       246\n",
      "         4.0       0.16      0.17      0.17       292\n",
      "         7.0       0.17      0.18      0.18       255\n",
      "         3.0       0.26      0.28      0.27       435\n",
      "         6.0       0.23      0.24      0.24       387\n",
      "         2.0       0.34      0.34      0.34       556\n",
      "         9.0       0.34      0.36      0.35       366\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3170447934845841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.61      0.60      0.60       611\n",
      "         8.0       0.20      0.20      0.20       245\n",
      "         1.0       0.15      0.16      0.15       240\n",
      "         4.0       0.18      0.18      0.18       304\n",
      "         7.0       0.24      0.22      0.23       274\n",
      "         3.0       0.26      0.23      0.24       434\n",
      "         6.0       0.21      0.22      0.21       412\n",
      "         2.0       0.29      0.31      0.30       541\n",
      "         9.0       0.32      0.34      0.33       377\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.30570098894706227\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.60      0.57      0.59       649\n",
      "         8.0       0.16      0.16      0.16       249\n",
      "         1.0       0.17      0.15      0.16       256\n",
      "         4.0       0.13      0.13      0.13       298\n",
      "         7.0       0.18      0.21      0.20       247\n",
      "         3.0       0.22      0.23      0.22       444\n",
      "         6.0       0.19      0.18      0.19       400\n",
      "         2.0       0.34      0.38      0.36       530\n",
      "         9.0       0.32      0.30      0.31       365\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.29842931937172773\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.61      0.61      0.61       595\n",
      "         8.0       0.21      0.17      0.19       264\n",
      "         1.0       0.13      0.12      0.13       230\n",
      "         4.0       0.15      0.14      0.15       298\n",
      "         7.0       0.20      0.20      0.20       282\n",
      "         3.0       0.26      0.28      0.27       425\n",
      "         6.0       0.23      0.26      0.25       399\n",
      "         2.0       0.34      0.32      0.33       567\n",
      "         9.0       0.34      0.36      0.35       378\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.31180919139034324\n",
      "mean accuracy 0.31064572425828973\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.74      0.59       610\n",
      "         8.0       0.18      0.19      0.18       253\n",
      "         1.0       0.16      0.12      0.14       258\n",
      "         4.0       0.15      0.14      0.14       287\n",
      "         7.0       0.17      0.13      0.15       273\n",
      "         3.0       0.24      0.25      0.25       415\n",
      "         6.0       0.24      0.18      0.21       421\n",
      "         2.0       0.33      0.37      0.35       549\n",
      "         9.0       0.40      0.29      0.33       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3170447934845841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.72      0.62       634\n",
      "         8.0       0.17      0.14      0.15       260\n",
      "         1.0       0.15      0.18      0.16       228\n",
      "         4.0       0.19      0.18      0.18       309\n",
      "         7.0       0.19      0.14      0.16       256\n",
      "         3.0       0.27      0.20      0.23       454\n",
      "         6.0       0.23      0.22      0.22       378\n",
      "         2.0       0.34      0.40      0.37       548\n",
      "         9.0       0.40      0.29      0.33       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.72      0.59       614\n",
      "         8.0       0.18      0.19      0.18       247\n",
      "         1.0       0.14      0.12      0.13       250\n",
      "         4.0       0.18      0.15      0.16       309\n",
      "         7.0       0.19      0.15      0.17       266\n",
      "         3.0       0.25      0.25      0.25       437\n",
      "         6.0       0.25      0.20      0.22       399\n",
      "         2.0       0.36      0.33      0.34       572\n",
      "         9.0       0.35      0.34      0.34       344\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.30      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.3196625945317045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.73      0.60       630\n",
      "         8.0       0.20      0.19      0.19       266\n",
      "         1.0       0.16      0.15      0.16       236\n",
      "         4.0       0.17      0.17      0.17       287\n",
      "         7.0       0.18      0.11      0.14       263\n",
      "         3.0       0.27      0.25      0.26       432\n",
      "         6.0       0.26      0.21      0.23       400\n",
      "         2.0       0.36      0.43      0.40       525\n",
      "         9.0       0.48      0.31      0.37       399\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.29      0.28      0.28      3438\n",
      "weighted avg       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.33856893542757416\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.70      0.60       638\n",
      "         8.0       0.18      0.14      0.16       273\n",
      "         1.0       0.15      0.19      0.17       232\n",
      "         4.0       0.20      0.15      0.17       315\n",
      "         7.0       0.22      0.16      0.19       266\n",
      "         3.0       0.24      0.29      0.26       397\n",
      "         6.0       0.24      0.19      0.21       384\n",
      "         2.0       0.38      0.39      0.39       564\n",
      "         9.0       0.42      0.31      0.36       369\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.33391506689936007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.75      0.60       606\n",
      "         8.0       0.16      0.16      0.16       240\n",
      "         1.0       0.16      0.13      0.14       254\n",
      "         4.0       0.17      0.19      0.18       281\n",
      "         7.0       0.20      0.17      0.18       263\n",
      "         3.0       0.26      0.20      0.23       472\n",
      "         6.0       0.27      0.21      0.24       415\n",
      "         2.0       0.31      0.34      0.32       533\n",
      "         9.0       0.44      0.34      0.38       374\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.73      0.58       582\n",
      "         8.0       0.14      0.15      0.14       252\n",
      "         1.0       0.16      0.13      0.14       267\n",
      "         4.0       0.19      0.18      0.18       289\n",
      "         7.0       0.24      0.15      0.19       284\n",
      "         3.0       0.28      0.29      0.28       423\n",
      "         6.0       0.25      0.19      0.22       404\n",
      "         2.0       0.35      0.38      0.36       552\n",
      "         9.0       0.43      0.34      0.38       385\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.55      0.73      0.63       662\n",
      "         8.0       0.19      0.16      0.17       261\n",
      "         1.0       0.16      0.18      0.17       219\n",
      "         4.0       0.15      0.11      0.13       307\n",
      "         7.0       0.20      0.17      0.18       245\n",
      "         3.0       0.26      0.24      0.25       446\n",
      "         6.0       0.26      0.23      0.24       395\n",
      "         2.0       0.36      0.37      0.37       545\n",
      "         9.0       0.39      0.35      0.37       358\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.34      0.33      3438\n",
      "\n",
      "accuracy:  0.33885980221058754\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.73      0.59       614\n",
      "         8.0       0.13      0.13      0.13       262\n",
      "         1.0       0.12      0.12      0.12       229\n",
      "         4.0       0.13      0.12      0.12       303\n",
      "         7.0       0.19      0.14      0.16       267\n",
      "         3.0       0.24      0.23      0.24       427\n",
      "         6.0       0.27      0.20      0.23       404\n",
      "         2.0       0.36      0.35      0.35       569\n",
      "         9.0       0.40      0.30      0.34       363\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.3135543920884235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.75      0.61       630\n",
      "         8.0       0.16      0.16      0.16       251\n",
      "         1.0       0.17      0.12      0.14       257\n",
      "         4.0       0.17      0.17      0.17       293\n",
      "         7.0       0.17      0.13      0.15       262\n",
      "         3.0       0.25      0.22      0.24       442\n",
      "         6.0       0.27      0.22      0.24       395\n",
      "         2.0       0.34      0.40      0.37       528\n",
      "         9.0       0.42      0.31      0.35       380\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "mean accuracy 0.32739965095986034\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model2 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.73      0.61       615\n",
      "         8.0       0.19      0.13      0.16       260\n",
      "         1.0       0.14      0.17      0.15       216\n",
      "         4.0       0.19      0.14      0.16       295\n",
      "         7.0       0.16      0.15      0.16       252\n",
      "         3.0       0.23      0.26      0.24       427\n",
      "         6.0       0.24      0.18      0.20       394\n",
      "         2.0       0.39      0.37      0.38       592\n",
      "         9.0       0.42      0.39      0.40       387\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.74      0.60       629\n",
      "         8.0       0.17      0.15      0.16       253\n",
      "         1.0       0.17      0.11      0.13       270\n",
      "         4.0       0.21      0.17      0.19       301\n",
      "         7.0       0.23      0.12      0.16       277\n",
      "         3.0       0.25      0.25      0.25       442\n",
      "         6.0       0.23      0.15      0.18       405\n",
      "         2.0       0.30      0.43      0.35       505\n",
      "         9.0       0.37      0.30      0.33       356\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32315299592786506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.58      0.73      0.65       637\n",
      "         8.0       0.24      0.21      0.22       258\n",
      "         1.0       0.13      0.12      0.12       229\n",
      "         4.0       0.20      0.15      0.17       310\n",
      "         7.0       0.20      0.13      0.15       279\n",
      "         3.0       0.22      0.26      0.24       429\n",
      "         6.0       0.20      0.16      0.18       397\n",
      "         2.0       0.33      0.42      0.37       543\n",
      "         9.0       0.40      0.33      0.36       356\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.33391506689936007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.78      0.60       607\n",
      "         8.0       0.19      0.14      0.16       255\n",
      "         1.0       0.17      0.11      0.13       257\n",
      "         4.0       0.17      0.19      0.18       286\n",
      "         7.0       0.19      0.16      0.17       250\n",
      "         3.0       0.26      0.23      0.24       440\n",
      "         6.0       0.23      0.16      0.19       402\n",
      "         2.0       0.35      0.40      0.37       554\n",
      "         9.0       0.47      0.34      0.40       387\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.33507853403141363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.74      0.62       628\n",
      "         8.0       0.22      0.16      0.18       274\n",
      "         1.0       0.20      0.17      0.18       236\n",
      "         4.0       0.20      0.14      0.16       295\n",
      "         7.0       0.19      0.16      0.17       246\n",
      "         3.0       0.23      0.29      0.26       409\n",
      "         6.0       0.25      0.15      0.19       410\n",
      "         2.0       0.35      0.43      0.39       564\n",
      "         9.0       0.39      0.32      0.35       376\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.3403141361256545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.75      0.61       616\n",
      "         8.0       0.18      0.16      0.17       239\n",
      "         1.0       0.14      0.10      0.11       250\n",
      "         4.0       0.20      0.21      0.20       301\n",
      "         7.0       0.22      0.15      0.18       283\n",
      "         3.0       0.27      0.26      0.27       460\n",
      "         6.0       0.24      0.17      0.20       389\n",
      "         2.0       0.33      0.40      0.36       533\n",
      "         9.0       0.42      0.35      0.38       367\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.31      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.33595113438045376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.73      0.60       626\n",
      "         8.0       0.16      0.14      0.15       250\n",
      "         1.0       0.18      0.17      0.17       229\n",
      "         4.0       0.16      0.14      0.15       295\n",
      "         7.0       0.11      0.08      0.09       248\n",
      "         3.0       0.23      0.24      0.24       436\n",
      "         6.0       0.27      0.17      0.21       418\n",
      "         2.0       0.34      0.41      0.37       560\n",
      "         9.0       0.43      0.32      0.37       376\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.76      0.62       618\n",
      "         8.0       0.23      0.18      0.20       263\n",
      "         1.0       0.16      0.09      0.12       257\n",
      "         4.0       0.18      0.17      0.17       301\n",
      "         7.0       0.16      0.10      0.12       281\n",
      "         3.0       0.25      0.27      0.26       433\n",
      "         6.0       0.24      0.21      0.22       381\n",
      "         2.0       0.35      0.43      0.39       537\n",
      "         9.0       0.43      0.30      0.36       367\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.3371146015125073\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.75      0.60       611\n",
      "         8.0       0.23      0.18      0.20       254\n",
      "         1.0       0.14      0.11      0.12       237\n",
      "         4.0       0.22      0.19      0.20       284\n",
      "         7.0       0.20      0.13      0.16       260\n",
      "         3.0       0.25      0.24      0.25       448\n",
      "         6.0       0.25      0.18      0.21       413\n",
      "         2.0       0.31      0.41      0.35       548\n",
      "         9.0       0.41      0.31      0.35       383\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.76      0.63       633\n",
      "         8.0       0.19      0.15      0.17       259\n",
      "         1.0       0.22      0.16      0.18       249\n",
      "         4.0       0.19      0.15      0.17       312\n",
      "         7.0       0.18      0.11      0.14       269\n",
      "         3.0       0.21      0.24      0.23       421\n",
      "         6.0       0.20      0.16      0.18       386\n",
      "         2.0       0.31      0.37      0.34       549\n",
      "         9.0       0.38      0.32      0.35       360\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3254799301919721\n",
      "mean accuracy 0.3321989528795811\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model3 = sum(acc)/10 \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.75      0.61       619\n",
      "         8.0       0.14      0.10      0.12       276\n",
      "         1.0       0.21      0.12      0.16       258\n",
      "         4.0       0.18      0.17      0.17       293\n",
      "         7.0       0.19      0.14      0.16       253\n",
      "         3.0       0.23      0.25      0.24       423\n",
      "         6.0       0.19      0.13      0.15       398\n",
      "         2.0       0.29      0.40      0.34       536\n",
      "         9.0       0.40      0.29      0.33       382\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3170447934845841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.77      0.62       625\n",
      "         8.0       0.15      0.15      0.15       237\n",
      "         1.0       0.13      0.11      0.12       228\n",
      "         4.0       0.13      0.08      0.10       303\n",
      "         7.0       0.18      0.11      0.13       276\n",
      "         3.0       0.28      0.30      0.29       446\n",
      "         6.0       0.21      0.16      0.18       401\n",
      "         2.0       0.36      0.42      0.39       561\n",
      "         9.0       0.43      0.36      0.39       361\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.30      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3365328679464805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.77      0.63       641\n",
      "         8.0       0.18      0.18      0.18       250\n",
      "         1.0       0.20      0.10      0.13       256\n",
      "         4.0       0.20      0.20      0.20       298\n",
      "         7.0       0.23      0.15      0.18       271\n",
      "         3.0       0.23      0.25      0.24       442\n",
      "         6.0       0.19      0.14      0.16       373\n",
      "         2.0       0.39      0.40      0.39       567\n",
      "         9.0       0.39      0.36      0.37       340\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.29      0.28      3438\n",
      "weighted avg       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.3435136707388016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.76      0.61       603\n",
      "         8.0       0.22      0.17      0.19       263\n",
      "         1.0       0.18      0.12      0.14       230\n",
      "         4.0       0.18      0.15      0.16       298\n",
      "         7.0       0.18      0.10      0.13       258\n",
      "         3.0       0.25      0.28      0.26       427\n",
      "         6.0       0.24      0.14      0.17       426\n",
      "         2.0       0.33      0.47      0.39       530\n",
      "         9.0       0.46      0.33      0.39       403\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3379872018615474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.56      0.74      0.64       667\n",
      "         8.0       0.16      0.12      0.14       256\n",
      "         1.0       0.16      0.12      0.14       235\n",
      "         4.0       0.17      0.20      0.18       273\n",
      "         7.0       0.18      0.09      0.12       265\n",
      "         3.0       0.24      0.27      0.25       425\n",
      "         6.0       0.24      0.14      0.18       403\n",
      "         2.0       0.32      0.40      0.35       554\n",
      "         9.0       0.39      0.35      0.37       360\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.31      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.33507853403141363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.75      0.58       577\n",
      "         8.0       0.18      0.12      0.14       257\n",
      "         1.0       0.17      0.11      0.13       251\n",
      "         4.0       0.20      0.14      0.17       323\n",
      "         7.0       0.14      0.11      0.12       264\n",
      "         3.0       0.24      0.25      0.24       444\n",
      "         6.0       0.22      0.15      0.18       396\n",
      "         2.0       0.32      0.41      0.36       543\n",
      "         9.0       0.44      0.35      0.39       383\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3196625945317045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.74      0.62       646\n",
      "         8.0       0.21      0.16      0.18       246\n",
      "         1.0       0.21      0.13      0.16       248\n",
      "         4.0       0.21      0.15      0.17       321\n",
      "         7.0       0.23      0.13      0.17       257\n",
      "         3.0       0.24      0.26      0.25       431\n",
      "         6.0       0.21      0.17      0.19       395\n",
      "         2.0       0.33      0.43      0.37       536\n",
      "         9.0       0.44      0.38      0.41       358\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.29      0.28      0.28      3438\n",
      "weighted avg       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.3426410703897615\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.61       598\n",
      "         8.0       0.18      0.12      0.15       267\n",
      "         1.0       0.13      0.08      0.10       238\n",
      "         4.0       0.16      0.20      0.18       275\n",
      "         7.0       0.15      0.09      0.11       272\n",
      "         3.0       0.23      0.25      0.24       438\n",
      "         6.0       0.25      0.16      0.19       404\n",
      "         2.0       0.36      0.45      0.40       561\n",
      "         9.0       0.45      0.31      0.37       385\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3307155322862129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.74      0.65       666\n",
      "         8.0       0.18      0.17      0.18       243\n",
      "         1.0       0.18      0.10      0.13       245\n",
      "         4.0       0.15      0.15      0.15       281\n",
      "         7.0       0.21      0.13      0.16       277\n",
      "         3.0       0.25      0.31      0.28       428\n",
      "         6.0       0.25      0.16      0.20       399\n",
      "         2.0       0.31      0.40      0.35       534\n",
      "         9.0       0.38      0.32      0.35       365\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.34002326934264104\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.78      0.59       578\n",
      "         8.0       0.20      0.13      0.16       270\n",
      "         1.0       0.16      0.12      0.14       241\n",
      "         4.0       0.22      0.14      0.17       315\n",
      "         7.0       0.18      0.12      0.14       252\n",
      "         3.0       0.24      0.24      0.24       441\n",
      "         6.0       0.23      0.17      0.19       400\n",
      "         2.0       0.34      0.43      0.38       563\n",
      "         9.0       0.44      0.33      0.38       378\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "mean accuracy 0.33327515997673063\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model4 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.77      0.62       640\n",
      "         8.0       0.17      0.12      0.14       249\n",
      "         1.0       0.14      0.09      0.11       243\n",
      "         4.0       0.20      0.19      0.20       284\n",
      "         7.0       0.18      0.10      0.13       276\n",
      "         3.0       0.22      0.23      0.23       433\n",
      "         6.0       0.23      0.16      0.19       388\n",
      "         2.0       0.32      0.42      0.36       548\n",
      "         9.0       0.41      0.33      0.36       377\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3307155322862129\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.60       604\n",
      "         8.0       0.16      0.11      0.13       264\n",
      "         1.0       0.14      0.07      0.10       243\n",
      "         4.0       0.19      0.14      0.16       312\n",
      "         7.0       0.19      0.14      0.16       253\n",
      "         3.0       0.23      0.27      0.25       436\n",
      "         6.0       0.19      0.13      0.16       411\n",
      "         2.0       0.31      0.43      0.36       549\n",
      "         9.0       0.39      0.31      0.35       366\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3182082606166376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.70      0.61       639\n",
      "         8.0       0.15      0.14      0.14       244\n",
      "         1.0       0.17      0.12      0.14       241\n",
      "         4.0       0.20      0.11      0.14       322\n",
      "         7.0       0.14      0.10      0.12       248\n",
      "         3.0       0.24      0.32      0.27       434\n",
      "         6.0       0.24      0.15      0.18       405\n",
      "         2.0       0.34      0.46      0.39       542\n",
      "         9.0       0.42      0.36      0.38       363\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.78      0.60       605\n",
      "         8.0       0.18      0.10      0.13       269\n",
      "         1.0       0.25      0.16      0.19       245\n",
      "         4.0       0.18      0.17      0.17       274\n",
      "         7.0       0.25      0.09      0.13       281\n",
      "         3.0       0.27      0.29      0.28       435\n",
      "         6.0       0.20      0.14      0.16       394\n",
      "         2.0       0.32      0.45      0.38       555\n",
      "         9.0       0.46      0.34      0.39       380\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.29      0.28      0.27      3438\n",
      "weighted avg       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.341477603257708\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.79      0.61       605\n",
      "         8.0       0.18      0.13      0.15       258\n",
      "         1.0       0.16      0.08      0.11       256\n",
      "         4.0       0.21      0.16      0.18       310\n",
      "         7.0       0.14      0.10      0.12       251\n",
      "         3.0       0.24      0.26      0.25       444\n",
      "         6.0       0.21      0.15      0.17       405\n",
      "         2.0       0.34      0.39      0.36       556\n",
      "         9.0       0.40      0.37      0.38       353\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.75      0.61       639\n",
      "         8.0       0.20      0.17      0.18       255\n",
      "         1.0       0.17      0.10      0.13       230\n",
      "         4.0       0.20      0.14      0.16       286\n",
      "         7.0       0.20      0.09      0.12       278\n",
      "         3.0       0.21      0.24      0.23       425\n",
      "         6.0       0.22      0.15      0.18       394\n",
      "         2.0       0.32      0.48      0.38       541\n",
      "         9.0       0.42      0.28      0.34       390\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.75      0.59       597\n",
      "         8.0       0.21      0.15      0.18       239\n",
      "         1.0       0.17      0.07      0.10       247\n",
      "         4.0       0.21      0.18      0.20       296\n",
      "         7.0       0.17      0.08      0.11       272\n",
      "         3.0       0.26      0.25      0.25       462\n",
      "         6.0       0.21      0.17      0.19       400\n",
      "         2.0       0.32      0.45      0.37       551\n",
      "         9.0       0.40      0.36      0.38       374\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.61       647\n",
      "         8.0       0.19      0.10      0.13       274\n",
      "         1.0       0.16      0.13      0.14       239\n",
      "         4.0       0.20      0.12      0.15       300\n",
      "         7.0       0.17      0.11      0.13       257\n",
      "         3.0       0.22      0.30      0.25       407\n",
      "         6.0       0.21      0.12      0.15       399\n",
      "         2.0       0.33      0.40      0.37       546\n",
      "         9.0       0.42      0.34      0.38       369\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3310063990692263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.80      0.61       611\n",
      "         8.0       0.21      0.11      0.15       270\n",
      "         1.0       0.18      0.10      0.13       234\n",
      "         4.0       0.18      0.16      0.17       295\n",
      "         7.0       0.25      0.10      0.14       278\n",
      "         3.0       0.27      0.33      0.29       423\n",
      "         6.0       0.22      0.12      0.16       415\n",
      "         2.0       0.34      0.49      0.40       537\n",
      "         9.0       0.43      0.30      0.35       375\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3435136707388016\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.72      0.61       633\n",
      "         8.0       0.16      0.15      0.15       243\n",
      "         1.0       0.16      0.10      0.12       252\n",
      "         4.0       0.17      0.12      0.14       301\n",
      "         7.0       0.18      0.13      0.15       251\n",
      "         3.0       0.22      0.24      0.23       446\n",
      "         6.0       0.25      0.18      0.21       384\n",
      "         2.0       0.32      0.42      0.36       560\n",
      "         9.0       0.38      0.33      0.35       368\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "mean accuracy 0.33152995927865037\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model5 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.80      0.61       610\n",
      "         8.0       0.17      0.12      0.14       260\n",
      "         1.0       0.16      0.08      0.10       237\n",
      "         4.0       0.16      0.10      0.12       310\n",
      "         7.0       0.11      0.07      0.09       241\n",
      "         3.0       0.20      0.25      0.22       424\n",
      "         6.0       0.22      0.13      0.16       413\n",
      "         2.0       0.32      0.42      0.36       563\n",
      "         9.0       0.42      0.36      0.39       380\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.32257126236183825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.78      0.61       634\n",
      "         8.0       0.20      0.13      0.15       253\n",
      "         1.0       0.23      0.10      0.14       249\n",
      "         4.0       0.18      0.19      0.19       286\n",
      "         7.0       0.23      0.07      0.11       288\n",
      "         3.0       0.20      0.19      0.20       445\n",
      "         6.0       0.16      0.11      0.13       386\n",
      "         2.0       0.30      0.46      0.37       534\n",
      "         9.0       0.38      0.32      0.35       363\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.80      0.63       625\n",
      "         8.0       0.13      0.08      0.10       255\n",
      "         1.0       0.23      0.12      0.16       240\n",
      "         4.0       0.19      0.15      0.17       299\n",
      "         7.0       0.14      0.09      0.11       257\n",
      "         3.0       0.23      0.23      0.23       431\n",
      "         6.0       0.27      0.11      0.16       425\n",
      "         2.0       0.31      0.47      0.37       552\n",
      "         9.0       0.35      0.35      0.35       354\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.25      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3344968004653869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.78      0.60       619\n",
      "         8.0       0.17      0.12      0.14       258\n",
      "         1.0       0.19      0.07      0.10       246\n",
      "         4.0       0.18      0.18      0.18       297\n",
      "         7.0       0.09      0.03      0.04       272\n",
      "         3.0       0.19      0.22      0.21       438\n",
      "         6.0       0.20      0.14      0.16       374\n",
      "         2.0       0.32      0.45      0.37       545\n",
      "         9.0       0.42      0.28      0.34       389\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3182082606166376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.75      0.61       597\n",
      "         8.0       0.18      0.14      0.15       251\n",
      "         1.0       0.16      0.13      0.14       222\n",
      "         4.0       0.22      0.17      0.19       309\n",
      "         7.0       0.13      0.06      0.08       260\n",
      "         3.0       0.21      0.25      0.23       455\n",
      "         6.0       0.25      0.12      0.16       418\n",
      "         2.0       0.30      0.46      0.36       534\n",
      "         9.0       0.41      0.27      0.33       392\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3193717277486911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       647\n",
      "         8.0       0.16      0.07      0.10       262\n",
      "         1.0       0.21      0.07      0.11       264\n",
      "         4.0       0.17      0.11      0.14       287\n",
      "         7.0       0.15      0.09      0.11       269\n",
      "         3.0       0.19      0.25      0.22       414\n",
      "         6.0       0.19      0.13      0.15       381\n",
      "         2.0       0.34      0.41      0.37       563\n",
      "         9.0       0.35      0.35      0.35       351\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3278068644560791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.60       626\n",
      "         8.0       0.13      0.09      0.11       264\n",
      "         1.0       0.22      0.09      0.13       250\n",
      "         4.0       0.14      0.13      0.14       289\n",
      "         7.0       0.11      0.04      0.06       275\n",
      "         3.0       0.23      0.30      0.26       416\n",
      "         6.0       0.21      0.14      0.17       395\n",
      "         2.0       0.32      0.42      0.37       569\n",
      "         9.0       0.47      0.38      0.42       354\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.80      0.61       618\n",
      "         8.0       0.19      0.11      0.14       249\n",
      "         1.0       0.17      0.11      0.13       236\n",
      "         4.0       0.21      0.14      0.17       307\n",
      "         7.0       0.16      0.09      0.12       254\n",
      "         3.0       0.23      0.25      0.24       453\n",
      "         6.0       0.23      0.12      0.16       404\n",
      "         2.0       0.31      0.48      0.38       528\n",
      "         9.0       0.40      0.31      0.35       389\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33304246655031994\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.82      0.60       591\n",
      "         8.0       0.20      0.14      0.17       257\n",
      "         1.0       0.14      0.05      0.08       246\n",
      "         4.0       0.19      0.12      0.14       304\n",
      "         7.0       0.10      0.05      0.06       287\n",
      "         3.0       0.23      0.28      0.25       422\n",
      "         6.0       0.23      0.14      0.18       400\n",
      "         2.0       0.31      0.43      0.36       549\n",
      "         9.0       0.43      0.36      0.39       382\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.27      0.25      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.76      0.63       653\n",
      "         8.0       0.18      0.12      0.14       256\n",
      "         1.0       0.20      0.12      0.15       240\n",
      "         4.0       0.18      0.13      0.15       292\n",
      "         7.0       0.11      0.09      0.10       242\n",
      "         3.0       0.22      0.24      0.23       447\n",
      "         6.0       0.22      0.11      0.15       399\n",
      "         2.0       0.33      0.46      0.38       548\n",
      "         9.0       0.39      0.30      0.34       361\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3286794648051193\n",
      "mean accuracy 0.32678883071553233\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model6 = sum(acc)/10\n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.80      0.64       650\n",
      "         8.0       0.17      0.10      0.13       244\n",
      "         1.0       0.19      0.11      0.14       236\n",
      "         4.0       0.19      0.16      0.17       296\n",
      "         7.0       0.10      0.06      0.07       254\n",
      "         3.0       0.22      0.24      0.23       438\n",
      "         6.0       0.17      0.10      0.13       392\n",
      "         2.0       0.33      0.47      0.39       543\n",
      "         9.0       0.42      0.30      0.35       385\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3344968004653869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.82      0.59       594\n",
      "         8.0       0.16      0.07      0.09       269\n",
      "         1.0       0.22      0.07      0.10       250\n",
      "         4.0       0.22      0.14      0.17       300\n",
      "         7.0       0.13      0.03      0.05       275\n",
      "         3.0       0.21      0.27      0.24       431\n",
      "         6.0       0.17      0.07      0.10       407\n",
      "         2.0       0.28      0.44      0.34       554\n",
      "         9.0       0.33      0.34      0.33       358\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.24      0.25      0.23      3438\n",
      "weighted avg       0.26      0.31      0.27      3438\n",
      "\n",
      "accuracy:  0.31413612565445026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.76      0.61       623\n",
      "         8.0       0.16      0.08      0.11       241\n",
      "         1.0       0.17      0.09      0.11       241\n",
      "         4.0       0.24      0.13      0.17       316\n",
      "         7.0       0.16      0.04      0.06       274\n",
      "         3.0       0.21      0.36      0.27       420\n",
      "         6.0       0.20      0.09      0.13       400\n",
      "         2.0       0.32      0.46      0.38       563\n",
      "         9.0       0.40      0.36      0.38       360\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.83      0.61       621\n",
      "         8.0       0.15      0.06      0.09       272\n",
      "         1.0       0.27      0.06      0.10       245\n",
      "         4.0       0.20      0.18      0.19       280\n",
      "         7.0       0.18      0.07      0.10       255\n",
      "         3.0       0.22      0.21      0.22       449\n",
      "         6.0       0.15      0.10      0.12       399\n",
      "         2.0       0.29      0.51      0.37       534\n",
      "         9.0       0.44      0.27      0.34       383\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.25      0.24      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3278068644560791\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.79      0.61       598\n",
      "         8.0       0.16      0.10      0.12       249\n",
      "         1.0       0.17      0.04      0.06       261\n",
      "         4.0       0.21      0.16      0.18       307\n",
      "         7.0       0.15      0.08      0.11       258\n",
      "         3.0       0.21      0.24      0.22       434\n",
      "         6.0       0.17      0.08      0.11       411\n",
      "         2.0       0.29      0.48      0.36       544\n",
      "         9.0       0.39      0.32      0.35       376\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.31849912739965097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.83      0.63       646\n",
      "         8.0       0.17      0.04      0.07       264\n",
      "         1.0       0.22      0.15      0.17       225\n",
      "         4.0       0.21      0.18      0.19       289\n",
      "         7.0       0.20      0.04      0.06       271\n",
      "         3.0       0.22      0.31      0.26       435\n",
      "         6.0       0.17      0.08      0.11       388\n",
      "         2.0       0.34      0.47      0.39       553\n",
      "         9.0       0.40      0.34      0.37       367\n",
      "\n",
      "   micro avg       0.35      0.35      0.35      3438\n",
      "   macro avg       0.27      0.27      0.25      3438\n",
      "weighted avg       0.30      0.35      0.30      3438\n",
      "\n",
      "accuracy:  0.34758580570098896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       601\n",
      "         8.0       0.22      0.07      0.10       261\n",
      "         1.0       0.19      0.13      0.15       236\n",
      "         4.0       0.21      0.13      0.16       318\n",
      "         7.0       0.07      0.01      0.02       282\n",
      "         3.0       0.21      0.26      0.23       442\n",
      "         6.0       0.17      0.10      0.13       390\n",
      "         2.0       0.31      0.45      0.36       544\n",
      "         9.0       0.35      0.34      0.34       364\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.23      3438\n",
      "weighted avg       0.27      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.32257126236183825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       643\n",
      "         8.0       0.12      0.07      0.09       252\n",
      "         1.0       0.12      0.02      0.04       250\n",
      "         4.0       0.21      0.17      0.18       278\n",
      "         7.0       0.12      0.04      0.06       247\n",
      "         3.0       0.21      0.27      0.23       427\n",
      "         6.0       0.22      0.11      0.15       409\n",
      "         2.0       0.32      0.49      0.38       553\n",
      "         9.0       0.42      0.30      0.35       379\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.23      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.33478766724840026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.82      0.61       634\n",
      "         8.0       0.19      0.09      0.12       253\n",
      "         1.0       0.18      0.05      0.08       248\n",
      "         4.0       0.22      0.12      0.16       307\n",
      "         7.0       0.18      0.07      0.10       256\n",
      "         3.0       0.21      0.24      0.23       450\n",
      "         6.0       0.21      0.10      0.13       401\n",
      "         2.0       0.28      0.50      0.36       526\n",
      "         9.0       0.41      0.28      0.33       363\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.25      0.24      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.80      0.62       610\n",
      "         8.0       0.20      0.10      0.13       260\n",
      "         1.0       0.23      0.13      0.16       238\n",
      "         4.0       0.21      0.19      0.20       289\n",
      "         7.0       0.12      0.03      0.05       273\n",
      "         3.0       0.22      0.26      0.24       419\n",
      "         6.0       0.22      0.13      0.16       398\n",
      "         2.0       0.33      0.45      0.38       571\n",
      "         9.0       0.37      0.35      0.36       380\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.29      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.3371146015125073\n",
      "mean accuracy 0.3297556719022688\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model7 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       628\n",
      "         8.0       0.14      0.04      0.06       253\n",
      "         1.0       0.26      0.04      0.07       224\n",
      "         4.0       0.22      0.17      0.19       310\n",
      "         7.0       0.17      0.03      0.06       261\n",
      "         3.0       0.23      0.28      0.25       434\n",
      "         6.0       0.17      0.07      0.10       400\n",
      "         2.0       0.29      0.61      0.39       533\n",
      "         9.0       0.50      0.25      0.33       395\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.26      0.23      3438\n",
      "weighted avg       0.30      0.34      0.29      3438\n",
      "\n",
      "accuracy:  0.337696335078534\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.83      0.60       616\n",
      "         8.0       0.16      0.03      0.05       260\n",
      "         1.0       0.22      0.04      0.07       262\n",
      "         4.0       0.18      0.10      0.13       286\n",
      "         7.0       0.12      0.03      0.05       268\n",
      "         3.0       0.18      0.24      0.20       435\n",
      "         6.0       0.10      0.05      0.07       399\n",
      "         2.0       0.29      0.45      0.35       564\n",
      "         9.0       0.31      0.34      0.32       348\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.23      0.23      0.21      3438\n",
      "weighted avg       0.25      0.31      0.26      3438\n",
      "\n",
      "accuracy:  0.30860965677719604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.82      0.62       625\n",
      "         8.0       0.22      0.07      0.10       246\n",
      "         1.0       0.19      0.03      0.05       246\n",
      "         4.0       0.21      0.16      0.19       279\n",
      "         7.0       0.11      0.06      0.07       253\n",
      "         3.0       0.19      0.25      0.22       439\n",
      "         6.0       0.20      0.12      0.15       413\n",
      "         2.0       0.30      0.47      0.36       557\n",
      "         9.0       0.41      0.27      0.32       380\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.25      0.23      3438\n",
      "weighted avg       0.29      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.83      0.62       619\n",
      "         8.0       0.20      0.04      0.06       267\n",
      "         1.0       0.17      0.03      0.06       240\n",
      "         4.0       0.26      0.16      0.20       317\n",
      "         7.0       0.14      0.00      0.01       276\n",
      "         3.0       0.19      0.27      0.22       430\n",
      "         6.0       0.14      0.04      0.07       386\n",
      "         2.0       0.27      0.52      0.36       540\n",
      "         9.0       0.37      0.35      0.36       363\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.22      3438\n",
      "weighted avg       0.27      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3263525305410122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       631\n",
      "         8.0       0.11      0.03      0.05       254\n",
      "         1.0       0.32      0.05      0.08       253\n",
      "         4.0       0.22      0.24      0.23       281\n",
      "         7.0       0.06      0.01      0.01       276\n",
      "         3.0       0.20      0.30      0.24       425\n",
      "         6.0       0.14      0.05      0.07       426\n",
      "         2.0       0.28      0.48      0.35       534\n",
      "         9.0       0.37      0.27      0.32       358\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.22      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.32140779522978474\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.82      0.60       613\n",
      "         8.0       0.17      0.03      0.06       259\n",
      "         1.0       0.20      0.06      0.09       233\n",
      "         4.0       0.22      0.11      0.15       315\n",
      "         7.0       0.04      0.00      0.01       253\n",
      "         3.0       0.18      0.23      0.20       444\n",
      "         6.0       0.16      0.14      0.15       373\n",
      "         2.0       0.33      0.51      0.40       563\n",
      "         9.0       0.41      0.34      0.37       385\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.25      0.23      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.83      0.62       622\n",
      "         8.0       0.22      0.05      0.09       259\n",
      "         1.0       0.24      0.07      0.11       236\n",
      "         4.0       0.23      0.15      0.18       317\n",
      "         7.0       0.20      0.01      0.01       263\n",
      "         3.0       0.20      0.31      0.24       416\n",
      "         6.0       0.16      0.09      0.11       398\n",
      "         2.0       0.30      0.54      0.39       555\n",
      "         9.0       0.46      0.26      0.33       372\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.26      0.23      3438\n",
      "weighted avg       0.30      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3344968004653869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       622\n",
      "         8.0       0.22      0.04      0.07       254\n",
      "         1.0       0.14      0.04      0.06       250\n",
      "         4.0       0.19      0.13      0.15       279\n",
      "         7.0       0.18      0.09      0.12       266\n",
      "         3.0       0.22      0.25      0.23       453\n",
      "         6.0       0.20      0.07      0.11       401\n",
      "         2.0       0.29      0.54      0.38       542\n",
      "         9.0       0.37      0.32      0.34       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.23      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.81      0.60       618\n",
      "         8.0       0.15      0.05      0.08       255\n",
      "         1.0       0.20      0.04      0.07       254\n",
      "         4.0       0.21      0.13      0.16       298\n",
      "         7.0       0.12      0.04      0.06       270\n",
      "         3.0       0.22      0.30      0.25       428\n",
      "         6.0       0.19      0.07      0.10       409\n",
      "         2.0       0.28      0.51      0.36       540\n",
      "         9.0       0.38      0.28      0.32       366\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.22      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3222803955788249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.82      0.63       626\n",
      "         8.0       0.11      0.01      0.02       258\n",
      "         1.0       0.22      0.05      0.08       232\n",
      "         4.0       0.21      0.16      0.18       298\n",
      "         7.0       0.11      0.01      0.02       259\n",
      "         3.0       0.20      0.27      0.23       441\n",
      "         6.0       0.14      0.08      0.10       390\n",
      "         2.0       0.29      0.49      0.37       557\n",
      "         9.0       0.36      0.34      0.35       377\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.25      0.22      3438\n",
      "weighted avg       0.27      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3275159976730657\n",
      "mean accuracy 0.3262943571844096\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model8 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       641\n",
      "         8.0       0.16      0.03      0.05       254\n",
      "         1.0       0.21      0.03      0.05       248\n",
      "         4.0       0.20      0.20      0.20       316\n",
      "         7.0       0.14      0.02      0.03       255\n",
      "         3.0       0.22      0.27      0.25       437\n",
      "         6.0       0.19      0.07      0.11       380\n",
      "         2.0       0.27      0.53      0.36       539\n",
      "         9.0       0.37      0.24      0.29       368\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.24      0.22      3438\n",
      "weighted avg       0.28      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3263525305410122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.83      0.60       603\n",
      "         8.0       0.25      0.01      0.02       259\n",
      "         1.0       0.29      0.03      0.05       238\n",
      "         4.0       0.17      0.15      0.16       280\n",
      "         7.0       0.22      0.01      0.01       274\n",
      "         3.0       0.19      0.26      0.22       432\n",
      "         6.0       0.17      0.05      0.07       419\n",
      "         2.0       0.28      0.55      0.37       558\n",
      "         9.0       0.39      0.31      0.35       375\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.24      0.21      3438\n",
      "weighted avg       0.29      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3222803955788249\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.79      0.62       635\n",
      "         8.0       0.13      0.04      0.07       246\n",
      "         1.0       0.30      0.06      0.10       240\n",
      "         4.0       0.19      0.12      0.15       311\n",
      "         7.0       0.25      0.01      0.01       273\n",
      "         3.0       0.20      0.28      0.23       428\n",
      "         6.0       0.15      0.05      0.07       394\n",
      "         2.0       0.26      0.57      0.36       536\n",
      "         9.0       0.46      0.26      0.33       375\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.24      0.22      3438\n",
      "weighted avg       0.29      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3228621291448517\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.83      0.60       609\n",
      "         8.0       0.00      0.00      0.00       267\n",
      "         1.0       0.26      0.02      0.04       246\n",
      "         4.0       0.22      0.20      0.21       285\n",
      "         7.0       0.18      0.05      0.07       256\n",
      "         3.0       0.22      0.26      0.24       441\n",
      "         6.0       0.20      0.07      0.11       405\n",
      "         2.0       0.27      0.49      0.35       561\n",
      "         9.0       0.35      0.32      0.34       368\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.25      0.22      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.83      0.61       604\n",
      "         8.0       0.17      0.02      0.04       258\n",
      "         1.0       0.17      0.02      0.04       252\n",
      "         4.0       0.19      0.13      0.16       302\n",
      "         7.0       0.03      0.00      0.01       272\n",
      "         3.0       0.19      0.22      0.20       455\n",
      "         6.0       0.14      0.04      0.06       402\n",
      "         2.0       0.25      0.52      0.34       542\n",
      "         9.0       0.36      0.36      0.36       351\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.22      0.24      0.20      3438\n",
      "weighted avg       0.25      0.31      0.25      3438\n",
      "\n",
      "accuracy:  0.31297265852239675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.80      0.60       640\n",
      "         8.0       0.06      0.01      0.01       255\n",
      "         1.0       0.24      0.03      0.05       234\n",
      "         4.0       0.21      0.16      0.18       294\n",
      "         7.0       0.19      0.03      0.05       257\n",
      "         3.0       0.21      0.33      0.26       414\n",
      "         6.0       0.15      0.06      0.08       397\n",
      "         2.0       0.30      0.56      0.39       555\n",
      "         9.0       0.47      0.25      0.32       392\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.25      0.22      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3321698662012798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.82      0.61       620\n",
      "         8.0       0.16      0.04      0.06       257\n",
      "         1.0       0.17      0.00      0.01       264\n",
      "         4.0       0.19      0.17      0.18       287\n",
      "         7.0       0.07      0.01      0.02       267\n",
      "         3.0       0.22      0.21      0.22       454\n",
      "         6.0       0.16      0.07      0.09       395\n",
      "         2.0       0.26      0.55      0.36       531\n",
      "         9.0       0.36      0.31      0.33       363\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.23      0.24      0.21      3438\n",
      "weighted avg       0.26      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3193717277486911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.80      0.61       624\n",
      "         8.0       0.16      0.01      0.02       256\n",
      "         1.0       0.14      0.06      0.08       222\n",
      "         4.0       0.20      0.11      0.14       309\n",
      "         7.0       0.12      0.01      0.02       262\n",
      "         3.0       0.20      0.34      0.25       415\n",
      "         6.0       0.15      0.05      0.07       404\n",
      "         2.0       0.29      0.50      0.37       566\n",
      "         9.0       0.38      0.29      0.33       380\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.24      0.21      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.32257126236183825\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.80      0.61       620\n",
      "         8.0       0.17      0.06      0.09       235\n",
      "         1.0       0.21      0.08      0.11       249\n",
      "         4.0       0.22      0.09      0.13       309\n",
      "         7.0       0.07      0.00      0.01       276\n",
      "         3.0       0.23      0.29      0.25       454\n",
      "         6.0       0.21      0.06      0.09       393\n",
      "         2.0       0.27      0.60      0.37       534\n",
      "         9.0       0.43      0.26      0.33       368\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.25      0.22      3438\n",
      "weighted avg       0.28      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       624\n",
      "         8.0       0.08      0.00      0.01       278\n",
      "         1.0       0.13      0.03      0.05       237\n",
      "         4.0       0.21      0.19      0.20       287\n",
      "         7.0       0.24      0.03      0.05       253\n",
      "         3.0       0.20      0.28      0.23       415\n",
      "         6.0       0.14      0.04      0.06       406\n",
      "         2.0       0.28      0.50      0.36       563\n",
      "         9.0       0.33      0.30      0.31       375\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.23      0.24      0.21      3438\n",
      "weighted avg       0.26      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.32111692844677137\n",
      "mean accuracy 0.32324025596276906\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model9 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31064572425828973, 0.32739965095986034, 0.3321989528795811, 0.33327515997673063, 0.33152995927865037, 0.32678883071553233, 0.3297556719022688, 0.3262943571844096, 0.32324025596276906]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_euclidean\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski and k tuning on 10% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.63      0.61      0.62       636\n",
      "         8.0       0.19      0.16      0.17       261\n",
      "         1.0       0.17      0.18      0.17       227\n",
      "         4.0       0.16      0.17      0.16       301\n",
      "         7.0       0.21      0.22      0.22       274\n",
      "         3.0       0.27      0.24      0.25       445\n",
      "         6.0       0.24      0.26      0.25       379\n",
      "         2.0       0.33      0.37      0.35       539\n",
      "         9.0       0.35      0.32      0.33       376\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3222803955788249\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.60      0.59      0.59       608\n",
      "         8.0       0.21      0.19      0.20       252\n",
      "         1.0       0.16      0.15      0.15       259\n",
      "         4.0       0.15      0.16      0.16       295\n",
      "         7.0       0.20      0.21      0.20       255\n",
      "         3.0       0.25      0.27      0.26       424\n",
      "         6.0       0.28      0.24      0.26       420\n",
      "         2.0       0.33      0.36      0.35       558\n",
      "         9.0       0.34      0.33      0.33       367\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3150087260034904\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.60      0.59      0.60       625\n",
      "         8.0       0.19      0.17      0.18       254\n",
      "         1.0       0.14      0.14      0.14       239\n",
      "         4.0       0.11      0.10      0.10       307\n",
      "         7.0       0.21      0.22      0.21       283\n",
      "         3.0       0.24      0.25      0.24       439\n",
      "         6.0       0.23      0.22      0.22       395\n",
      "         2.0       0.32      0.33      0.33       558\n",
      "         9.0       0.33      0.35      0.34       338\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.30162885398487493\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.62      0.60       619\n",
      "         8.0       0.19      0.18      0.18       259\n",
      "         1.0       0.16      0.15      0.16       247\n",
      "         4.0       0.15      0.14      0.15       289\n",
      "         7.0       0.16      0.18      0.17       246\n",
      "         3.0       0.26      0.26      0.26       430\n",
      "         6.0       0.24      0.24      0.24       404\n",
      "         2.0       0.31      0.34      0.32       539\n",
      "         9.0       0.37      0.34      0.35       405\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3126817917393834\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.63      0.60      0.62       634\n",
      "         8.0       0.19      0.15      0.17       274\n",
      "         1.0       0.15      0.14      0.14       228\n",
      "         4.0       0.19      0.20      0.20       290\n",
      "         7.0       0.18      0.19      0.18       254\n",
      "         3.0       0.24      0.22      0.23       459\n",
      "         6.0       0.26      0.25      0.26       393\n",
      "         2.0       0.31      0.37      0.34       547\n",
      "         9.0       0.34      0.33      0.34       359\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.28      0.27      0.27      3438\n",
      "weighted avg       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3158813263525305\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.59      0.59       610\n",
      "         8.0       0.14      0.15      0.15       239\n",
      "         1.0       0.16      0.15      0.16       258\n",
      "         4.0       0.18      0.16      0.17       306\n",
      "         7.0       0.25      0.21      0.23       275\n",
      "         3.0       0.25      0.26      0.25       410\n",
      "         6.0       0.24      0.26      0.25       406\n",
      "         2.0       0.31      0.33      0.32       550\n",
      "         9.0       0.40      0.40      0.40       384\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.32      0.32      3438\n",
      "\n",
      "accuracy:  0.3158813263525305\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.62      0.61      0.61       630\n",
      "         8.0       0.23      0.21      0.22       277\n",
      "         1.0       0.15      0.17      0.16       232\n",
      "         4.0       0.16      0.16      0.16       293\n",
      "         7.0       0.17      0.21      0.19       248\n",
      "         3.0       0.24      0.26      0.25       447\n",
      "         6.0       0.20      0.18      0.19       388\n",
      "         2.0       0.30      0.31      0.31       542\n",
      "         9.0       0.36      0.31      0.34       381\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.30570098894706227\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.58      0.59      0.58       614\n",
      "         8.0       0.18      0.17      0.18       236\n",
      "         1.0       0.17      0.15      0.16       254\n",
      "         4.0       0.16      0.16      0.16       303\n",
      "         7.0       0.23      0.18      0.20       281\n",
      "         3.0       0.24      0.25      0.24       422\n",
      "         6.0       0.20      0.21      0.21       411\n",
      "         2.0       0.29      0.31      0.30       555\n",
      "         9.0       0.35      0.36      0.36       362\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.3022105875509017\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.60      0.58      0.59       615\n",
      "         8.0       0.18      0.17      0.18       258\n",
      "         1.0       0.16      0.14      0.15       250\n",
      "         4.0       0.19      0.19      0.19       298\n",
      "         7.0       0.20      0.22      0.21       248\n",
      "         3.0       0.24      0.28      0.26       429\n",
      "         6.0       0.22      0.23      0.23       398\n",
      "         2.0       0.33      0.34      0.33       556\n",
      "         9.0       0.38      0.33      0.35       386\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.28      0.27      0.28      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3112274578243165\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.59      0.59       629\n",
      "         8.0       0.21      0.18      0.19       255\n",
      "         1.0       0.15      0.16      0.15       236\n",
      "         4.0       0.19      0.17      0.18       298\n",
      "         7.0       0.19      0.18      0.18       281\n",
      "         3.0       0.23      0.22      0.23       440\n",
      "         6.0       0.23      0.23      0.23       401\n",
      "         2.0       0.34      0.36      0.35       541\n",
      "         9.0       0.34      0.39      0.36       357\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.28      0.27      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.3135543920884235\n",
      "mean accuracy 0.3116055846422338\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_minkowski = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.60       632\n",
      "         8.0       0.17      0.18      0.17       259\n",
      "         1.0       0.16      0.13      0.14       245\n",
      "         4.0       0.16      0.16      0.16       297\n",
      "         7.0       0.21      0.15      0.17       267\n",
      "         3.0       0.23      0.23      0.23       421\n",
      "         6.0       0.26      0.21      0.23       384\n",
      "         2.0       0.35      0.36      0.35       561\n",
      "         9.0       0.43      0.29      0.35       372\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.27      0.27      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.75      0.60       612\n",
      "         8.0       0.15      0.15      0.15       254\n",
      "         1.0       0.19      0.14      0.16       241\n",
      "         4.0       0.21      0.18      0.19       299\n",
      "         7.0       0.17      0.15      0.16       262\n",
      "         3.0       0.26      0.24      0.25       448\n",
      "         6.0       0.24      0.20      0.22       415\n",
      "         2.0       0.34      0.36      0.35       536\n",
      "         9.0       0.45      0.29      0.36       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.27      0.27      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.74      0.62       655\n",
      "         8.0       0.19      0.20      0.19       251\n",
      "         1.0       0.15      0.10      0.12       249\n",
      "         4.0       0.19      0.17      0.18       303\n",
      "         7.0       0.21      0.17      0.19       256\n",
      "         3.0       0.26      0.22      0.24       428\n",
      "         6.0       0.24      0.21      0.22       384\n",
      "         2.0       0.35      0.40      0.37       556\n",
      "         9.0       0.42      0.31      0.36       356\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.33885980221058754\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.75      0.59       589\n",
      "         8.0       0.18      0.16      0.17       262\n",
      "         1.0       0.14      0.13      0.14       237\n",
      "         4.0       0.16      0.16      0.16       293\n",
      "         7.0       0.16      0.14      0.15       273\n",
      "         3.0       0.25      0.23      0.24       441\n",
      "         6.0       0.25      0.18      0.21       415\n",
      "         2.0       0.35      0.38      0.37       541\n",
      "         9.0       0.42      0.28      0.34       387\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.31675392670157065\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.79      0.61       605\n",
      "         8.0       0.18      0.16      0.17       255\n",
      "         1.0       0.13      0.12      0.13       243\n",
      "         4.0       0.17      0.13      0.15       307\n",
      "         7.0       0.18      0.13      0.15       269\n",
      "         3.0       0.26      0.26      0.26       442\n",
      "         6.0       0.26      0.19      0.22       400\n",
      "         2.0       0.37      0.38      0.37       555\n",
      "         9.0       0.41      0.33      0.36       362\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.28      0.27      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.71      0.61       639\n",
      "         8.0       0.17      0.17      0.17       258\n",
      "         1.0       0.14      0.11      0.12       243\n",
      "         4.0       0.17      0.18      0.17       289\n",
      "         7.0       0.18      0.15      0.16       260\n",
      "         3.0       0.24      0.23      0.24       427\n",
      "         6.0       0.23      0.19      0.21       399\n",
      "         2.0       0.34      0.38      0.36       542\n",
      "         9.0       0.41      0.30      0.35       381\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.32344386271087844\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.73      0.60       623\n",
      "         8.0       0.18      0.18      0.18       246\n",
      "         1.0       0.16      0.17      0.16       237\n",
      "         4.0       0.17      0.18      0.17       293\n",
      "         7.0       0.21      0.18      0.19       262\n",
      "         3.0       0.24      0.20      0.22       444\n",
      "         6.0       0.24      0.17      0.20       401\n",
      "         2.0       0.34      0.36      0.35       551\n",
      "         9.0       0.40      0.27      0.32       381\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.3205351948807446\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.60       621\n",
      "         8.0       0.17      0.16      0.17       267\n",
      "         1.0       0.18      0.14      0.16       249\n",
      "         4.0       0.18      0.16      0.17       303\n",
      "         7.0       0.19      0.17      0.18       267\n",
      "         3.0       0.27      0.26      0.27       425\n",
      "         6.0       0.25      0.21      0.23       398\n",
      "         2.0       0.40      0.40      0.40       546\n",
      "         9.0       0.41      0.33      0.36       362\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.29      0.28      3438\n",
      "weighted avg       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.33885980221058754\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.70      0.60       624\n",
      "         8.0       0.17      0.16      0.17       263\n",
      "         1.0       0.16      0.19      0.17       244\n",
      "         4.0       0.19      0.18      0.19       283\n",
      "         7.0       0.22      0.15      0.18       272\n",
      "         3.0       0.28      0.26      0.27       445\n",
      "         6.0       0.24      0.18      0.20       408\n",
      "         2.0       0.34      0.40      0.37       526\n",
      "         9.0       0.42      0.33      0.37       373\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.32      0.33      0.32      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.80      0.62       620\n",
      "         8.0       0.18      0.19      0.19       250\n",
      "         1.0       0.17      0.14      0.16       242\n",
      "         4.0       0.19      0.17      0.18       313\n",
      "         7.0       0.17      0.11      0.13       257\n",
      "         3.0       0.26      0.24      0.25       424\n",
      "         6.0       0.24      0.23      0.23       391\n",
      "         2.0       0.35      0.35      0.35       571\n",
      "         9.0       0.46      0.27      0.34       370\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "mean accuracy 0.3286503781268179\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model11 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.78      0.64       636\n",
      "         8.0       0.23      0.21      0.22       244\n",
      "         1.0       0.18      0.14      0.16       229\n",
      "         4.0       0.22      0.13      0.17       315\n",
      "         7.0       0.20      0.16      0.18       258\n",
      "         3.0       0.27      0.26      0.27       443\n",
      "         6.0       0.24      0.18      0.20       405\n",
      "         2.0       0.32      0.40      0.35       535\n",
      "         9.0       0.40      0.34      0.37       373\n",
      "\n",
      "   micro avg       0.35      0.35      0.35      3438\n",
      "   macro avg       0.29      0.29      0.28      3438\n",
      "weighted avg       0.32      0.35      0.33      3438\n",
      "\n",
      "accuracy:  0.34642233856893545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.70      0.59       608\n",
      "         8.0       0.17      0.14      0.16       269\n",
      "         1.0       0.20      0.11      0.14       257\n",
      "         4.0       0.16      0.15      0.16       281\n",
      "         7.0       0.18      0.13      0.15       271\n",
      "         3.0       0.26      0.31      0.28       426\n",
      "         6.0       0.21      0.15      0.17       394\n",
      "         2.0       0.34      0.42      0.37       562\n",
      "         9.0       0.42      0.33      0.37       370\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.3240255962769052\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.77      0.61       602\n",
      "         8.0       0.18      0.12      0.15       257\n",
      "         1.0       0.17      0.14      0.15       249\n",
      "         4.0       0.21      0.21      0.21       300\n",
      "         7.0       0.17      0.11      0.14       254\n",
      "         3.0       0.22      0.23      0.22       440\n",
      "         6.0       0.23      0.18      0.20       388\n",
      "         2.0       0.34      0.38      0.36       553\n",
      "         9.0       0.42      0.30      0.35       395\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3254799301919721\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.73      0.62       642\n",
      "         8.0       0.19      0.19      0.19       256\n",
      "         1.0       0.18      0.13      0.15       237\n",
      "         4.0       0.19      0.16      0.17       296\n",
      "         7.0       0.19      0.13      0.15       275\n",
      "         3.0       0.22      0.22      0.22       429\n",
      "         6.0       0.26      0.18      0.21       411\n",
      "         2.0       0.33      0.40      0.36       544\n",
      "         9.0       0.35      0.33      0.34       348\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.73      0.60       622\n",
      "         8.0       0.19      0.18      0.18       249\n",
      "         1.0       0.15      0.11      0.12       232\n",
      "         4.0       0.18      0.17      0.17       296\n",
      "         7.0       0.24      0.14      0.18       284\n",
      "         3.0       0.24      0.24      0.24       444\n",
      "         6.0       0.20      0.15      0.18       400\n",
      "         2.0       0.33      0.38      0.36       550\n",
      "         9.0       0.39      0.36      0.38       361\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.77      0.63       622\n",
      "         8.0       0.19      0.15      0.17       264\n",
      "         1.0       0.17      0.15      0.16       254\n",
      "         4.0       0.19      0.17      0.18       300\n",
      "         7.0       0.21      0.18      0.19       245\n",
      "         3.0       0.23      0.25      0.24       425\n",
      "         6.0       0.21      0.15      0.17       399\n",
      "         2.0       0.36      0.42      0.39       547\n",
      "         9.0       0.41      0.30      0.35       382\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.31      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.3371146015125073\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.55      0.74      0.63       658\n",
      "         8.0       0.13      0.11      0.12       247\n",
      "         1.0       0.14      0.12      0.13       239\n",
      "         4.0       0.14      0.15      0.14       275\n",
      "         7.0       0.16      0.12      0.14       248\n",
      "         3.0       0.27      0.23      0.25       452\n",
      "         6.0       0.21      0.16      0.18       400\n",
      "         2.0       0.35      0.42      0.39       558\n",
      "         9.0       0.38      0.30      0.33       361\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.76      0.59       586\n",
      "         8.0       0.20      0.15      0.17       266\n",
      "         1.0       0.15      0.10      0.12       247\n",
      "         4.0       0.19      0.15      0.17       321\n",
      "         7.0       0.23      0.15      0.18       281\n",
      "         3.0       0.22      0.26      0.24       417\n",
      "         6.0       0.21      0.17      0.19       399\n",
      "         2.0       0.36      0.42      0.39       539\n",
      "         9.0       0.40      0.31      0.35       382\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.76      0.63       631\n",
      "         8.0       0.16      0.19      0.17       243\n",
      "         1.0       0.19      0.12      0.15       231\n",
      "         4.0       0.21      0.18      0.19       308\n",
      "         7.0       0.18      0.12      0.14       254\n",
      "         3.0       0.25      0.23      0.24       451\n",
      "         6.0       0.23      0.19      0.21       401\n",
      "         2.0       0.34      0.41      0.38       550\n",
      "         9.0       0.40      0.32      0.36       369\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.33856893542757416\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.76      0.61       613\n",
      "         8.0       0.17      0.11      0.14       270\n",
      "         1.0       0.18      0.12      0.15       255\n",
      "         4.0       0.16      0.16      0.16       288\n",
      "         7.0       0.19      0.15      0.17       275\n",
      "         3.0       0.23      0.28      0.26       418\n",
      "         6.0       0.21      0.16      0.18       398\n",
      "         2.0       0.36      0.40      0.38       547\n",
      "         9.0       0.43      0.29      0.34       374\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3254799301919721\n",
      "mean accuracy 0.3303955788248982\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model12 = sum(acc)/10 \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.75      0.61       619\n",
      "         8.0       0.14      0.17      0.16       232\n",
      "         1.0       0.20      0.10      0.13       259\n",
      "         4.0       0.19      0.15      0.17       304\n",
      "         7.0       0.29      0.15      0.20       273\n",
      "         3.0       0.24      0.28      0.26       435\n",
      "         6.0       0.25      0.15      0.18       425\n",
      "         2.0       0.30      0.43      0.36       529\n",
      "         9.0       0.38      0.27      0.31       362\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.27      0.26      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3275159976730657\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.75      0.60       625\n",
      "         8.0       0.25      0.12      0.16       281\n",
      "         1.0       0.19      0.14      0.16       227\n",
      "         4.0       0.15      0.14      0.15       292\n",
      "         7.0       0.18      0.16      0.17       256\n",
      "         3.0       0.23      0.23      0.23       434\n",
      "         6.0       0.19      0.14      0.17       374\n",
      "         2.0       0.36      0.45      0.40       568\n",
      "         9.0       0.40      0.31      0.35       381\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33158813263525305\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.76      0.61       633\n",
      "         8.0       0.18      0.16      0.17       230\n",
      "         1.0       0.21      0.15      0.17       258\n",
      "         4.0       0.22      0.14      0.18       313\n",
      "         7.0       0.22      0.15      0.18       262\n",
      "         3.0       0.24      0.27      0.25       426\n",
      "         6.0       0.26      0.19      0.22       392\n",
      "         2.0       0.35      0.44      0.39       544\n",
      "         9.0       0.46      0.32      0.38       380\n",
      "\n",
      "   micro avg       0.35      0.35      0.35      3438\n",
      "   macro avg       0.29      0.29      0.28      3438\n",
      "weighted avg       0.32      0.35      0.33      3438\n",
      "\n",
      "accuracy:  0.34758580570098896\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.75      0.60       611\n",
      "         8.0       0.16      0.08      0.11       283\n",
      "         1.0       0.17      0.11      0.14       228\n",
      "         4.0       0.17      0.15      0.16       283\n",
      "         7.0       0.18      0.10      0.13       267\n",
      "         3.0       0.25      0.26      0.25       443\n",
      "         6.0       0.19      0.15      0.17       407\n",
      "         2.0       0.33      0.40      0.36       553\n",
      "         9.0       0.40      0.38      0.39       363\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.75      0.63       643\n",
      "         8.0       0.18      0.17      0.17       247\n",
      "         1.0       0.23      0.14      0.18       244\n",
      "         4.0       0.25      0.18      0.21       289\n",
      "         7.0       0.15      0.14      0.14       258\n",
      "         3.0       0.26      0.32      0.29       419\n",
      "         6.0       0.24      0.12      0.16       434\n",
      "         2.0       0.34      0.45      0.39       537\n",
      "         9.0       0.40      0.30      0.34       367\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.29      0.29      0.28      3438\n",
      "weighted avg       0.32      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.343804537521815\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.79      0.60       601\n",
      "         8.0       0.18      0.12      0.14       266\n",
      "         1.0       0.18      0.11      0.14       242\n",
      "         4.0       0.20      0.14      0.17       307\n",
      "         7.0       0.14      0.08      0.10       271\n",
      "         3.0       0.22      0.21      0.22       450\n",
      "         6.0       0.21      0.20      0.20       365\n",
      "         2.0       0.34      0.40      0.37       560\n",
      "         9.0       0.44      0.38      0.40       376\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.75      0.60       604\n",
      "         8.0       0.18      0.14      0.16       258\n",
      "         1.0       0.14      0.11      0.13       244\n",
      "         4.0       0.18      0.10      0.13       328\n",
      "         7.0       0.16      0.11      0.13       263\n",
      "         3.0       0.26      0.25      0.25       463\n",
      "         6.0       0.19      0.17      0.18       375\n",
      "         2.0       0.32      0.40      0.36       544\n",
      "         9.0       0.39      0.33      0.35       359\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.31878999418266435\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.76      0.62       640\n",
      "         8.0       0.18      0.11      0.14       255\n",
      "         1.0       0.14      0.08      0.10       242\n",
      "         4.0       0.17      0.20      0.18       268\n",
      "         7.0       0.17      0.11      0.13       266\n",
      "         3.0       0.22      0.26      0.24       406\n",
      "         6.0       0.19      0.11      0.14       424\n",
      "         2.0       0.34      0.43      0.38       553\n",
      "         9.0       0.44      0.35      0.39       384\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3321698662012798\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.77      0.60       614\n",
      "         8.0       0.15      0.12      0.13       260\n",
      "         1.0       0.18      0.10      0.13       259\n",
      "         4.0       0.20      0.17      0.18       286\n",
      "         7.0       0.13      0.07      0.09       275\n",
      "         3.0       0.25      0.25      0.25       436\n",
      "         6.0       0.24      0.17      0.20       406\n",
      "         2.0       0.31      0.44      0.36       531\n",
      "         9.0       0.47      0.32      0.38       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.74      0.61       630\n",
      "         8.0       0.15      0.11      0.13       253\n",
      "         1.0       0.13      0.11      0.12       227\n",
      "         4.0       0.19      0.12      0.15       310\n",
      "         7.0       0.16      0.13      0.15       254\n",
      "         3.0       0.24      0.26      0.25       433\n",
      "         6.0       0.20      0.16      0.18       393\n",
      "         2.0       0.32      0.37      0.35       566\n",
      "         9.0       0.37      0.32      0.34       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3190808609656777\n",
      "mean accuracy 0.33025014543339154\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model11 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.74      0.59       620\n",
      "         8.0       0.18      0.13      0.15       262\n",
      "         1.0       0.16      0.08      0.11       243\n",
      "         4.0       0.20      0.14      0.17       310\n",
      "         7.0       0.22      0.09      0.13       285\n",
      "         3.0       0.22      0.27      0.24       427\n",
      "         6.0       0.20      0.16      0.18       414\n",
      "         2.0       0.34      0.42      0.37       541\n",
      "         9.0       0.37      0.36      0.37       336\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3228621291448517\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.79      0.62       624\n",
      "         8.0       0.17      0.10      0.13       251\n",
      "         1.0       0.20      0.11      0.14       243\n",
      "         4.0       0.20      0.20      0.20       286\n",
      "         7.0       0.19      0.14      0.16       244\n",
      "         3.0       0.24      0.22      0.23       442\n",
      "         6.0       0.25      0.14      0.18       385\n",
      "         2.0       0.31      0.46      0.37       556\n",
      "         9.0       0.45      0.27      0.34       407\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.27      0.26      3438\n",
      "weighted avg       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3368237347294939\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.73      0.62       657\n",
      "         8.0       0.21      0.16      0.18       251\n",
      "         1.0       0.19      0.13      0.15       247\n",
      "         4.0       0.19      0.12      0.15       310\n",
      "         7.0       0.16      0.10      0.12       258\n",
      "         3.0       0.21      0.23      0.22       441\n",
      "         6.0       0.15      0.12      0.14       368\n",
      "         2.0       0.33      0.45      0.38       547\n",
      "         9.0       0.37      0.31      0.34       359\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3254799301919721\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.83      0.60       587\n",
      "         8.0       0.23      0.11      0.15       262\n",
      "         1.0       0.22      0.12      0.16       239\n",
      "         4.0       0.15      0.15      0.15       286\n",
      "         7.0       0.14      0.09      0.11       271\n",
      "         3.0       0.23      0.23      0.23       428\n",
      "         6.0       0.21      0.09      0.13       431\n",
      "         2.0       0.33      0.45      0.38       550\n",
      "         9.0       0.36      0.30      0.33       384\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.32344386271087844\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.61       616\n",
      "         8.0       0.18      0.13      0.15       250\n",
      "         1.0       0.22      0.11      0.15       246\n",
      "         4.0       0.20      0.14      0.17       300\n",
      "         7.0       0.16      0.10      0.12       266\n",
      "         3.0       0.26      0.30      0.28       435\n",
      "         6.0       0.18      0.16      0.17       392\n",
      "         2.0       0.34      0.40      0.36       556\n",
      "         9.0       0.49      0.39      0.43       377\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.34      0.32      3438\n",
      "\n",
      "accuracy:  0.33973240255962767\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.78      0.62       628\n",
      "         8.0       0.19      0.13      0.16       263\n",
      "         1.0       0.16      0.12      0.14       240\n",
      "         4.0       0.23      0.19      0.21       296\n",
      "         7.0       0.20      0.10      0.13       263\n",
      "         3.0       0.24      0.26      0.25       434\n",
      "         6.0       0.23      0.13      0.17       407\n",
      "         2.0       0.32      0.46      0.37       541\n",
      "         9.0       0.40      0.31      0.35       366\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.33740546829552065\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.77      0.62       633\n",
      "         8.0       0.17      0.14      0.15       246\n",
      "         1.0       0.17      0.10      0.13       246\n",
      "         4.0       0.17      0.14      0.16       294\n",
      "         7.0       0.11      0.05      0.07       271\n",
      "         3.0       0.22      0.24      0.23       435\n",
      "         6.0       0.19      0.12      0.15       381\n",
      "         2.0       0.32      0.41      0.36       557\n",
      "         9.0       0.38      0.35      0.36       375\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3240255962769052\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.77      0.61       611\n",
      "         8.0       0.21      0.13      0.16       267\n",
      "         1.0       0.19      0.13      0.16       240\n",
      "         4.0       0.16      0.14      0.15       302\n",
      "         7.0       0.14      0.09      0.11       258\n",
      "         3.0       0.23      0.24      0.24       434\n",
      "         6.0       0.22      0.14      0.18       418\n",
      "         2.0       0.32      0.43      0.37       540\n",
      "         9.0       0.44      0.35      0.39       368\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3286794648051193\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.73      0.60       617\n",
      "         8.0       0.21      0.14      0.17       268\n",
      "         1.0       0.15      0.10      0.12       250\n",
      "         4.0       0.13      0.13      0.13       296\n",
      "         7.0       0.15      0.07      0.10       278\n",
      "         3.0       0.21      0.25      0.23       433\n",
      "         6.0       0.20      0.13      0.16       395\n",
      "         2.0       0.31      0.46      0.37       525\n",
      "         9.0       0.41      0.28      0.34       376\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.25      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.31326352530541013\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.78      0.62       627\n",
      "         8.0       0.21      0.13      0.16       245\n",
      "         1.0       0.17      0.13      0.14       236\n",
      "         4.0       0.14      0.08      0.10       300\n",
      "         7.0       0.17      0.11      0.14       251\n",
      "         3.0       0.25      0.27      0.26       436\n",
      "         6.0       0.24      0.17      0.20       404\n",
      "         2.0       0.30      0.37      0.33       572\n",
      "         9.0       0.41      0.37      0.39       367\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3321698662012798\n",
      "mean accuracy 0.32838859802210596\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model13 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.77      0.63       650\n",
      "         8.0       0.20      0.14      0.16       255\n",
      "         1.0       0.25      0.09      0.14       244\n",
      "         4.0       0.25      0.21      0.23       289\n",
      "         7.0       0.16      0.11      0.13       250\n",
      "         3.0       0.20      0.26      0.22       421\n",
      "         6.0       0.25      0.12      0.16       424\n",
      "         2.0       0.29      0.47      0.36       519\n",
      "         9.0       0.46      0.29      0.35       386\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.29      0.27      0.26      3438\n",
      "weighted avg       0.32      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.3371146015125073\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.79      0.60       594\n",
      "         8.0       0.23      0.14      0.17       258\n",
      "         1.0       0.20      0.10      0.13       242\n",
      "         4.0       0.19      0.14      0.16       307\n",
      "         7.0       0.17      0.05      0.08       279\n",
      "         3.0       0.24      0.26      0.25       448\n",
      "         6.0       0.21      0.16      0.18       375\n",
      "         2.0       0.36      0.43      0.39       578\n",
      "         9.0       0.35      0.40      0.37       357\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.3356602675974404\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.81      0.62       632\n",
      "         8.0       0.14      0.07      0.09       259\n",
      "         1.0       0.14      0.10      0.12       238\n",
      "         4.0       0.21      0.16      0.18       291\n",
      "         7.0       0.21      0.05      0.08       286\n",
      "         3.0       0.22      0.30      0.25       420\n",
      "         6.0       0.20      0.12      0.15       394\n",
      "         2.0       0.31      0.43      0.36       540\n",
      "         9.0       0.41      0.33      0.36       378\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33187899941826643\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.75      0.62       612\n",
      "         8.0       0.16      0.11      0.13       254\n",
      "         1.0       0.15      0.08      0.11       248\n",
      "         4.0       0.22      0.16      0.19       305\n",
      "         7.0       0.20      0.12      0.15       243\n",
      "         3.0       0.23      0.26      0.24       449\n",
      "         6.0       0.20      0.13      0.16       405\n",
      "         2.0       0.32      0.48      0.38       557\n",
      "         9.0       0.41      0.33      0.37       365\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3307155322862129\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       633\n",
      "         8.0       0.20      0.09      0.12       265\n",
      "         1.0       0.17      0.13      0.15       237\n",
      "         4.0       0.21      0.15      0.17       307\n",
      "         7.0       0.13      0.06      0.08       260\n",
      "         3.0       0.23      0.25      0.24       429\n",
      "         6.0       0.21      0.16      0.18       392\n",
      "         2.0       0.34      0.42      0.37       558\n",
      "         9.0       0.39      0.36      0.38       357\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.337696335078534\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.78      0.60       611\n",
      "         8.0       0.17      0.10      0.13       248\n",
      "         1.0       0.29      0.12      0.17       249\n",
      "         4.0       0.15      0.12      0.13       289\n",
      "         7.0       0.16      0.07      0.10       269\n",
      "         3.0       0.23      0.31      0.26       440\n",
      "         6.0       0.23      0.10      0.14       407\n",
      "         2.0       0.32      0.49      0.39       539\n",
      "         9.0       0.45      0.30      0.36       386\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.27      0.25      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33304246655031994\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.78      0.61       626\n",
      "         8.0       0.18      0.12      0.15       265\n",
      "         1.0       0.13      0.06      0.08       251\n",
      "         4.0       0.19      0.15      0.17       288\n",
      "         7.0       0.14      0.04      0.06       264\n",
      "         3.0       0.25      0.25      0.25       442\n",
      "         6.0       0.17      0.15      0.16       382\n",
      "         2.0       0.32      0.47      0.38       541\n",
      "         9.0       0.46      0.37      0.41       379\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33420593368237345\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.60       618\n",
      "         8.0       0.16      0.10      0.13       248\n",
      "         1.0       0.18      0.09      0.12       235\n",
      "         4.0       0.20      0.14      0.17       308\n",
      "         7.0       0.09      0.03      0.05       265\n",
      "         3.0       0.23      0.34      0.27       427\n",
      "         6.0       0.21      0.10      0.14       417\n",
      "         2.0       0.34      0.45      0.39       556\n",
      "         9.0       0.35      0.30      0.32       364\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3266433973240256\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.79      0.60       617\n",
      "         8.0       0.21      0.08      0.12       275\n",
      "         1.0       0.18      0.11      0.14       230\n",
      "         4.0       0.18      0.09      0.12       302\n",
      "         7.0       0.12      0.09      0.10       250\n",
      "         3.0       0.22      0.28      0.24       412\n",
      "         6.0       0.15      0.08      0.10       434\n",
      "         2.0       0.32      0.44      0.37       558\n",
      "         9.0       0.38      0.34      0.36       360\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.32111692844677137\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.77      0.61       627\n",
      "         8.0       0.19      0.16      0.18       238\n",
      "         1.0       0.19      0.08      0.11       256\n",
      "         4.0       0.19      0.18      0.19       294\n",
      "         7.0       0.16      0.06      0.08       279\n",
      "         3.0       0.25      0.24      0.24       457\n",
      "         6.0       0.21      0.16      0.18       365\n",
      "         2.0       0.30      0.43      0.35       539\n",
      "         9.0       0.36      0.31      0.34       383\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.27      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "mean accuracy 0.33161721931355437\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model14 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.78      0.60       612\n",
      "         8.0       0.17      0.11      0.13       253\n",
      "         1.0       0.23      0.05      0.08       253\n",
      "         4.0       0.24      0.21      0.22       302\n",
      "         7.0       0.11      0.04      0.05       277\n",
      "         3.0       0.20      0.24      0.22       435\n",
      "         6.0       0.19      0.09      0.12       386\n",
      "         2.0       0.28      0.44      0.34       551\n",
      "         9.0       0.37      0.34      0.35       369\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3193717277486911\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.83      0.63       632\n",
      "         8.0       0.16      0.07      0.09       260\n",
      "         1.0       0.17      0.09      0.12       233\n",
      "         4.0       0.27      0.17      0.21       294\n",
      "         7.0       0.13      0.04      0.07       252\n",
      "         3.0       0.23      0.28      0.26       434\n",
      "         6.0       0.18      0.11      0.14       413\n",
      "         2.0       0.32      0.50      0.39       546\n",
      "         9.0       0.43      0.33      0.37       374\n",
      "\n",
      "   micro avg       0.35      0.35      0.35      3438\n",
      "   macro avg       0.27      0.27      0.25      3438\n",
      "weighted avg       0.30      0.35      0.31      3438\n",
      "\n",
      "accuracy:  0.34554973821989526\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.80      0.62       629\n",
      "         8.0       0.14      0.10      0.12       232\n",
      "         1.0       0.15      0.07      0.09       259\n",
      "         4.0       0.19      0.18      0.19       278\n",
      "         7.0       0.04      0.01      0.01       278\n",
      "         3.0       0.23      0.24      0.24       442\n",
      "         6.0       0.19      0.11      0.14       416\n",
      "         2.0       0.31      0.47      0.38       548\n",
      "         9.0       0.38      0.35      0.36       356\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.26      0.24      3438\n",
      "weighted avg       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.81      0.61       615\n",
      "         8.0       0.22      0.06      0.09       281\n",
      "         1.0       0.22      0.11      0.14       227\n",
      "         4.0       0.20      0.11      0.14       318\n",
      "         7.0       0.20      0.09      0.12       251\n",
      "         3.0       0.22      0.30      0.26       427\n",
      "         6.0       0.20      0.10      0.14       383\n",
      "         2.0       0.31      0.51      0.38       549\n",
      "         9.0       0.39      0.27      0.32       387\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.26      0.25      3438\n",
      "weighted avg       0.30      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.33391506689936007\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.83      0.62       636\n",
      "         8.0       0.20      0.07      0.10       258\n",
      "         1.0       0.21      0.10      0.13       254\n",
      "         4.0       0.25      0.14      0.18       304\n",
      "         7.0       0.16      0.04      0.07       270\n",
      "         3.0       0.22      0.29      0.25       437\n",
      "         6.0       0.18      0.12      0.14       359\n",
      "         2.0       0.32      0.47      0.38       553\n",
      "         9.0       0.39      0.34      0.36       367\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.27      0.25      3438\n",
      "weighted avg       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.3411867364746946\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.80      0.63       608\n",
      "         8.0       0.09      0.04      0.06       255\n",
      "         1.0       0.20      0.12      0.15       232\n",
      "         4.0       0.18      0.15      0.16       292\n",
      "         7.0       0.14      0.03      0.06       259\n",
      "         3.0       0.22      0.30      0.25       432\n",
      "         6.0       0.20      0.07      0.10       440\n",
      "         2.0       0.30      0.49      0.37       544\n",
      "         9.0       0.44      0.34      0.38       376\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.81      0.61       613\n",
      "         8.0       0.20      0.06      0.10       262\n",
      "         1.0       0.22      0.13      0.16       240\n",
      "         4.0       0.23      0.12      0.15       320\n",
      "         7.0       0.12      0.05      0.07       261\n",
      "         3.0       0.25      0.28      0.26       436\n",
      "         6.0       0.12      0.08      0.09       395\n",
      "         2.0       0.30      0.54      0.39       526\n",
      "         9.0       0.46      0.31      0.37       385\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.24      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3336242001163467\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.82      0.62       631\n",
      "         8.0       0.17      0.09      0.12       251\n",
      "         1.0       0.18      0.07      0.10       246\n",
      "         4.0       0.19      0.19      0.19       276\n",
      "         7.0       0.15      0.03      0.05       268\n",
      "         3.0       0.22      0.29      0.25       433\n",
      "         6.0       0.23      0.11      0.15       404\n",
      "         2.0       0.31      0.40      0.35       571\n",
      "         9.0       0.33      0.32      0.32       358\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       620\n",
      "         8.0       0.22      0.11      0.15       244\n",
      "         1.0       0.22      0.05      0.09       260\n",
      "         4.0       0.18      0.15      0.16       295\n",
      "         7.0       0.12      0.05      0.07       275\n",
      "         3.0       0.22      0.26      0.24       444\n",
      "         6.0       0.20      0.14      0.17       373\n",
      "         2.0       0.31      0.43      0.36       556\n",
      "         9.0       0.41      0.34      0.37       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3307155322862129\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.79      0.61       624\n",
      "         8.0       0.17      0.08      0.11       269\n",
      "         1.0       0.18      0.10      0.13       226\n",
      "         4.0       0.21      0.11      0.14       301\n",
      "         7.0       0.11      0.05      0.07       254\n",
      "         3.0       0.22      0.29      0.25       425\n",
      "         6.0       0.16      0.07      0.09       426\n",
      "         2.0       0.30      0.50      0.38       541\n",
      "         9.0       0.43      0.34      0.38       372\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "mean accuracy 0.3319662594531704\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model15 = sum(acc)/10 \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.83      0.63       610\n",
      "         8.0       0.19      0.05      0.08       256\n",
      "         1.0       0.24      0.07      0.11       237\n",
      "         4.0       0.20      0.15      0.17       305\n",
      "         7.0       0.15      0.04      0.06       250\n",
      "         3.0       0.21      0.31      0.25       453\n",
      "         6.0       0.18      0.07      0.10       398\n",
      "         2.0       0.29      0.44      0.35       561\n",
      "         9.0       0.39      0.35      0.37       368\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.23      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3298429319371728\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.82      0.61       634\n",
      "         8.0       0.12      0.02      0.04       257\n",
      "         1.0       0.18      0.04      0.06       249\n",
      "         4.0       0.19      0.13      0.15       291\n",
      "         7.0       0.20      0.02      0.04       279\n",
      "         3.0       0.20      0.26      0.23       416\n",
      "         6.0       0.12      0.05      0.07       401\n",
      "         2.0       0.26      0.49      0.34       536\n",
      "         9.0       0.37      0.31      0.34       375\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.24      0.21      3438\n",
      "weighted avg       0.26      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3158813263525305\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.80      0.63       630\n",
      "         8.0       0.18      0.04      0.07       274\n",
      "         1.0       0.21      0.04      0.07       245\n",
      "         4.0       0.21      0.12      0.15       307\n",
      "         7.0       0.12      0.07      0.09       238\n",
      "         3.0       0.20      0.28      0.23       419\n",
      "         6.0       0.19      0.07      0.10       408\n",
      "         2.0       0.29      0.53      0.38       548\n",
      "         9.0       0.38      0.28      0.32       369\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.23      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3263525305410122\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.85      0.61       614\n",
      "         8.0       0.17      0.06      0.09       239\n",
      "         1.0       0.23      0.05      0.08       241\n",
      "         4.0       0.24      0.16      0.19       289\n",
      "         7.0       0.20      0.00      0.01       291\n",
      "         3.0       0.24      0.27      0.25       450\n",
      "         6.0       0.15      0.08      0.11       391\n",
      "         2.0       0.28      0.50      0.36       549\n",
      "         9.0       0.42      0.34      0.38       374\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.26      0.23      3438\n",
      "weighted avg       0.29      0.34      0.28      3438\n",
      "\n",
      "accuracy:  0.33595113438045376\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.84      0.64       656\n",
      "         8.0       0.24      0.06      0.09       254\n",
      "         1.0       0.24      0.09      0.13       237\n",
      "         4.0       0.21      0.16      0.18       284\n",
      "         7.0       0.11      0.01      0.02       277\n",
      "         3.0       0.21      0.30      0.25       414\n",
      "         6.0       0.17      0.04      0.07       414\n",
      "         2.0       0.28      0.54      0.37       527\n",
      "         9.0       0.44      0.34      0.38       375\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.26      0.24      3438\n",
      "weighted avg       0.30      0.34      0.29      3438\n",
      "\n",
      "accuracy:  0.34467713787085513\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.46      0.80      0.59       588\n",
      "         8.0       0.09      0.02      0.03       259\n",
      "         1.0       0.18      0.04      0.07       249\n",
      "         4.0       0.21      0.15      0.18       312\n",
      "         7.0       0.20      0.04      0.07       252\n",
      "         3.0       0.21      0.23      0.22       455\n",
      "         6.0       0.15      0.11      0.13       385\n",
      "         2.0       0.30      0.50      0.38       570\n",
      "         9.0       0.33      0.29      0.31       368\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.24      0.22      3438\n",
      "weighted avg       0.26      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3150087260034904\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.84      0.61       621\n",
      "         8.0       0.24      0.05      0.08       277\n",
      "         1.0       0.33      0.08      0.12       261\n",
      "         4.0       0.21      0.08      0.11       318\n",
      "         7.0       0.15      0.03      0.06       258\n",
      "         3.0       0.19      0.33      0.24       410\n",
      "         6.0       0.18      0.08      0.11       392\n",
      "         2.0       0.31      0.49      0.38       542\n",
      "         9.0       0.37      0.26      0.31       359\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.25      0.22      3438\n",
      "weighted avg       0.29      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.81      0.63       623\n",
      "         8.0       0.12      0.04      0.06       236\n",
      "         1.0       0.15      0.03      0.05       225\n",
      "         4.0       0.16      0.21      0.18       278\n",
      "         7.0       0.00      0.00      0.00       271\n",
      "         3.0       0.23      0.19      0.21       459\n",
      "         6.0       0.16      0.07      0.10       407\n",
      "         2.0       0.27      0.53      0.36       555\n",
      "         9.0       0.41      0.35      0.38       384\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.23      0.25      0.22      3438\n",
      "weighted avg       0.26      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3263525305410122\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.85      0.63       633\n",
      "         8.0       0.21      0.05      0.08       267\n",
      "         1.0       0.33      0.02      0.04       252\n",
      "         4.0       0.21      0.14      0.17       301\n",
      "         7.0       0.18      0.03      0.05       256\n",
      "         3.0       0.23      0.23      0.23       444\n",
      "         6.0       0.16      0.07      0.10       401\n",
      "         2.0       0.25      0.51      0.33       522\n",
      "         9.0       0.31      0.30      0.30       362\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.24      0.21      3438\n",
      "weighted avg       0.28      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3222803955788249\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.81      0.61       611\n",
      "         8.0       0.04      0.01      0.01       246\n",
      "         1.0       0.17      0.09      0.12       234\n",
      "         4.0       0.19      0.13      0.15       295\n",
      "         7.0       0.03      0.00      0.01       273\n",
      "         3.0       0.20      0.30      0.24       425\n",
      "         6.0       0.15      0.07      0.10       398\n",
      "         2.0       0.32      0.50      0.39       575\n",
      "         9.0       0.41      0.32      0.36       381\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.22      0.25      0.22      3438\n",
      "weighted avg       0.26      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3266433973240256\n",
      "mean accuracy 0.32687609075043633\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model16 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.80      0.61       624\n",
      "         8.0       0.07      0.02      0.03       244\n",
      "         1.0       0.15      0.05      0.08       233\n",
      "         4.0       0.26      0.14      0.18       308\n",
      "         7.0       0.19      0.02      0.04       282\n",
      "         3.0       0.21      0.33      0.26       421\n",
      "         6.0       0.12      0.03      0.05       406\n",
      "         2.0       0.27      0.55      0.37       541\n",
      "         9.0       0.39      0.25      0.30       379\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.24      0.21      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3219895287958115\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.82      0.62       620\n",
      "         8.0       0.25      0.01      0.02       269\n",
      "         1.0       0.18      0.04      0.06       253\n",
      "         4.0       0.18      0.15      0.16       288\n",
      "         7.0       0.19      0.01      0.02       247\n",
      "         3.0       0.19      0.21      0.20       448\n",
      "         6.0       0.14      0.06      0.09       393\n",
      "         2.0       0.27      0.55      0.36       556\n",
      "         9.0       0.38      0.32      0.34       364\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.24      0.21      3438\n",
      "weighted avg       0.28      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3216986620127981\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       622\n",
      "         8.0       0.05      0.00      0.01       251\n",
      "         1.0       0.22      0.02      0.04       245\n",
      "         4.0       0.23      0.15      0.18       292\n",
      "         7.0       0.13      0.03      0.05       244\n",
      "         3.0       0.20      0.25      0.23       430\n",
      "         6.0       0.16      0.05      0.07       419\n",
      "         2.0       0.28      0.55      0.37       564\n",
      "         9.0       0.36      0.32      0.34       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.24      0.21      3438\n",
      "weighted avg       0.27      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32809773123909247\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.81      0.60       622\n",
      "         8.0       0.17      0.03      0.05       262\n",
      "         1.0       0.17      0.03      0.05       241\n",
      "         4.0       0.20      0.17      0.18       304\n",
      "         7.0       0.00      0.00      0.00       285\n",
      "         3.0       0.24      0.33      0.27       439\n",
      "         6.0       0.14      0.05      0.08       380\n",
      "         2.0       0.28      0.56      0.37       533\n",
      "         9.0       0.44      0.28      0.34       372\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.25      0.22      3438\n",
      "weighted avg       0.27      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3307155322862129\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.80      0.61       627\n",
      "         8.0       0.15      0.01      0.01       263\n",
      "         1.0       0.14      0.04      0.06       241\n",
      "         4.0       0.19      0.13      0.16       282\n",
      "         7.0       0.12      0.01      0.01       267\n",
      "         3.0       0.22      0.31      0.26       434\n",
      "         6.0       0.20      0.08      0.12       385\n",
      "         2.0       0.29      0.52      0.37       564\n",
      "         9.0       0.34      0.31      0.33       375\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.25      0.21      3438\n",
      "weighted avg       0.27      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3286794648051193\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.62       617\n",
      "         8.0       0.08      0.03      0.04       250\n",
      "         1.0       0.17      0.02      0.03       245\n",
      "         4.0       0.22      0.17      0.19       314\n",
      "         7.0       0.22      0.03      0.05       262\n",
      "         3.0       0.21      0.26      0.23       435\n",
      "         6.0       0.18      0.04      0.06       414\n",
      "         2.0       0.27      0.61      0.37       533\n",
      "         9.0       0.46      0.25      0.32       368\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.21      3438\n",
      "weighted avg       0.28      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.83      0.61       612\n",
      "         8.0       0.26      0.02      0.04       261\n",
      "         1.0       0.12      0.02      0.03       231\n",
      "         4.0       0.19      0.17      0.18       292\n",
      "         7.0       0.11      0.02      0.04       252\n",
      "         3.0       0.21      0.24      0.23       446\n",
      "         6.0       0.16      0.08      0.11       399\n",
      "         2.0       0.29      0.46      0.36       579\n",
      "         9.0       0.31      0.35      0.33       366\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.24      0.21      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3205351948807446\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       632\n",
      "         8.0       0.11      0.02      0.03       252\n",
      "         1.0       0.15      0.04      0.06       255\n",
      "         4.0       0.22      0.11      0.15       304\n",
      "         7.0       0.18      0.01      0.01       277\n",
      "         3.0       0.20      0.29      0.24       423\n",
      "         6.0       0.21      0.03      0.05       400\n",
      "         2.0       0.26      0.63      0.37       518\n",
      "         9.0       0.43      0.23      0.30       377\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.24      0.20      3438\n",
      "weighted avg       0.28      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.82      0.62       629\n",
      "         8.0       0.16      0.03      0.05       245\n",
      "         1.0       0.32      0.08      0.12       246\n",
      "         4.0       0.23      0.14      0.18       322\n",
      "         7.0       0.13      0.04      0.06       257\n",
      "         3.0       0.24      0.31      0.27       429\n",
      "         6.0       0.16      0.04      0.07       396\n",
      "         2.0       0.28      0.54      0.37       553\n",
      "         9.0       0.32      0.27      0.29       361\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.25      0.22      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3321698662012798\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       615\n",
      "         8.0       0.17      0.01      0.01       268\n",
      "         1.0       0.36      0.04      0.07       240\n",
      "         4.0       0.20      0.18      0.19       274\n",
      "         7.0       0.18      0.01      0.02       272\n",
      "         3.0       0.21      0.29      0.24       440\n",
      "         6.0       0.19      0.05      0.08       403\n",
      "         2.0       0.27      0.56      0.37       544\n",
      "         9.0       0.45      0.31      0.37       382\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.25      0.22      3438\n",
      "weighted avg       0.30      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3310063990692263\n",
      "mean accuracy 0.3264688772542176\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model17 = sum(acc)/10\n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3116055846422338, 0.3286503781268179, 0.3303955788248982, 0.33025014543339154, 0.32838859802210596, 0.33161721931355437, 0.3319662594531704, 0.32687609075043633, 0.3264688772542176]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_minkowski\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
