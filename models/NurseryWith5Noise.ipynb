{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Parents', ' Has_nurs', ' Form', ' Children', ' Housing',\n",
       "       ' Finance', ' Social', ' Health', 'Class'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nurseryNum = pd.read_csv(\"nursery_numerical.csv\")\n",
    "nurseryNum.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "j = 1\n",
    "features = ['Parents', ' Has_nurs', ' Form', ' Children', ' Housing', ' Finance', ' Social', ' Health']\n",
    "for index, m in nurseryNum.iterrows():\n",
    "    if index % 40 == 0:\n",
    "        nurseryNum.at[index+2, features[j]] = i + 1\n",
    "        j += 3\n",
    "        i+=10\n",
    "        if j >= 7:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for index, m in nurseryNum.iterrows():\n",
    "    if index % 40 == 0:\n",
    "        if index < 12954:\n",
    "            nurseryNum.at[index+4,:] = nurseryNum.loc[j,:]\n",
    "        else:\n",
    "            nurseryNum.at[index+1,:] = nurseryNum.loc[j,:]\n",
    "        j += 120\n",
    "        if j >= 12959:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurseryNum.to_csv('/home/valia/Documents/AppliedDataScience/nureryNoise5perCent.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nurseryNum.iloc[:,0:8]\n",
    "labels = nurseryNum.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True) #5 fores me 2 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean and k tuning on 5% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.86      0.84      2145\n",
      "    priority       0.78      0.81      0.79      2163\n",
      "  spec_prior       0.40      1.00      0.57         2\n",
      "   recommend       0.87      0.77      0.81      2007\n",
      "   not_recom       0.58      0.68      0.63       163\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.69      0.82      0.73      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8101851851851852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.83      0.83      2138\n",
      "    priority       0.76      0.80      0.78      2110\n",
      "  spec_prior       0.50      1.00      0.67         3\n",
      "   recommend       0.85      0.77      0.81      2070\n",
      "   not_recom       0.56      0.76      0.64       159\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.70      0.83      0.74      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7989197530864197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.86      0.84      2117\n",
      "    priority       0.76      0.82      0.79      2153\n",
      "  spec_prior       0.40      1.00      0.57         2\n",
      "   recommend       0.88      0.75      0.81      2064\n",
      "   not_recom       0.56      0.76      0.65       144\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.68      0.84      0.73      6480\n",
      "weighted avg       0.82      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.808641975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.86      0.85      2166\n",
      "    priority       0.76      0.80      0.78      2120\n",
      "  spec_prior       0.33      1.00      0.50         3\n",
      "   recommend       0.85      0.77      0.81      2013\n",
      "   not_recom       0.58      0.72      0.64       178\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.67      0.83      0.72      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8060185185185185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valia/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.86      0.84      2101\n",
      "    priority       0.77      0.82      0.79      2179\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.87      0.75      0.81      2031\n",
      "   not_recom       0.57      0.71      0.63       164\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.61      0.63      0.61      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8083333333333333\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valia/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1145: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples.\n",
      "  'recall', 'true', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.85      0.85      0.85      2182\n",
      "    priority       0.76      0.83      0.80      2094\n",
      "  spec_prior       0.00      0.00      0.00         0\n",
      "   recommend       0.87      0.77      0.82      2046\n",
      "   not_recom       0.57      0.76      0.65       158\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6480\n",
      "   macro avg       0.61      0.64      0.62      6480\n",
      "weighted avg       0.82      0.82      0.82      6480\n",
      "\n",
      "accuracy:  0.8157407407407408\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.83      0.83      2181\n",
      "    priority       0.76      0.81      0.78      2110\n",
      "  spec_prior       0.50      1.00      0.67         3\n",
      "   recommend       0.86      0.75      0.80      2033\n",
      "   not_recom       0.50      0.73      0.59       153\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.69      0.82      0.73      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7978395061728395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.87      0.84      2102\n",
      "    priority       0.77      0.81      0.79      2163\n",
      "  spec_prior       0.40      1.00      0.57         2\n",
      "   recommend       0.88      0.74      0.80      2044\n",
      "   not_recom       0.59      0.67      0.62       169\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.69      0.82      0.73      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8050925925925926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.85      0.84      2135\n",
      "    priority       0.79      0.81      0.80      2123\n",
      "  spec_prior       0.80      1.00      0.89         4\n",
      "   recommend       0.85      0.78      0.81      2054\n",
      "   not_recom       0.59      0.71      0.64       164\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.77      0.83      0.80      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8114197530864198\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.86      0.85      2148\n",
      "    priority       0.76      0.83      0.79      2150\n",
      "  spec_prior       0.14      1.00      0.25         1\n",
      "   recommend       0.89      0.75      0.81      2023\n",
      "   not_recom       0.52      0.68      0.59       158\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.63      0.82      0.66      6480\n",
      "weighted avg       0.82      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8114197530864198\n",
      "mean accuracy 0.8073611111111113\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2120\n",
      "    priority       0.88      0.92      0.90      2153\n",
      "  spec_prior       0.40      1.00      0.57         2\n",
      "   recommend       0.95      0.87      0.91      2057\n",
      "   not_recom       0.94      0.72      0.81       148\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.82      0.89      0.83      6480\n",
      "weighted avg       0.92      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9141975308641975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.95      2163\n",
      "    priority       0.87      0.93      0.90      2120\n",
      "  spec_prior       0.33      0.67      0.44         3\n",
      "   recommend       0.94      0.86      0.89      2020\n",
      "   not_recom       0.96      0.62      0.75       174\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.80      0.81      0.79      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9103395061728395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2166\n",
      "    priority       0.87      0.92      0.89      2176\n",
      "  spec_prior       1.00      1.00      1.00         2\n",
      "   recommend       0.93      0.86      0.89      1988\n",
      "   not_recom       0.93      0.67      0.78       148\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.93      0.88      0.90      6480\n",
      "weighted avg       0.91      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9044753086419753\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2117\n",
      "    priority       0.84      0.93      0.88      2097\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.95      0.83      0.89      2089\n",
      "   not_recom       0.95      0.60      0.73       174\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.66      0.69      6480\n",
      "weighted avg       0.90      0.90      0.89      6480\n",
      "\n",
      "accuracy:  0.8953703703703704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2108\n",
      "    priority       0.86      0.92      0.89      2146\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.94      0.84      0.89      2067\n",
      "   not_recom       0.93      0.71      0.80       156\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.68      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9018518518518519\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2175\n",
      "    priority       0.87      0.93      0.90      2127\n",
      "  spec_prior       0.29      1.00      0.44         2\n",
      "   recommend       0.93      0.85      0.89      2010\n",
      "   not_recom       0.92      0.66      0.77       166\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.79      0.88      0.79      6480\n",
      "weighted avg       0.91      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9040123456790123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2164\n",
      "    priority       0.88      0.93      0.90      2134\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.87      0.90      2028\n",
      "   not_recom       0.91      0.64      0.75       150\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.68      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9111111111111111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2119\n",
      "    priority       0.85      0.92      0.88      2139\n",
      "  spec_prior       0.14      1.00      0.25         1\n",
      "   recommend       0.94      0.84      0.89      2049\n",
      "   not_recom       0.94      0.57      0.71       172\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.76      0.86      0.73      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8983024691358025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.95      0.94      2140\n",
      "    priority       0.85      0.94      0.89      2110\n",
      "  spec_prior       0.29      1.00      0.44         2\n",
      "   recommend       0.95      0.84      0.89      2070\n",
      "   not_recom       0.93      0.65      0.77       158\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.79      0.88      0.79      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9060185185185186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.94      2143\n",
      "    priority       0.87      0.92      0.90      2163\n",
      "  spec_prior       0.75      1.00      0.86         3\n",
      "   recommend       0.94      0.86      0.90      2007\n",
      "   not_recom       0.93      0.70      0.79       164\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.88      0.89      0.88      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9103395061728395\n",
      "mean accuracy 0.9056018518518518\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.97      0.94      2115\n",
      "    priority       0.87      0.92      0.89      2161\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.95      0.84      0.89      2056\n",
      "   not_recom       0.86      0.78      0.81       147\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.72      0.70      0.71      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9080246913580247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2168\n",
      "    priority       0.85      0.94      0.89      2112\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.84      0.89      2021\n",
      "   not_recom       0.92      0.70      0.79       175\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.69      0.71      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9089506172839507\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2133\n",
      "    priority       0.87      0.92      0.90      2139\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.87      0.90      2055\n",
      "   not_recom       0.86      0.81      0.84       149\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.72      0.71      0.72      6480\n",
      "weighted avg       0.92      0.92      0.91      6480\n",
      "\n",
      "accuracy:  0.9152777777777777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.97      0.94      2150\n",
      "    priority       0.85      0.93      0.89      2134\n",
      "  spec_prior       0.17      1.00      0.29         1\n",
      "   recommend       0.94      0.82      0.87      2022\n",
      "   not_recom       0.95      0.61      0.74       173\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.76      0.87      0.75      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.899074074074074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.97      0.95      2134\n",
      "    priority       0.85      0.92      0.88      2143\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.93      0.84      0.88      2027\n",
      "   not_recom       0.89      0.66      0.76       173\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.68      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9023148148148148\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.97      0.96      2149\n",
      "    priority       0.89      0.94      0.91      2130\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.94      0.88      0.91      2050\n",
      "   not_recom       0.90      0.77      0.83       149\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.71      0.72      6480\n",
      "weighted avg       0.93      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9246913580246914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2152\n",
      "    priority       0.86      0.93      0.90      2131\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.84      0.89      2028\n",
      "   not_recom       0.90      0.69      0.78       165\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.72      0.68      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9060185185185186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.95      2131\n",
      "    priority       0.88      0.92      0.90      2142\n",
      "  spec_prior       0.50      1.00      0.67         1\n",
      "   recommend       0.93      0.88      0.91      2049\n",
      "   not_recom       0.94      0.66      0.77       157\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.84      0.88      0.84      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.912962962962963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.97      0.95      2121\n",
      "    priority       0.86      0.92      0.89      2160\n",
      "  spec_prior       1.00      1.00      1.00         1\n",
      "   recommend       0.93      0.85      0.89      2032\n",
      "   not_recom       0.95      0.63      0.75       166\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.93      0.87      0.90      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9060185185185186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.94      2162\n",
      "    priority       0.85      0.92      0.88      2113\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.83      0.88      2045\n",
      "   not_recom       0.82      0.73      0.77       156\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.71      0.69      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9003086419753087\n",
      "mean accuracy 0.9083641975308643\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2141\n",
      "    priority       0.86      0.93      0.90      2148\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.93      0.85      0.89      2021\n",
      "   not_recom       0.93      0.67      0.78       167\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.68      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9100308641975309\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.95      0.95      2142\n",
      "    priority       0.86      0.93      0.89      2125\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.92      0.85      0.88      2056\n",
      "   not_recom       0.96      0.71      0.82       155\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.69      0.71      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9070987654320988\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.95      0.94      2119\n",
      "    priority       0.86      0.93      0.90      2138\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.87      0.90      2040\n",
      "   not_recom       0.97      0.57      0.72       179\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.66      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9078703703703703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.95      0.94      2164\n",
      "    priority       0.84      0.93      0.88      2135\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.92      0.83      0.88      2037\n",
      "   not_recom       0.98      0.64      0.78       143\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.74      0.67      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8989197530864198\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2135\n",
      "    priority       0.88      0.93      0.91      2160\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.93      0.88      0.90      2035\n",
      "   not_recom       0.93      0.66      0.77       148\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.69      0.71      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9162037037037037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.96      0.96      2148\n",
      "    priority       0.85      0.93      0.89      2113\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.93      0.86      0.89      2042\n",
      "   not_recom       0.94      0.64      0.76       174\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.68      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9117283950617284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.95      0.95      2165\n",
      "    priority       0.87      0.94      0.90      2125\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.92      0.87      0.90      2012\n",
      "   not_recom       0.94      0.58      0.72       177\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.67      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.97      0.96      2118\n",
      "    priority       0.86      0.94      0.90      2148\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.85      0.89      2065\n",
      "   not_recom       0.90      0.76      0.82       145\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.70      0.71      6480\n",
      "weighted avg       0.92      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9140432098765432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.96      0.95      2122\n",
      "    priority       0.88      0.92      0.90      2187\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.92      0.89      0.91      2004\n",
      "   not_recom       0.95      0.66      0.78       166\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.69      0.71      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9174382716049383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2161\n",
      "    priority       0.85      0.93      0.89      2086\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.93      0.84      0.88      2073\n",
      "   not_recom       0.90      0.69      0.78       156\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.68      0.70      6480\n",
      "weighted avg       0.91      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9046296296296297\n",
      "mean accuracy 0.9100462962962963\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.96      0.96      2166\n",
      "    priority       0.87      0.93      0.90      2146\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.91      0.87      0.89      2008\n",
      "   not_recom       0.97      0.61      0.75       157\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.67      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9135802469135802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.97      0.95      2117\n",
      "    priority       0.87      0.91      0.89      2127\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.93      0.87      0.90      2069\n",
      "   not_recom       0.92      0.55      0.69       165\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.66      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9108024691358024\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2161\n",
      "    priority       0.89      0.92      0.90      2160\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.92      0.88      0.90      2008\n",
      "   not_recom       0.94      0.60      0.73       149\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.67      0.70      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9171296296296296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.97      0.96      2122\n",
      "    priority       0.88      0.94      0.91      2113\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.95      0.87      0.91      2069\n",
      "   not_recom       0.95      0.61      0.74       173\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.68      0.70      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9205246913580247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.97      0.95      2093\n",
      "    priority       0.87      0.93      0.90      2176\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.94      0.86      0.90      2042\n",
      "   not_recom       0.94      0.56      0.70       167\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.67      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9126543209876543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.97      0.96      2190\n",
      "    priority       0.87      0.93      0.90      2097\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.88      0.90      2035\n",
      "   not_recom       0.96      0.58      0.72       155\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.67      0.70      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9166666666666666\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2180\n",
      "    priority       0.87      0.92      0.89      2115\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.90      0.91      2001\n",
      "   not_recom       0.96      0.49      0.64       181\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.65      0.68      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9118827160493828\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.97      0.96      2103\n",
      "    priority       0.88      0.92      0.90      2158\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.93      0.88      0.90      2076\n",
      "   not_recom       0.92      0.65      0.76       141\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.68      0.70      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.917283950617284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.96      0.96      2160\n",
      "    priority       0.87      0.94      0.90      2143\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.94      0.87      0.90      2003\n",
      "   not_recom       0.92      0.61      0.73       171\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.68      0.70      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9195987654320987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.97      0.96      2123\n",
      "    priority       0.89      0.92      0.91      2130\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.92      0.91      0.91      2074\n",
      "   not_recom       0.95      0.63      0.76       151\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.68      0.71      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9220679012345679\n",
      "mean accuracy 0.916219135802469\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.96      0.93      2150\n",
      "    priority       0.85      0.92      0.88      2100\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.93      0.84      0.88      2057\n",
      "   not_recom       0.92      0.38      0.54       169\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.72      0.62      0.65      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8921296296296296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.94      0.94      2133\n",
      "    priority       0.88      0.91      0.90      2173\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.90      0.89      0.90      2020\n",
      "   not_recom       0.98      0.54      0.70       153\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.66      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.907716049382716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.94      2141\n",
      "    priority       0.87      0.91      0.89      2147\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.89      0.88      0.88      2024\n",
      "   not_recom       0.92      0.43      0.59       163\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.63      0.66      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8984567901234568\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2142\n",
      "    priority       0.86      0.93      0.89      2126\n",
      "  spec_prior       0.94      0.85      0.89      2053\n",
      "   not_recom       1.00      0.47      0.64       159\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.93      0.80      0.84      6480\n",
      "weighted avg       0.91      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9041666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.93      2170\n",
      "    priority       0.87      0.92      0.90      2129\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.90      0.87      0.89      2031\n",
      "   not_recom       0.92      0.56      0.70       145\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.66      0.68      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9020061728395061\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.96      0.93      2113\n",
      "    priority       0.86      0.92      0.89      2144\n",
      "  spec_prior       0.93      0.84      0.89      2046\n",
      "   not_recom       0.97      0.34      0.51       177\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.91      0.77      0.80      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8933641975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2140\n",
      "    priority       0.88      0.91      0.89      2117\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.88      0.90      2048\n",
      "   not_recom       0.95      0.40      0.56       172\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.63      0.66      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.904320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.95      0.94      2143\n",
      "    priority       0.87      0.92      0.89      2156\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.92      0.87      0.89      2029\n",
      "   not_recom       0.96      0.53      0.69       150\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.66      0.68      6480\n",
      "weighted avg       0.91      0.91      0.90      6480\n",
      "\n",
      "accuracy:  0.9060185185185186\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2162\n",
      "    priority       0.87      0.90      0.89      2116\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.90      0.88      0.89      2037\n",
      "   not_recom       0.95      0.43      0.59       162\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.63      0.66      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8987654320987655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.96      0.93      2121\n",
      "    priority       0.87      0.92      0.90      2157\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.93      0.86      0.89      2040\n",
      "   not_recom       0.95      0.51      0.66       160\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.65      0.68      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9024691358024691\n",
      "mean accuracy 0.9009413580246914\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.92      0.88      2118\n",
      "    priority       0.85      0.87      0.86      2136\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.90      0.83      0.86      2072\n",
      "   not_recom       0.94      0.39      0.55       150\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.70      0.60      0.63      6480\n",
      "weighted avg       0.86      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.8598765432098765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.90      0.90      2165\n",
      "    priority       0.84      0.90      0.87      2137\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.88      0.85      0.87      2005\n",
      "   not_recom       0.96      0.29      0.45       172\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.72      0.59      0.62      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.871141975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.85      0.92      0.88      2098\n",
      "    priority       0.85      0.89      0.87      2174\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.89      0.82      0.85      2049\n",
      "   not_recom       0.94      0.31      0.47       158\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.71      0.59      0.61      6480\n",
      "weighted avg       0.86      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.8625\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.89      0.89      2185\n",
      "    priority       0.83      0.89      0.86      2099\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.88      0.86      0.87      2028\n",
      "   not_recom       0.90      0.34      0.49       164\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.70      0.60      0.62      6480\n",
      "weighted avg       0.87      0.87      0.86      6480\n",
      "\n",
      "accuracy:  0.8665123456790124\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.92      0.87      2118\n",
      "    priority       0.84      0.86      0.85      2135\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.90      0.81      0.85      2063\n",
      "   not_recom       0.89      0.39      0.55       160\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.69      0.60      0.62      6480\n",
      "weighted avg       0.85      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.8510802469135802\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.90      0.89      2165\n",
      "    priority       0.84      0.90      0.87      2138\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.88      0.86      0.87      2014\n",
      "   not_recom       0.98      0.32      0.48       162\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.72      0.60      0.62      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.8714506172839506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.88      0.92      0.90      2184\n",
      "    priority       0.84      0.90      0.87      2132\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.89      0.83      0.86      1999\n",
      "   not_recom       0.95      0.38      0.55       162\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.71      0.61      0.64      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.871141975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.87      0.91      0.89      2099\n",
      "    priority       0.84      0.90      0.87      2141\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.83      0.86      2078\n",
      "   not_recom       0.96      0.33      0.49       160\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.71      0.59      0.62      6480\n",
      "weighted avg       0.87      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.8641975308641975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.85      0.92      0.88      2138\n",
      "    priority       0.84      0.89      0.87      2130\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.90      0.83      0.86      2048\n",
      "   not_recom       0.92      0.28      0.43       162\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.70      0.58      0.61      6480\n",
      "weighted avg       0.87      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.8631172839506173\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.89      0.91      0.90      2145\n",
      "    priority       0.82      0.90      0.86      2143\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.89      0.83      0.86      2029\n",
      "   not_recom       0.95      0.33      0.48       160\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.71      0.59      0.62      6480\n",
      "weighted avg       0.87      0.87      0.86      6480\n",
      "\n",
      "accuracy:  0.8660493827160494\n",
      "mean accuracy 0.8647067901234567\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.74      0.84      0.79      2114\n",
      "    priority       0.82      0.82      0.82      2156\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.86      0.79      0.82      2051\n",
      "   not_recom       0.94      0.19      0.32       157\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.67      0.53      0.55      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8012345679012346\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.78      0.81      0.80      2169\n",
      "    priority       0.81      0.84      0.82      2117\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.82      0.81      0.81      2026\n",
      "   not_recom       0.81      0.10      0.18       165\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.64      0.51      0.52      6480\n",
      "weighted avg       0.80      0.80      0.79      6480\n",
      "\n",
      "accuracy:  0.8015432098765433\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.76      0.81      0.78      2140\n",
      "    priority       0.81      0.84      0.82      2128\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.83      0.80      0.81      2055\n",
      "   not_recom       0.84      0.17      0.28       153\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.65      0.52      0.54      6480\n",
      "weighted avg       0.80      0.80      0.79      6480\n",
      "\n",
      "accuracy:  0.7981481481481482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.75      0.84      0.79      2143\n",
      "    priority       0.82      0.81      0.81      2145\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.84      0.81      0.82      2022\n",
      "   not_recom       0.96      0.15      0.26       169\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.67      0.52      0.54      6480\n",
      "weighted avg       0.81      0.80      0.79      6480\n",
      "\n",
      "accuracy:  0.799537037037037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.77      0.80      0.79      2143\n",
      "    priority       0.81      0.84      0.82      2150\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.80      0.80      0.80      2018\n",
      "   not_recom       0.94      0.10      0.17       167\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.67      0.51      0.52      6480\n",
      "weighted avg       0.80      0.80      0.79      6480\n",
      "\n",
      "accuracy:  0.7955246913580247\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.73      0.85      0.79      2140\n",
      "    priority       0.81      0.82      0.82      2123\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.85      0.76      0.80      2059\n",
      "   not_recom       0.84      0.17      0.29       155\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6480\n",
      "   macro avg       0.65      0.52      0.54      6480\n",
      "weighted avg       0.80      0.79      0.79      6480\n",
      "\n",
      "accuracy:  0.7921296296296296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.78      0.83      0.80      2142\n",
      "    priority       0.81      0.85      0.83      2130\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.84      0.79      0.81      2058\n",
      "   not_recom       0.96      0.15      0.26       149\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.68      0.52      0.54      6480\n",
      "weighted avg       0.81      0.81      0.80      6480\n",
      "\n",
      "accuracy:  0.8078703703703703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.76      0.83      0.79      2141\n",
      "    priority       0.82      0.82      0.82      2143\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.83      0.80      0.82      2019\n",
      "   not_recom       0.89      0.18      0.30       173\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.66      0.53      0.55      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8018518518518518\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.80      0.77      0.79      2210\n",
      "    priority       0.77      0.86      0.81      2067\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.82      0.81      0.82      2044\n",
      "   not_recom       0.79      0.10      0.17       155\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.64      0.51      0.52      6480\n",
      "weighted avg       0.80      0.80      0.79      6480\n",
      "\n",
      "accuracy:  0.7967592592592593\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.69      0.85      0.76      2073\n",
      "    priority       0.83      0.76      0.79      2206\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.84      0.78      0.81      2033\n",
      "   not_recom       0.96      0.14      0.25       167\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.67      0.51      0.52      6480\n",
      "weighted avg       0.79      0.78      0.78      6480\n",
      "\n",
      "accuracy:  0.7807098765432099\n",
      "mean accuracy 0.7975308641975308\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.72      0.72      0.72      2151\n",
      "    priority       0.77      0.82      0.79      2133\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.78      0.78      0.78      2024\n",
      "   not_recom       0.50      0.02      0.03       169\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.55      0.47      0.46      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7533950617283951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.63      0.76      0.69      2132\n",
      "    priority       0.79      0.74      0.77      2140\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.80      0.74      0.77      2053\n",
      "   not_recom       0.93      0.08      0.16       153\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.63      0.47      0.48      6480\n",
      "weighted avg       0.74      0.73      0.73      6480\n",
      "\n",
      "accuracy:  0.7314814814814815\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.67      0.77      0.71      2131\n",
      "    priority       0.81      0.77      0.79      2145\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.80      0.78      0.79      2040\n",
      "   not_recom       0.70      0.04      0.08       162\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.60      0.47      0.48      6480\n",
      "weighted avg       0.76      0.75      0.75      6480\n",
      "\n",
      "accuracy:  0.7541666666666667\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.68      0.73      0.70      2152\n",
      "    priority       0.75      0.76      0.76      2128\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.79      0.76      0.77      2037\n",
      "   not_recom       0.77      0.06      0.12       160\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.60      0.46      0.47      6480\n",
      "weighted avg       0.74      0.73      0.73      6480\n",
      "\n",
      "accuracy:  0.7347222222222223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.69      0.75      0.72      2149\n",
      "    priority       0.80      0.78      0.79      2141\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.79      0.78      0.79      2034\n",
      "   not_recom       0.75      0.06      0.11       153\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.60      0.48      0.48      6480\n",
      "weighted avg       0.76      0.75      0.75      6480\n",
      "\n",
      "accuracy:  0.7549382716049383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.68      0.74      0.71      2134\n",
      "    priority       0.76      0.77      0.77      2132\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.79      0.77      0.78      2043\n",
      "   not_recom       0.73      0.05      0.09       169\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.59      0.47      0.47      6480\n",
      "weighted avg       0.74      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.7412037037037037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.67      0.73      0.70      2158\n",
      "    priority       0.78      0.78      0.78      2125\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.76      0.76      0.76      2031\n",
      "   not_recom       1.00      0.02      0.04       163\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.64      0.46      0.45      6480\n",
      "weighted avg       0.74      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.7351851851851852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.69      0.76      0.73      2125\n",
      "    priority       0.78      0.80      0.79      2148\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.82      0.77      0.79      2046\n",
      "   not_recom       0.81      0.08      0.15       159\n",
      "\n",
      "   micro avg       0.76      0.76      0.76      6480\n",
      "   macro avg       0.62      0.48      0.49      6480\n",
      "weighted avg       0.76      0.76      0.75      6480\n",
      "\n",
      "accuracy:  0.7608024691358025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.67      0.76      0.72      2156\n",
      "    priority       0.80      0.77      0.79      2121\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.80      0.78      0.79      2049\n",
      "   not_recom       0.85      0.11      0.20       151\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.63      0.48      0.50      6480\n",
      "weighted avg       0.76      0.75      0.75      6480\n",
      "\n",
      "accuracy:  0.7549382716049383\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.67      0.74      0.70      2127\n",
      "    priority       0.76      0.78      0.77      2152\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.79      0.75      0.77      2028\n",
      "   not_recom       1.00      0.02      0.03       171\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.64      0.46      0.46      6480\n",
      "weighted avg       0.75      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.7376543209876543\n",
      "mean accuracy 0.7458487654320988\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.8073611111111113, 0.9056018518518518, 0.9083641975308643, 0.9100462962962963, 0.916219135802469, 0.9009413580246914, 0.8647067901234567, 0.7975308641975308, 0.7458487654320988]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_euclidean\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski and k tuning on 5% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.85      0.85      2159\n",
      "    priority       0.79      0.83      0.81      2188\n",
      "  spec_prior       0.33      1.00      0.50         2\n",
      "   recommend       0.85      0.78      0.82      1973\n",
      "   not_recom       0.61      0.77      0.68       158\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6480\n",
      "   macro avg       0.69      0.85      0.73      6480\n",
      "weighted avg       0.82      0.82      0.82      6480\n",
      "\n",
      "accuracy:  0.8194444444444444\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.86      0.84      2124\n",
      "    priority       0.77      0.82      0.79      2085\n",
      "  spec_prior       0.50      1.00      0.67         3\n",
      "   recommend       0.87      0.74      0.80      2104\n",
      "   not_recom       0.58      0.72      0.64       164\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.71      0.83      0.75      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8054012345679012\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.87      0.85      2103\n",
      "    priority       0.78      0.82      0.80      2150\n",
      "  spec_prior       0.40      1.00      0.57         2\n",
      "   recommend       0.88      0.77      0.82      2060\n",
      "   not_recom       0.56      0.73      0.64       165\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6480\n",
      "   macro avg       0.69      0.84      0.74      6480\n",
      "weighted avg       0.83      0.82      0.82      6480\n",
      "\n",
      "accuracy:  0.8205246913580246\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.85      0.84      2180\n",
      "    priority       0.77      0.82      0.79      2123\n",
      "  spec_prior       0.60      1.00      0.75         3\n",
      "   recommend       0.86      0.76      0.81      2017\n",
      "   not_recom       0.65      0.77      0.70       157\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.74      0.84      0.78      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8095679012345679\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.86      0.85      2148\n",
      "    priority       0.78      0.80      0.79      2132\n",
      "  spec_prior       0.25      1.00      0.40         1\n",
      "   recommend       0.85      0.78      0.81      2028\n",
      "   not_recom       0.60      0.75      0.66       171\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.66      0.84      0.70      6480\n",
      "weighted avg       0.82      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8125\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.87      0.85      2135\n",
      "    priority       0.77      0.83      0.80      2141\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.88      0.75      0.81      2049\n",
      "   not_recom       0.55      0.72      0.63       151\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.61      0.63      0.62      6480\n",
      "weighted avg       0.82      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8126543209876543\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.86      0.84      2169\n",
      "    priority       0.78      0.81      0.80      2164\n",
      "  spec_prior       0.33      1.00      0.50         2\n",
      "   recommend       0.86      0.76      0.81      1995\n",
      "   not_recom       0.52      0.71      0.60       150\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.66      0.83      0.71      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8089506172839506\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.86      0.85      2114\n",
      "    priority       0.76      0.84      0.80      2109\n",
      "  spec_prior       0.38      1.00      0.55         3\n",
      "   recommend       0.90      0.75      0.82      2082\n",
      "   not_recom       0.63      0.73      0.67       172\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6480\n",
      "   macro avg       0.70      0.84      0.74      6480\n",
      "weighted avg       0.83      0.82      0.82      6480\n",
      "\n",
      "accuracy:  0.8192901234567901\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.86      0.84      2115\n",
      "    priority       0.79      0.81      0.80      2164\n",
      "  spec_prior       0.25      1.00      0.40         1\n",
      "   recommend       0.86      0.77      0.81      2023\n",
      "   not_recom       0.58      0.69      0.63       177\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.66      0.83      0.70      6480\n",
      "weighted avg       0.82      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8121913580246913\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.87      0.85      2168\n",
      "    priority       0.77      0.83      0.80      2109\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.88      0.76      0.81      2054\n",
      "   not_recom       0.52      0.70      0.60       145\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6480\n",
      "   macro avg       0.60      0.63      0.61      6480\n",
      "weighted avg       0.82      0.82      0.82      6480\n",
      "\n",
      "accuracy:  0.8169753086419753\n",
      "mean accuracy 0.81375\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_minkowski = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2122\n",
      "    priority       0.86      0.93      0.89      2142\n",
      "  spec_prior       0.10      1.00      0.18         1\n",
      "   recommend       0.95      0.85      0.90      2054\n",
      "   not_recom       0.95      0.58      0.72       161\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.76      0.87      0.73      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9067901234567901\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.94      2161\n",
      "    priority       0.88      0.92      0.90      2131\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.88      0.91      2023\n",
      "   not_recom       0.89      0.66      0.76       161\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.68      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.912962962962963\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2164\n",
      "    priority       0.84      0.92      0.88      2084\n",
      "  spec_prior       0.17      1.00      0.29         1\n",
      "   recommend       0.95      0.84      0.89      2073\n",
      "   not_recom       0.91      0.61      0.73       158\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.76      0.87      0.74      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9001543209876544\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.94      2119\n",
      "    priority       0.88      0.92      0.90      2189\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.93      0.86      0.90      2004\n",
      "   not_recom       0.91      0.70      0.79       164\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.69      0.71      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9100308641975309\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2164\n",
      "    priority       0.86      0.92      0.89      2117\n",
      "  spec_prior       0.40      0.67      0.50         3\n",
      "   recommend       0.94      0.85      0.89      2043\n",
      "   not_recom       0.95      0.68      0.79       153\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.81      0.81      0.80      6480\n",
      "weighted avg       0.91      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.904783950617284\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2119\n",
      "    priority       0.86      0.93      0.90      2156\n",
      "  spec_prior       0.67      1.00      0.80         2\n",
      "   recommend       0.95      0.85      0.90      2034\n",
      "   not_recom       0.94      0.64      0.76       169\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.87      0.88      0.86      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9080246913580247\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2117\n",
      "    priority       0.86      0.92      0.89      2135\n",
      "  spec_prior       0.50      1.00      0.67         2\n",
      "   recommend       0.94      0.85      0.90      2067\n",
      "   not_recom       0.95      0.65      0.77       159\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.84      0.88      0.83      6480\n",
      "weighted avg       0.91      0.91      0.90      6480\n",
      "\n",
      "accuracy:  0.9055555555555556\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.94      2166\n",
      "    priority       0.86      0.92      0.89      2138\n",
      "  spec_prior       1.00      1.00      1.00         3\n",
      "   recommend       0.94      0.86      0.90      2010\n",
      "   not_recom       0.92      0.66      0.77       163\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.93      0.88      0.90      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9066358024691358\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2129\n",
      "    priority       0.87      0.93      0.90      2160\n",
      "  spec_prior       0.50      1.00      0.67         2\n",
      "   recommend       0.94      0.85      0.90      2023\n",
      "   not_recom       0.93      0.68      0.78       166\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.83      0.88      0.84      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9095679012345679\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2154\n",
      "    priority       0.87      0.93      0.90      2113\n",
      "  spec_prior       0.67      0.67      0.67         3\n",
      "   recommend       0.95      0.86      0.90      2054\n",
      "   not_recom       0.93      0.67      0.78       156\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.87      0.82      0.84      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9103395061728395\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2176\n",
      "    priority       0.86      0.92      0.89      2114\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.93      0.86      0.90      2029\n",
      "   not_recom       0.86      0.67      0.76       159\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.72      0.68      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9098765432098765\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.97      0.94      2107\n",
      "    priority       0.86      0.92      0.89      2159\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.95      0.83      0.89      2048\n",
      "   not_recom       0.91      0.64      0.75       163\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9004629629629629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.95      0.94      2167\n",
      "    priority       0.84      0.92      0.88      2130\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.83      0.87      2015\n",
      "   not_recom       0.90      0.67      0.77       165\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8959876543209877\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.97      0.94      2116\n",
      "    priority       0.87      0.92      0.89      2143\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.94      0.85      0.89      2062\n",
      "   not_recom       0.95      0.62      0.75       157\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.67      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9072530864197531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.95      2166\n",
      "    priority       0.86      0.93      0.89      2120\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.94      0.86      0.90      2028\n",
      "   not_recom       0.90      0.68      0.77       164\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.68      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9108024691358024\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.97      0.95      2117\n",
      "    priority       0.87      0.93      0.90      2153\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.94      0.86      0.90      2049\n",
      "   not_recom       0.93      0.72      0.81       158\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.70      0.71      6480\n",
      "weighted avg       0.92      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9141975308641975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.95      2103\n",
      "    priority       0.88      0.92      0.90      2148\n",
      "  spec_prior       0.00      0.00      0.00         0\n",
      "   recommend       0.93      0.87      0.90      2076\n",
      "   not_recom       0.94      0.72      0.81       153\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.69      0.71      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9128086419753086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2180\n",
      "    priority       0.85      0.94      0.89      2125\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.94      0.84      0.89      2001\n",
      "   not_recom       0.89      0.63      0.74       169\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.67      0.69      6480\n",
      "weighted avg       0.91      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.904783950617284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2159\n",
      "    priority       0.87      0.93      0.90      2138\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.93      0.87      0.90      2010\n",
      "   not_recom       0.90      0.70      0.79       170\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.69      0.71      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9140432098765432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.97      0.94      2124\n",
      "    priority       0.86      0.91      0.89      2135\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.93      0.84      0.88      2067\n",
      "   not_recom       0.92      0.66      0.77       152\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.68      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9015432098765432\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.95      0.95      2153\n",
      "    priority       0.87      0.93      0.90      2128\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.91      0.87      0.89      2033\n",
      "   not_recom       0.94      0.65      0.77       162\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.68      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9111111111111111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.96      0.95      2130\n",
      "    priority       0.86      0.93      0.90      2145\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.92      0.87      0.89      2044\n",
      "   not_recom       0.95      0.59      0.73       160\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.67      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9101851851851852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.95      2108\n",
      "    priority       0.87      0.93      0.89      2207\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.93      0.85      0.89      2011\n",
      "   not_recom       0.92      0.63      0.75       153\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.67      0.70      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9069444444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.95      0.96      2175\n",
      "    priority       0.86      0.94      0.90      2066\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.93      0.88      0.90      2066\n",
      "   not_recom       0.94      0.69      0.80       169\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.69      0.71      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9158950617283951\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.94      0.95      2159\n",
      "    priority       0.88      0.93      0.90      2102\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.91      0.89      0.90      2058\n",
      "   not_recom       0.90      0.74      0.81       159\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.73      0.70      0.71      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9160493827160494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.94      2124\n",
      "    priority       0.86      0.93      0.89      2171\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.94      0.86      0.90      2019\n",
      "   not_recom       0.97      0.60      0.74       163\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.67      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9078703703703703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.95      0.95      2141\n",
      "    priority       0.86      0.92      0.89      2159\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.91      0.87      0.89      2009\n",
      "   not_recom       0.94      0.58      0.72       170\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.66      0.69      6480\n",
      "weighted avg       0.91      0.91      0.90      6480\n",
      "\n",
      "accuracy:  0.9050925925925926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.96      0.95      2142\n",
      "    priority       0.85      0.93      0.89      2114\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.94      0.83      0.88      2068\n",
      "   not_recom       0.87      0.68      0.76       152\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.68      0.70      6480\n",
      "weighted avg       0.91      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9029320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.96      0.96      2094\n",
      "    priority       0.87      0.93      0.90      2174\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.93      0.86      0.90      2055\n",
      "   not_recom       0.87      0.72      0.79       153\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.70      0.71      6480\n",
      "weighted avg       0.92      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9149691358024692\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.95      0.95      2189\n",
      "    priority       0.86      0.94      0.90      2099\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.94      0.87      0.90      2022\n",
      "   not_recom       0.96      0.62      0.75       169\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.68      0.70      6480\n",
      "weighted avg       0.92      0.92      0.91      6480\n",
      "\n",
      "accuracy:  0.9157407407407407\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.96      0.95      2157\n",
      "    priority       0.87      0.93      0.90      2125\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.93      0.89      0.91      2028\n",
      "   not_recom       0.94      0.51      0.66       166\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.66      0.69      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.917283950617284\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.97      0.96      2126\n",
      "    priority       0.87      0.92      0.89      2148\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.93      0.86      0.89      2049\n",
      "   not_recom       0.96      0.67      0.79       156\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.69      0.71      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9128086419753086\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.97      0.95      2103\n",
      "    priority       0.87      0.93      0.90      2157\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.94      0.85      0.89      2047\n",
      "   not_recom       0.96      0.53      0.68       171\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.66      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9097222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.96      0.96      2180\n",
      "    priority       0.88      0.93      0.90      2116\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.89      0.91      2030\n",
      "   not_recom       0.93      0.54      0.68       151\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.66      0.69      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9194444444444444\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.97      0.96      2129\n",
      "    priority       0.86      0.94      0.90      2134\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.93      0.85      0.89      2073\n",
      "   not_recom       0.95      0.54      0.69       142\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.66      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9123456790123456\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.96      0.96      2154\n",
      "    priority       0.87      0.93      0.90      2139\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.88      0.90      2004\n",
      "   not_recom       0.97      0.54      0.70       180\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.66      0.69      6480\n",
      "weighted avg       0.92      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9140432098765432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.96      0.95      2085\n",
      "    priority       0.89      0.93      0.91      2161\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.93      0.89      0.91      2071\n",
      "   not_recom       0.95      0.66      0.78       159\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.69      0.71      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.921604938271605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.96      0.96      0.96      2198\n",
      "    priority       0.86      0.93      0.89      2112\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.92      0.87      0.89      2006\n",
      "   not_recom       0.98      0.55      0.70       163\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.66      0.69      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9095679012345679\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.98      0.96      2130\n",
      "    priority       0.87      0.95      0.91      2136\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.95      0.86      0.90      2057\n",
      "   not_recom       0.94      0.60      0.73       154\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.74      0.68      0.70      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9203703703703704\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.95      0.96      0.96      2153\n",
      "    priority       0.88      0.93      0.91      2137\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.92      0.89      0.90      2020\n",
      "   not_recom       0.98      0.54      0.69       168\n",
      "\n",
      "   micro avg       0.92      0.92      0.92      6480\n",
      "   macro avg       0.75      0.66      0.69      6480\n",
      "weighted avg       0.92      0.92      0.92      6480\n",
      "\n",
      "accuracy:  0.9180555555555555\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.96      0.93      2148\n",
      "    priority       0.87      0.91      0.89      2140\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.86      0.89      2028\n",
      "   not_recom       0.95      0.47      0.63       161\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.64      0.67      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9003086419753087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.94      2135\n",
      "    priority       0.87      0.91      0.89      2133\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.91      0.87      0.89      2049\n",
      "   not_recom       0.96      0.44      0.60       161\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.73      0.63      0.66      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8993827160493827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.94      2148\n",
      "    priority       0.86      0.92      0.89      2147\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.90      0.87      0.89      2025\n",
      "   not_recom       0.99      0.44      0.61       159\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.74      0.63      0.66      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8993827160493827\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.95      0.94      2135\n",
      "    priority       0.88      0.92      0.90      2126\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.92      0.89      0.90      2052\n",
      "   not_recom       0.95      0.47      0.63       163\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.73      0.65      0.67      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.907716049382716\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.96      0.94      2124\n",
      "    priority       0.88      0.92      0.90      2135\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.93      0.87      0.90      2065\n",
      "   not_recom       0.96      0.45      0.61       155\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.64      0.67      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9078703703703703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.94      2159\n",
      "    priority       0.86      0.91      0.88      2138\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.90      0.87      0.89      2012\n",
      "   not_recom       0.93      0.46      0.61       167\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.64      0.66      6480\n",
      "weighted avg       0.90      0.90      0.89      6480\n",
      "\n",
      "accuracy:  0.8962962962962963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.96      0.93      2094\n",
      "    priority       0.85      0.91      0.88      2132\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.92      0.85      0.89      2082\n",
      "   not_recom       0.96      0.40      0.56       170\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.73      0.62      0.65      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8933641975308642\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.94      0.94      0.94      2189\n",
      "    priority       0.88      0.91      0.90      2141\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.91      0.90      0.90      1995\n",
      "   not_recom       0.95      0.52      0.67       152\n",
      "\n",
      "   micro avg       0.91      0.91      0.91      6480\n",
      "   macro avg       0.74      0.66      0.68      6480\n",
      "weighted avg       0.91      0.91      0.91      6480\n",
      "\n",
      "accuracy:  0.9097222222222222\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2117\n",
      "    priority       0.87      0.90      0.88      2139\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.91      0.87      0.89      2061\n",
      "   not_recom       0.94      0.41      0.57       160\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.73      0.63      0.66      6480\n",
      "weighted avg       0.90      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8949074074074074\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.93      0.94      0.93      2166\n",
      "    priority       0.87      0.93      0.90      2134\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.92      0.88      0.90      2016\n",
      "   not_recom       0.97      0.52      0.67       162\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.74      0.65      0.68      6480\n",
      "weighted avg       0.91      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.9049382716049382\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.93      0.88      2067\n",
      "    priority       0.83      0.87      0.85      2190\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.90      0.78      0.84      2058\n",
      "   not_recom       0.96      0.40      0.57       163\n",
      "\n",
      "   micro avg       0.85      0.85      0.85      6480\n",
      "   macro avg       0.70      0.60      0.63      6480\n",
      "weighted avg       0.86      0.85      0.85      6480\n",
      "\n",
      "accuracy:  0.850925925925926\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.88      0.90      2216\n",
      "    priority       0.82      0.91      0.87      2083\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.88      0.86      0.87      2019\n",
      "   not_recom       0.95      0.36      0.53       159\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.71      0.60      0.63      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.8716049382716049\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.92      0.88      2144\n",
      "    priority       0.86      0.87      0.87      2159\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.89      0.83      0.86      2027\n",
      "   not_recom       0.96      0.50      0.66       148\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.71      0.62      0.65      6480\n",
      "weighted avg       0.87      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.8641975308641975\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.87      0.90      0.89      2139\n",
      "    priority       0.82      0.90      0.86      2114\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.88      0.82      0.85      2050\n",
      "   not_recom       0.93      0.25      0.39       174\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.70      0.57      0.60      6480\n",
      "weighted avg       0.86      0.86      0.85      6480\n",
      "\n",
      "accuracy:  0.8572530864197531\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.88      0.91      0.90      2132\n",
      "    priority       0.85      0.90      0.88      2139\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.89      0.84      0.87      2060\n",
      "   not_recom       0.90      0.43      0.58       148\n",
      "\n",
      "   micro avg       0.88      0.88      0.88      6480\n",
      "   macro avg       0.71      0.62      0.65      6480\n",
      "weighted avg       0.88      0.88      0.87      6480\n",
      "\n",
      "accuracy:  0.875462962962963\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.88      0.92      0.90      2151\n",
      "    priority       0.84      0.89      0.86      2134\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.87      0.84      0.86      2017\n",
      "   not_recom       0.91      0.23      0.37       174\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.70      0.58      0.60      6480\n",
      "weighted avg       0.87      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.8646604938271605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.89      0.89      2180\n",
      "    priority       0.84      0.91      0.87      2138\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.87      0.84      0.86      2000\n",
      "   not_recom       0.92      0.38      0.54       158\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.71      0.60      0.63      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.8688271604938271\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.84      0.94      0.89      2103\n",
      "    priority       0.84      0.87      0.86      2135\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.90      0.81      0.85      2077\n",
      "   not_recom       0.97      0.35      0.52       164\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.71      0.59      0.62      6480\n",
      "weighted avg       0.86      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.8601851851851852\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.87      0.91      0.89      2155\n",
      "    priority       0.83      0.89      0.86      2121\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.88      0.83      0.86      2036\n",
      "   not_recom       0.90      0.37      0.53       164\n",
      "\n",
      "   micro avg       0.86      0.86      0.86      6480\n",
      "   macro avg       0.70      0.60      0.63      6480\n",
      "weighted avg       0.86      0.86      0.86      6480\n",
      "\n",
      "accuracy:  0.862037037037037\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.86      0.92      0.89      2128\n",
      "    priority       0.86      0.89      0.88      2152\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.90      0.84      0.87      2041\n",
      "   not_recom       0.98      0.39      0.56       158\n",
      "\n",
      "   micro avg       0.87      0.87      0.87      6480\n",
      "   macro avg       0.72      0.61      0.64      6480\n",
      "weighted avg       0.87      0.87      0.87      6480\n",
      "\n",
      "accuracy:  0.8717592592592592\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.70      0.85      0.77      2082\n",
      "    priority       0.82      0.79      0.80      2167\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.85      0.76      0.80      2072\n",
      "   not_recom       0.85      0.11      0.19       156\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.64      0.50      0.51      6480\n",
      "weighted avg       0.79      0.78      0.77      6480\n",
      "\n",
      "accuracy:  0.7802469135802469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.80      0.80      2201\n",
      "    priority       0.81      0.86      0.84      2106\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.82      0.83      0.82      2005\n",
      "   not_recom       0.93      0.16      0.27       166\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.67      0.53      0.55      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8125\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.74      0.81      0.78      2133\n",
      "    priority       0.81      0.82      0.81      2140\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.82      0.79      0.80      2039\n",
      "   not_recom       0.82      0.11      0.19       164\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6480\n",
      "   macro avg       0.64      0.51      0.52      6480\n",
      "weighted avg       0.79      0.79      0.78      6480\n",
      "\n",
      "accuracy:  0.7887345679012345\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.77      0.83      0.80      2150\n",
      "    priority       0.81      0.84      0.82      2133\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.83      0.80      0.81      2038\n",
      "   not_recom       0.96      0.15      0.25       158\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.67      0.52      0.54      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.804320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.74      0.82      0.78      2149\n",
      "    priority       0.80      0.81      0.81      2130\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.84      0.79      0.81      2024\n",
      "   not_recom       0.89      0.14      0.25       174\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6480\n",
      "   macro avg       0.65      0.51      0.53      6480\n",
      "weighted avg       0.79      0.79      0.78      6480\n",
      "\n",
      "accuracy:  0.7904320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.76      0.82      0.79      2134\n",
      "    priority       0.83      0.84      0.83      2143\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.82      0.80      0.81      2053\n",
      "   not_recom       0.93      0.17      0.29       148\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.67      0.52      0.54      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8027777777777778\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.77      0.81      0.79      2178\n",
      "    priority       0.81      0.82      0.82      2150\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.80      0.82      0.81      1980\n",
      "   not_recom       0.88      0.13      0.23       169\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.65      0.52      0.53      6480\n",
      "weighted avg       0.80      0.80      0.79      6480\n",
      "\n",
      "accuracy:  0.7972222222222223\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.74      0.84      0.78      2105\n",
      "    priority       0.81      0.83      0.82      2123\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.85      0.77      0.81      2097\n",
      "   not_recom       0.94      0.20      0.33       153\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.67      0.53      0.55      6480\n",
      "weighted avg       0.80      0.80      0.79      6480\n",
      "\n",
      "accuracy:  0.7969135802469136\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.73      0.84      0.78      2115\n",
      "    priority       0.80      0.82      0.81      2110\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.85      0.75      0.79      2088\n",
      "   not_recom       0.89      0.15      0.25       164\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6480\n",
      "   macro avg       0.65      0.51      0.53      6480\n",
      "weighted avg       0.79      0.79      0.78      6480\n",
      "\n",
      "accuracy:  0.7856481481481481\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.80      0.82      0.81      2168\n",
      "    priority       0.83      0.84      0.84      2163\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.82      0.84      0.83      1989\n",
      "   not_recom       0.92      0.15      0.25       158\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6480\n",
      "   macro avg       0.67      0.53      0.54      6480\n",
      "weighted avg       0.82      0.82      0.81      6480\n",
      "\n",
      "accuracy:  0.8152777777777778\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.73      0.69      0.71      2196\n",
      "    priority       0.77      0.82      0.79      2118\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.75      0.79      0.77      2008\n",
      "   not_recom       0.93      0.09      0.16       156\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.64      0.48      0.49      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7501543209876543\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.63      0.80      0.70      2087\n",
      "    priority       0.80      0.74      0.77      2155\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.82      0.73      0.77      2069\n",
      "   not_recom       0.60      0.04      0.07       166\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.57      0.46      0.46      6480\n",
      "weighted avg       0.75      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.7375\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.66      0.75      0.70      2114\n",
      "    priority       0.79      0.78      0.78      2145\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.80      0.75      0.77      2070\n",
      "   not_recom       0.87      0.13      0.23       149\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.62      0.48      0.50      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7453703703703703\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.67      0.72      0.69      2169\n",
      "    priority       0.76      0.79      0.77      2128\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.78      0.75      0.76      2007\n",
      "   not_recom       0.25      0.01      0.01       173\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.49      0.45      0.45      6480\n",
      "weighted avg       0.72      0.73      0.72      6480\n",
      "\n",
      "accuracy:  0.7325617283950617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.66      0.73      0.69      2135\n",
      "    priority       0.79      0.79      0.79      2150\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.77      0.74      0.75      2035\n",
      "   not_recom       0.78      0.09      0.16       156\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.60      0.47      0.48      6480\n",
      "weighted avg       0.74      0.74      0.73      6480\n",
      "\n",
      "accuracy:  0.7364197530864197\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.68      0.75      0.72      2148\n",
      "    priority       0.77      0.77      0.77      2123\n",
      "  spec_prior       0.00      0.00      0.00         1\n",
      "   recommend       0.82      0.78      0.80      2042\n",
      "   not_recom       1.00      0.02      0.05       166\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.65      0.47      0.47      6480\n",
      "weighted avg       0.76      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.75\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.69      0.75      0.72      2138\n",
      "    priority       0.78      0.78      0.78      2147\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.79      0.78      0.79      2023\n",
      "   not_recom       0.57      0.02      0.05       169\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.57      0.47      0.47      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7503086419753087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.65      0.73      0.69      2145\n",
      "    priority       0.76      0.76      0.76      2126\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.79      0.76      0.78      2054\n",
      "   not_recom       0.80      0.05      0.10       153\n",
      "\n",
      "   micro avg       0.73      0.73      0.73      6480\n",
      "   macro avg       0.60      0.46      0.46      6480\n",
      "weighted avg       0.74      0.73      0.72      6480\n",
      "\n",
      "accuracy:  0.7304012345679012\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.68      0.76      0.72      2137\n",
      "    priority       0.79      0.78      0.78      2137\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.80      0.77      0.79      2045\n",
      "   not_recom       0.73      0.05      0.09       158\n",
      "\n",
      "   micro avg       0.75      0.75      0.75      6480\n",
      "   macro avg       0.60      0.47      0.48      6480\n",
      "weighted avg       0.75      0.75      0.74      6480\n",
      "\n",
      "accuracy:  0.7515432098765432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.68      0.73      0.70      2146\n",
      "    priority       0.77      0.79      0.78      2136\n",
      "  spec_prior       0.00      0.00      0.00         2\n",
      "   recommend       0.79      0.77      0.78      2032\n",
      "   not_recom       0.86      0.04      0.07       164\n",
      "\n",
      "   micro avg       0.74      0.74      0.74      6480\n",
      "   macro avg       0.62      0.46      0.47      6480\n",
      "weighted avg       0.75      0.74      0.74      6480\n",
      "\n",
      "accuracy:  0.7430555555555556\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.81375, 0.9074845679012344, 0.907175925925926, 0.9106790123456789, 0.9155246913580248, 0.9013888888888889, 0.8646913580246913, 0.7974074074074075, 0.7427314814814815]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_minkowski\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
