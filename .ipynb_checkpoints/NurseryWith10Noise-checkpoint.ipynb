{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Parents', ' Has_nurs', ' Form', ' Children', ' Housing',\n",
       "       ' Finance', ' Social', ' Health', 'Class'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nurseryNum = pd.read_csv(\"nursery_numerical.csv\")\n",
    "nurseryNum.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 100\n",
    "j = 1\n",
    "features = ['Parents', ' Has_nurs', ' Form', ' Children', ' Housing', ' Finance', ' Social', ' Health']\n",
    "for index, m in nurseryNum.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        nurseryNum.at[index+2, features[j]] = i + 1\n",
    "        j += 3\n",
    "        i+=10\n",
    "        if j >= 7:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "for index, m in nurseryNum.iterrows():\n",
    "    if index % 20 == 0:\n",
    "        if index < 12954:\n",
    "            nurseryNum.at[index+5,:] = nurseryNum.loc[j,:]\n",
    "        else:\n",
    "            nurseryNum.at[index+1,:] = nurseryNum.loc[j,:]\n",
    "        j += 120\n",
    "        if j >= 12959:\n",
    "            j = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "nurseryNum.to_csv('/home/valia/Documents/AppliedDataScience/nureryNoise.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = nurseryNum.iloc[:,0:8]\n",
    "labels = nurseryNum.iloc[:,8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True) #5 fores me 2 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean and k tuning on 10% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.80      0.86      0.83      2104\n",
      "    priority       0.78      0.80      0.79      2133\n",
      "  spec_prior       0.57      1.00      0.73         4\n",
      "   recommend       0.86      0.76      0.81      2067\n",
      "   not_recom       0.57      0.74      0.65       172\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.72      0.83      0.76      6480\n",
      "weighted avg       0.81      0.80      0.81      6480\n",
      "\n",
      "accuracy:  0.8046296296296296\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.83      0.82      2169\n",
      "    priority       0.77      0.81      0.79      2134\n",
      "  spec_prior       0.50      1.00      0.67         3\n",
      "   recommend       0.85      0.77      0.81      2019\n",
      "   not_recom       0.62      0.72      0.66       155\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.71      0.83      0.75      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8029320987654321\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.84      0.83      2154\n",
      "    priority       0.77      0.81      0.79      2135\n",
      "  spec_prior       0.40      1.00      0.57         4\n",
      "   recommend       0.83      0.75      0.79      2028\n",
      "   not_recom       0.54      0.79      0.64       159\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.67      0.84      0.72      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7996913580246914\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.85      0.84      2119\n",
      "    priority       0.78      0.82      0.80      2132\n",
      "  spec_prior       1.00      1.00      1.00         3\n",
      "   recommend       0.86      0.77      0.81      2058\n",
      "   not_recom       0.57      0.72      0.64       168\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.81      0.83      0.82      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8103395061728395\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.86      0.83      2125\n",
      "    priority       0.78      0.82      0.80      2150\n",
      "  spec_prior       0.20      1.00      0.33         1\n",
      "   recommend       0.87      0.76      0.81      2037\n",
      "   not_recom       0.65      0.71      0.68       167\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.66      0.83      0.69      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.808179012345679\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.85      0.83      2148\n",
      "    priority       0.78      0.81      0.79      2117\n",
      "  spec_prior       0.67      1.00      0.80         6\n",
      "   recommend       0.86      0.75      0.80      2049\n",
      "   not_recom       0.58      0.81      0.68       160\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.74      0.85      0.78      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8057098765432099\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.86      0.85      2167\n",
      "    priority       0.78      0.82      0.80      2150\n",
      "  spec_prior       0.25      1.00      0.40         1\n",
      "   recommend       0.86      0.77      0.81      1995\n",
      "   not_recom       0.64      0.78      0.70       167\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6480\n",
      "   macro avg       0.67      0.85      0.71      6480\n",
      "weighted avg       0.82      0.82      0.82      6480\n",
      "\n",
      "accuracy:  0.8165123456790123\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.84      0.83      2106\n",
      "    priority       0.76      0.80      0.78      2117\n",
      "  spec_prior       0.60      1.00      0.75         6\n",
      "   recommend       0.85      0.75      0.80      2091\n",
      "   not_recom       0.52      0.74      0.61       160\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.71      0.83      0.75      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7950617283950617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.78      0.84      0.81      2119\n",
      "    priority       0.76      0.79      0.77      2136\n",
      "  spec_prior       0.38      1.00      0.56         5\n",
      "   recommend       0.83      0.73      0.78      2055\n",
      "   not_recom       0.54      0.63      0.58       165\n",
      "\n",
      "   micro avg       0.78      0.78      0.78      6480\n",
      "   macro avg       0.66      0.80      0.70      6480\n",
      "weighted avg       0.79      0.78      0.78      6480\n",
      "\n",
      "accuracy:  0.7816358024691358\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.85      0.83      2154\n",
      "    priority       0.79      0.83      0.81      2131\n",
      "  spec_prior       1.00      1.00      1.00         2\n",
      "   recommend       0.87      0.77      0.81      2031\n",
      "   not_recom       0.66      0.78      0.71       162\n",
      "\n",
      "   micro avg       0.82      0.82      0.82      6480\n",
      "   macro avg       0.83      0.85      0.83      6480\n",
      "weighted avg       0.82      0.82      0.82      6480\n",
      "\n",
      "accuracy:  0.817283950617284\n",
      "mean accuracy 0.8041975308641975\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/valia/.local/lib/python2.7/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.95      0.92      2075\n",
      "    priority       0.86      0.91      0.88      2143\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.92      0.84      0.88      2083\n",
      "   not_recom       0.95      0.67      0.79       175\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.73      0.67      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8927469135802469\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.93      0.92      2198\n",
      "    priority       0.84      0.91      0.88      2124\n",
      "  spec_prior       0.00      0.00      0.00         3\n",
      "   recommend       0.92      0.83      0.87      2003\n",
      "   not_recom       0.85      0.66      0.74       152\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.70      0.67      0.68      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8873456790123457\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.93      0.93      2149\n",
      "    priority       0.85      0.90      0.88      2165\n",
      "  spec_prior       0.12      1.00      0.22         2\n",
      "   recommend       0.90      0.85      0.88      2000\n",
      "   not_recom       0.93      0.62      0.74       164\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.75      0.86      0.73      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8890432098765432\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.95      0.93      2124\n",
      "    priority       0.84      0.91      0.87      2102\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.93      0.83      0.87      2086\n",
      "   not_recom       0.89      0.66      0.76       163\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.71      0.67      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8896604938271605\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.95      0.93      2101\n",
      "    priority       0.87      0.91      0.89      2128\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.92      0.85      0.88      2080\n",
      "   not_recom       0.86      0.71      0.78       166\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.71      0.68      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8983024691358025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2172\n",
      "    priority       0.86      0.91      0.88      2139\n",
      "  spec_prior       0.29      1.00      0.44         2\n",
      "   recommend       0.90      0.84      0.87      2006\n",
      "   not_recom       0.94      0.64      0.76       161\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.78      0.87      0.78      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8901234567901235\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.90      0.94      0.92      2079\n",
      "    priority       0.84      0.92      0.88      2132\n",
      "  spec_prior       0.33      1.00      0.50         3\n",
      "   recommend       0.93      0.83      0.87      2101\n",
      "   not_recom       0.93      0.58      0.71       165\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.79      0.85      0.78      6480\n",
      "weighted avg       0.89      0.89      0.88      6480\n",
      "\n",
      "accuracy:  0.8858024691358025\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2194\n",
      "    priority       0.86      0.91      0.88      2135\n",
      "  spec_prior       0.00      0.00      0.00         4\n",
      "   recommend       0.91      0.86      0.88      1985\n",
      "   not_recom       0.92      0.72      0.81       162\n",
      "\n",
      "   micro avg       0.90      0.90      0.90      6480\n",
      "   macro avg       0.72      0.69      0.70      6480\n",
      "weighted avg       0.90      0.90      0.90      6480\n",
      "\n",
      "accuracy:  0.8986111111111111\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.92      0.94      0.93      2156\n",
      "    priority       0.85      0.92      0.88      2129\n",
      "  spec_prior       0.13      1.00      0.24         2\n",
      "   recommend       0.92      0.84      0.87      2023\n",
      "   not_recom       0.95      0.56      0.71       170\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.75      0.85      0.73      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8910493827160494\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.91      0.94      0.92      2117\n",
      "    priority       0.86      0.90      0.88      2138\n",
      "  spec_prior       0.00      0.00      0.00         5\n",
      "   recommend       0.91      0.84      0.88      2063\n",
      "   not_recom       0.83      0.69      0.76       157\n",
      "\n",
      "   micro avg       0.89      0.89      0.89      6480\n",
      "   macro avg       0.70      0.68      0.69      6480\n",
      "weighted avg       0.89      0.89      0.89      6480\n",
      "\n",
      "accuracy:  0.8904320987654321\n",
      "mean accuracy 0.8913117283950618\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mean_accuracy_model_euclidean\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski and k tuning on 10% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.84      0.83      2159\n",
      "    priority       0.77      0.82      0.79      2135\n",
      "  spec_prior       0.60      1.00      0.75         3\n",
      "   recommend       0.88      0.76      0.81      2025\n",
      "   not_recom       0.56      0.76      0.64       158\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.72      0.84      0.77      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8061728395061728\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.84      0.83      2114\n",
      "    priority       0.77      0.80      0.79      2132\n",
      "  spec_prior       0.40      1.00      0.57         4\n",
      "   recommend       0.85      0.76      0.80      2061\n",
      "   not_recom       0.56      0.68      0.61       169\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.68      0.82      0.72      6480\n",
      "weighted avg       0.80      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7996913580246914\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.84      0.83      2132\n",
      "    priority       0.77      0.80      0.78      2156\n",
      "  spec_prior       0.60      1.00      0.75         3\n",
      "   recommend       0.85      0.77      0.81      2034\n",
      "   not_recom       0.53      0.70      0.60       155\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.71      0.82      0.75      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.8016975308641975\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.84      0.83      2141\n",
      "    priority       0.77      0.83      0.80      2111\n",
      "  spec_prior       0.44      1.00      0.62         4\n",
      "   recommend       0.87      0.75      0.81      2052\n",
      "   not_recom       0.56      0.78      0.65       172\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.69      0.84      0.74      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.8083333333333333\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.81      0.85      0.83      2143\n",
      "    priority       0.76      0.83      0.79      2089\n",
      "  spec_prior       0.20      1.00      0.33         1\n",
      "   recommend       0.87      0.75      0.80      2076\n",
      "   not_recom       0.62      0.65      0.64       171\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.65      0.81      0.68      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.803395061728395\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.83      0.83      2130\n",
      "    priority       0.77      0.81      0.79      2178\n",
      "  spec_prior       0.75      1.00      0.86         6\n",
      "   recommend       0.85      0.77      0.81      2010\n",
      "   not_recom       0.49      0.72      0.58       156\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.74      0.82      0.77      6480\n",
      "weighted avg       0.81      0.80      0.80      6480\n",
      "\n",
      "accuracy:  0.7998456790123457\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.79      0.83      0.81      2110\n",
      "    priority       0.78      0.77      0.77      2208\n",
      "  spec_prior       0.25      1.00      0.40         2\n",
      "   recommend       0.81      0.77      0.79      1991\n",
      "   not_recom       0.58      0.69      0.63       169\n",
      "\n",
      "   micro avg       0.79      0.79      0.79      6480\n",
      "   macro avg       0.64      0.81      0.68      6480\n",
      "weighted avg       0.79      0.79      0.79      6480\n",
      "\n",
      "accuracy:  0.7882716049382716\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.85      0.84      2163\n",
      "    priority       0.76      0.83      0.79      2059\n",
      "  spec_prior       0.83      1.00      0.91         5\n",
      "   recommend       0.87      0.75      0.81      2095\n",
      "   not_recom       0.58      0.77      0.66       158\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.77      0.84      0.80      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.807716049382716\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.82      0.86      0.84      2133\n",
      "    priority       0.77      0.81      0.79      2145\n",
      "  spec_prior       0.80      1.00      0.89         4\n",
      "   recommend       0.87      0.76      0.81      2037\n",
      "   not_recom       0.57      0.75      0.65       161\n",
      "\n",
      "   micro avg       0.81      0.81      0.81      6480\n",
      "   macro avg       0.77      0.84      0.80      6480\n",
      "weighted avg       0.81      0.81      0.81      6480\n",
      "\n",
      "accuracy:  0.808179012345679\n",
      "set([' very_recom', ' priority', ' spec_prior', ' recommend', ' not_recom'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  very_recom       0.83      0.83      0.83      2140\n",
      "    priority       0.77      0.81      0.79      2122\n",
      "  spec_prior       0.43      1.00      0.60         3\n",
      "   recommend       0.85      0.77      0.81      2049\n",
      "   not_recom       0.59      0.75      0.66       166\n",
      "\n",
      "   micro avg       0.80      0.80      0.80      6480\n",
      "   macro avg       0.69      0.83      0.74      6480\n",
      "weighted avg       0.81      0.80      0.81      6480\n",
      "\n",
      "accuracy:  0.8046296296296296\n",
      "mean accuracy 0.8027932098765431\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_minkowski = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(nurseryNum):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print mean_accuracy_model_minkowski\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
