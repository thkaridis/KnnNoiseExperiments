{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Sex', ' MaritalStatus', ' Age', ' Education', ' Occupation',\n",
       "       ' YearsInSf', ' DualIncome', ' HouseholdMembers', ' Under18',\n",
       "       ' HouseholdStatus', ' TypeOfHome', ' EthnicClass', ' Language',\n",
       "       'Class'], dtype=object)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing = pd.read_csv(\"marketing_numerical.csv\")\n",
    "(marketing.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sex</th>\n",
       "      <th>MaritalStatus</th>\n",
       "      <th>Age</th>\n",
       "      <th>Education</th>\n",
       "      <th>Occupation</th>\n",
       "      <th>YearsInSf</th>\n",
       "      <th>DualIncome</th>\n",
       "      <th>HouseholdMembers</th>\n",
       "      <th>Under18</th>\n",
       "      <th>HouseholdStatus</th>\n",
       "      <th>TypeOfHome</th>\n",
       "      <th>EthnicClass</th>\n",
       "      <th>Language</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>6.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6846</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6847</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6848</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6849</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6850</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6851</th>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6852</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6853</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6854</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6855</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6856</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6857</th>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6858</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6859</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6860</th>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6861</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6862</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6863</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6864</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6865</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6866</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6867</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6868</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6869</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6870</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>3</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6871</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6872</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6873</th>\n",
       "      <td>2</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6874</th>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6875</th>\n",
       "      <td>1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6876 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Sex   MaritalStatus   Age   Education   Occupation   YearsInSf  \\\n",
       "0       1             1.0     5         5.0          5.0         5.0   \n",
       "1       2             1.0     3         5.0          1.0         5.0   \n",
       "2       2             5.0     1         2.0          6.0         5.0   \n",
       "3       2             5.0     1         2.0          6.0         3.0   \n",
       "4       1             1.0     6         4.0          8.0         5.0   \n",
       "5       1             5.0     2         3.0          9.0         4.0   \n",
       "6       1             3.0     3         4.0          3.0         5.0   \n",
       "7       1             1.0     6         3.0          8.0         5.0   \n",
       "8       1             1.0     7         4.0          8.0         4.0   \n",
       "9       1             5.0     2         4.0          9.0         5.0   \n",
       "10      2             2.0     2         3.0          2.0         5.0   \n",
       "11      2             1.0     3         6.0          6.0         2.0   \n",
       "12      2             1.0     5         3.0          5.0         5.0   \n",
       "13      2             1.0     5         4.0          1.0         5.0   \n",
       "14      2             3.0     3         3.0          2.0         2.0   \n",
       "15      2             1.0     5         3.0          5.0         1.0   \n",
       "16      1             3.0     4         2.0          3.0         4.0   \n",
       "17      2             1.0     4         4.0          2.0         5.0   \n",
       "18      1             1.0     4         4.0          1.0         5.0   \n",
       "19      2             3.0     5         4.0          2.0         3.0   \n",
       "20      2             5.0     1         2.0          6.0         5.0   \n",
       "21      1             1.0     3         5.0          1.0         5.0   \n",
       "22      1             1.0     4         4.0          1.0         5.0   \n",
       "23      1             1.0     4         4.0          1.0         5.0   \n",
       "24      2             3.0     7         3.0          8.0         5.0   \n",
       "25      2             3.0     4         5.0          1.0         4.0   \n",
       "26      2             1.0     4         4.0          1.0         4.0   \n",
       "27      2             5.0     1         2.0          6.0         5.0   \n",
       "28      2             5.0     1         2.0          6.0         5.0   \n",
       "29      1             5.0     1         2.0          6.0         5.0   \n",
       "...   ...             ...   ...         ...          ...         ...   \n",
       "6846    1             5.0     2         4.0          2.0         5.0   \n",
       "6847    1             1.0     3         4.0          1.0         5.0   \n",
       "6848    1             1.0     3         4.0          2.0         5.0   \n",
       "6849    1             5.0     3         5.0          2.0         5.0   \n",
       "6850    2             1.0     6         5.0          1.0         5.0   \n",
       "6851    1             3.0     3         5.0          1.0         5.0   \n",
       "6852    1             5.0     1         2.0          6.0         5.0   \n",
       "6853    2             5.0     1         2.0          6.0         5.0   \n",
       "6854    1             5.0     3         4.0          4.0         5.0   \n",
       "6855    2             5.0     2         4.0          4.0         5.0   \n",
       "6856    1             1.0     7         4.0          8.0         5.0   \n",
       "6857    1             4.0     3         4.0          1.0         5.0   \n",
       "6858    2             5.0     2         3.0          6.0         2.0   \n",
       "6859    1             5.0     2         4.0          4.0         5.0   \n",
       "6860    2             3.0     4         4.0          1.0         5.0   \n",
       "6861    1             5.0     3         5.0          2.0         5.0   \n",
       "6862    1             5.0     2         3.0          3.0         5.0   \n",
       "6863    2             1.0     4         6.0          5.0         5.0   \n",
       "6864    2             5.0     1         2.0          6.0         5.0   \n",
       "6865    1             5.0     2         3.0          4.0         5.0   \n",
       "6866    1             5.0     3         4.0          6.0         5.0   \n",
       "6867    1             5.0     2         4.0          4.0         2.0   \n",
       "6868    2             5.0     2         4.0          4.0         2.0   \n",
       "6869    1             5.0     3         4.0          6.0         2.0   \n",
       "6870    1             1.0     3         6.0          1.0         5.0   \n",
       "6871    2             5.0     1         1.0          2.0         5.0   \n",
       "6872    1             5.0     2         4.0          1.0         5.0   \n",
       "6873    2             5.0     1         2.0          1.0         5.0   \n",
       "6874    1             1.0     6         4.0          3.0         5.0   \n",
       "6875    1             5.0     3         4.0          1.0         5.0   \n",
       "\n",
       "       DualIncome   HouseholdMembers   Under18   HouseholdStatus   TypeOfHome  \\\n",
       "0               3                5.0         2               1.0          1.0   \n",
       "1               2                3.0         1               2.0          3.0   \n",
       "2               1                4.0         2               3.0          1.0   \n",
       "3               1                4.0         2               3.0          1.0   \n",
       "4               3                2.0         0               1.0          1.0   \n",
       "5               1                3.0         1               2.0          3.0   \n",
       "6               1                1.0         0               2.0          3.0   \n",
       "7               3                3.0         0               2.0          3.0   \n",
       "8               3                2.0         0               2.0          3.0   \n",
       "9               1                1.0         0               2.0          3.0   \n",
       "10              1                2.0         0               1.0          1.0   \n",
       "11              2                4.0         2               1.0          1.0   \n",
       "12              3                4.0         0               2.0          1.0   \n",
       "13              2                2.0         2               1.0          1.0   \n",
       "14              1                2.0         1               2.0          3.0   \n",
       "15              3                2.0         0               2.0          3.0   \n",
       "16              1                2.0         0               2.0          3.0   \n",
       "17              3                5.0         3               1.0          1.0   \n",
       "18              3                5.0         3               1.0          1.0   \n",
       "19              1                3.0         0               2.0          5.0   \n",
       "20              1                4.0         1               3.0          1.0   \n",
       "21              2                3.0         1               2.0          3.0   \n",
       "22              2                3.0         0               1.0          1.0   \n",
       "23              2                3.0         1               1.0          1.0   \n",
       "24              1                1.0         0               1.0          5.0   \n",
       "25              1                5.0         3               1.0          1.0   \n",
       "26              2                4.0         2               2.0          1.0   \n",
       "27              1                4.0         2               3.0          1.0   \n",
       "28              1                3.0         1               3.0          1.0   \n",
       "29              1                6.0         4               3.0          1.0   \n",
       "...           ...                ...       ...               ...          ...   \n",
       "6846            1                3.0         0               3.0          1.0   \n",
       "6847            3                4.0         2               1.0          1.0   \n",
       "6848            2                5.0         3               1.0          1.0   \n",
       "6849            1                1.0         0               1.0          2.0   \n",
       "6850            2                2.0         0               1.0          1.0   \n",
       "6851            1                2.0         0               1.0          1.0   \n",
       "6852            1                3.0         1               3.0          1.0   \n",
       "6853            1                4.0         2               3.0          1.0   \n",
       "6854            1                2.0         0               2.0          3.0   \n",
       "6855            1                3.0         0               2.0          1.0   \n",
       "6856            3                2.0         0               1.0          1.0   \n",
       "6857            1                3.0         0               2.0          1.0   \n",
       "6858            1                4.0         2               3.0          3.0   \n",
       "6859            1                1.0         0               2.0          2.0   \n",
       "6860            1                1.0         0               1.0          2.0   \n",
       "6861            1                1.0         0               2.0          3.0   \n",
       "6862            1                1.0         0               2.0          1.0   \n",
       "6863            3                5.0         3               1.0          1.0   \n",
       "6864            1                4.0         1               3.0          1.0   \n",
       "6865            1                4.0         0               3.0          1.0   \n",
       "6866            1                2.0         0               2.0          5.0   \n",
       "6867            1                3.0         0               2.0          5.0   \n",
       "6868            1                4.0         0               2.0          1.0   \n",
       "6869            1                3.0         0               2.0          3.0   \n",
       "6870            2                2.0         0               1.0          1.0   \n",
       "6871            1                3.0         2               3.0          1.0   \n",
       "6872            1                4.0         0               3.0          1.0   \n",
       "6873            1                3.0         2               3.0          1.0   \n",
       "6874            2                3.0         1               2.0          3.0   \n",
       "6875            1                1.0         0               2.0          3.0   \n",
       "\n",
       "       EthnicClass   Language  Class  \n",
       "0              7.0        1.0      9  \n",
       "1              7.0        1.0      9  \n",
       "2              7.0        1.0      1  \n",
       "3              7.0        1.0      1  \n",
       "4              7.0        1.0      8  \n",
       "5              7.0        1.0      1  \n",
       "6              7.0        1.0      6  \n",
       "7              7.0        1.0      2  \n",
       "8              7.0        1.0      4  \n",
       "9              7.0        1.0      1  \n",
       "10             5.0        1.0      4  \n",
       "11             7.0        1.0      8  \n",
       "12             7.0        1.0      7  \n",
       "13             7.0        1.0      7  \n",
       "14             7.0        1.0      1  \n",
       "15             7.0        1.0      8  \n",
       "16             7.0        1.0      2  \n",
       "17             5.0        2.0      9  \n",
       "18             7.0        1.0      8  \n",
       "19             7.0        1.0      4  \n",
       "20             7.0        1.0      1  \n",
       "21             7.0        1.0      9  \n",
       "22             7.0        1.0      7  \n",
       "23             7.0        1.0      9  \n",
       "24             7.0        1.0      4  \n",
       "25             7.0        1.0      6  \n",
       "26             7.0        1.0      8  \n",
       "27             7.0        1.0      1  \n",
       "28             7.0        1.0      1  \n",
       "29             7.0        1.0      1  \n",
       "...            ...        ...    ...  \n",
       "6846           7.0        1.0      3  \n",
       "6847           2.0        1.0      9  \n",
       "6848           5.0        1.0      9  \n",
       "6849           7.0        1.0      6  \n",
       "6850           7.0        1.0      8  \n",
       "6851           7.0        1.0      7  \n",
       "6852           7.0        1.0      1  \n",
       "6853           5.0        1.0      1  \n",
       "6854           7.0        1.0      2  \n",
       "6855           7.0        1.0      4  \n",
       "6856           7.0        1.0      9  \n",
       "6857           7.0        1.0      4  \n",
       "6858           7.0        1.0      1  \n",
       "6859           7.0        1.0      6  \n",
       "6860           7.0        1.0      7  \n",
       "6861           7.0        1.0      5  \n",
       "6862           2.0        1.0      3  \n",
       "6863           5.0        1.0      7  \n",
       "6864           7.0        1.0      1  \n",
       "6865           7.0        1.0      6  \n",
       "6866           7.0        1.0      5  \n",
       "6867           8.0        3.0      1  \n",
       "6868           7.0        1.0      3  \n",
       "6869           8.0        3.0      2  \n",
       "6870           7.0        1.0      9  \n",
       "6871           7.0        1.0      1  \n",
       "6872           7.0        1.0      2  \n",
       "6873           7.0        1.0      1  \n",
       "6874           7.0        1.0      4  \n",
       "6875           5.0        1.0      6  \n",
       "\n",
       "[6876 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "marketing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "i = 100\n",
    "j = 0\n",
    "times = 0\n",
    "features = ['Sex', ' MaritalStatus', ' Age', ' Education', ' Occupation',\n",
    "       ' YearsInSf', ' DualIncome', ' HouseholdMembers', ' Under18',\n",
    "       ' HouseholdStatus', ' TypeOfHome', ' EthnicClass', ' Language']\n",
    "for index, m in marketing.iterrows():\n",
    "    if index % 40 == 0:\n",
    "        times += 1\n",
    "        marketing.at[index+2,features[j]] = i + 1\n",
    "        j += 3\n",
    "        i += 10\n",
    "        if j >= 12:\n",
    "            j = 0\n",
    "print times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "172\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "j = 0\n",
    "times = 0\n",
    "for index, m in marketing.iterrows():\n",
    "    if index % 40 == 0:\n",
    "        times += 1\n",
    "        if index < 6870:\n",
    "            marketing.at[index+5,:] = marketing.loc[j,:]\n",
    "        else:\n",
    "            marketing.at[index+1,:] = marketing.loc[j,:]\n",
    "        j += 120\n",
    "        if j >= 6500:\n",
    "            j = 0\n",
    "print times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "marketing.to_csv('/home/valia/Documents/AppliedDataScience/marketingNoise5perCent.csv', sep=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['9.0', '1.0', '8.0', '6.0', '2.0', '4.0', '7.0', '5.0', '3.0'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = marketing.iloc[:,0:13]\n",
    "labels = marketing.iloc[:,13].apply(str)\n",
    "labels.unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=2, shuffle=True) #5 times with 2 folds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Euclidean and k tuning on 5% noise datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.61      0.58      0.60       620\n",
      "         8.0       0.14      0.13      0.14       241\n",
      "         1.0       0.17      0.18      0.18       241\n",
      "         4.0       0.14      0.13      0.14       307\n",
      "         7.0       0.17      0.18      0.18       266\n",
      "         3.0       0.21      0.19      0.20       452\n",
      "         6.0       0.20      0.24      0.22       389\n",
      "         2.0       0.33      0.34      0.33       552\n",
      "         9.0       0.33      0.32      0.33       370\n",
      "\n",
      "   micro avg       0.29      0.29      0.29      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.29      0.29      3438\n",
      "\n",
      "accuracy:  0.2937754508435137\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.60      0.61      0.60       624\n",
      "         8.0       0.18      0.14      0.16       281\n",
      "         1.0       0.17      0.18      0.18       255\n",
      "         4.0       0.12      0.14      0.13       294\n",
      "         7.0       0.16      0.17      0.17       261\n",
      "         3.0       0.21      0.20      0.20       408\n",
      "         6.0       0.20      0.20      0.20       404\n",
      "         2.0       0.30      0.31      0.30       537\n",
      "         9.0       0.35      0.33      0.34       374\n",
      "\n",
      "   micro avg       0.29      0.29      0.29      3438\n",
      "   macro avg       0.25      0.25      0.25      3438\n",
      "weighted avg       0.29      0.29      0.29      3438\n",
      "\n",
      "accuracy:  0.29086678301337987\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.61      0.60       591\n",
      "         8.0       0.19      0.16      0.17       272\n",
      "         1.0       0.18      0.17      0.18       252\n",
      "         4.0       0.15      0.17      0.16       297\n",
      "         7.0       0.18      0.18      0.18       259\n",
      "         3.0       0.23      0.22      0.23       432\n",
      "         6.0       0.18      0.18      0.18       405\n",
      "         2.0       0.31      0.31      0.31       575\n",
      "         9.0       0.33      0.36      0.35       355\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.30      0.29      3438\n",
      "\n",
      "accuracy:  0.2952297847585806\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.65      0.60      0.63       653\n",
      "         8.0       0.19      0.19      0.19       250\n",
      "         1.0       0.16      0.15      0.16       244\n",
      "         4.0       0.13      0.13      0.13       304\n",
      "         7.0       0.17      0.16      0.16       268\n",
      "         3.0       0.22      0.20      0.21       428\n",
      "         6.0       0.21      0.21      0.21       388\n",
      "         2.0       0.28      0.35      0.31       514\n",
      "         9.0       0.33      0.31      0.32       389\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.30017452006980805\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.62      0.63      0.62       597\n",
      "         8.0       0.20      0.20      0.20       275\n",
      "         1.0       0.19      0.16      0.18       243\n",
      "         4.0       0.18      0.18      0.18       303\n",
      "         7.0       0.17      0.18      0.18       257\n",
      "         3.0       0.25      0.24      0.25       425\n",
      "         6.0       0.26      0.27      0.26       389\n",
      "         2.0       0.31      0.33      0.32       573\n",
      "         9.0       0.31      0.30      0.30       376\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.28      0.28      0.28      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.31413612565445026\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.61      0.60      0.60       647\n",
      "         8.0       0.21      0.19      0.20       247\n",
      "         1.0       0.20      0.18      0.19       253\n",
      "         4.0       0.15      0.13      0.14       298\n",
      "         7.0       0.15      0.15      0.15       270\n",
      "         3.0       0.22      0.24      0.23       435\n",
      "         6.0       0.23      0.21      0.22       404\n",
      "         2.0       0.28      0.33      0.30       516\n",
      "         9.0       0.33      0.31      0.32       368\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.3010471204188482\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.66      0.59      0.62       610\n",
      "         8.0       0.20      0.19      0.19       251\n",
      "         1.0       0.19      0.19      0.19       259\n",
      "         4.0       0.14      0.15      0.15       282\n",
      "         7.0       0.19      0.19      0.19       265\n",
      "         3.0       0.25      0.26      0.25       440\n",
      "         6.0       0.19      0.18      0.19       397\n",
      "         2.0       0.29      0.31      0.30       563\n",
      "         9.0       0.34      0.36      0.35       371\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.31      0.30      0.31      3438\n",
      "\n",
      "accuracy:  0.30395578824898195\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.62      0.60       634\n",
      "         8.0       0.22      0.17      0.19       271\n",
      "         1.0       0.17      0.18      0.18       237\n",
      "         4.0       0.15      0.14      0.15       319\n",
      "         7.0       0.18      0.19      0.18       262\n",
      "         3.0       0.19      0.20      0.19       420\n",
      "         6.0       0.21      0.20      0.20       396\n",
      "         2.0       0.30      0.33      0.32       526\n",
      "         9.0       0.32      0.29      0.31       373\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.2975567190226876\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.60      0.63      0.61       588\n",
      "         8.0       0.17      0.16      0.17       262\n",
      "         1.0       0.13      0.12      0.13       244\n",
      "         4.0       0.18      0.17      0.17       300\n",
      "         7.0       0.19      0.17      0.18       293\n",
      "         3.0       0.22      0.26      0.24       414\n",
      "         6.0       0.22      0.20      0.21       405\n",
      "         2.0       0.29      0.32      0.30       550\n",
      "         9.0       0.35      0.31      0.33       382\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.29813845258871435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.57      0.58       656\n",
      "         8.0       0.19      0.17      0.18       260\n",
      "         1.0       0.14      0.13      0.14       252\n",
      "         4.0       0.17      0.18      0.17       301\n",
      "         7.0       0.14      0.17      0.15       234\n",
      "         3.0       0.22      0.19      0.20       446\n",
      "         6.0       0.21      0.22      0.21       388\n",
      "         2.0       0.31      0.35      0.33       539\n",
      "         9.0       0.32      0.33      0.33       362\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.25      0.25      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.2961023851076207\n",
      "mean accuracy 0.29909831297265854\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_euclidean = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "mean_accuracy_model1 =  sum(acc)/10    \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.71      0.59       642\n",
      "         8.0       0.13      0.14      0.14       242\n",
      "         1.0       0.16      0.16      0.16       244\n",
      "         4.0       0.11      0.11      0.11       276\n",
      "         7.0       0.13      0.07      0.09       267\n",
      "         3.0       0.24      0.23      0.23       448\n",
      "         6.0       0.24      0.17      0.20       420\n",
      "         2.0       0.31      0.38      0.34       524\n",
      "         9.0       0.41      0.28      0.33       375\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.30744618964514253\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.76      0.59       602\n",
      "         8.0       0.18      0.16      0.17       280\n",
      "         1.0       0.12      0.12      0.12       252\n",
      "         4.0       0.20      0.14      0.17       325\n",
      "         7.0       0.15      0.12      0.14       260\n",
      "         3.0       0.25      0.24      0.24       412\n",
      "         6.0       0.19      0.18      0.18       373\n",
      "         2.0       0.35      0.35      0.35       565\n",
      "         9.0       0.42      0.30      0.35       369\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.3138452588714369\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.74      0.59       613\n",
      "         8.0       0.14      0.15      0.15       269\n",
      "         1.0       0.13      0.11      0.12       253\n",
      "         4.0       0.15      0.14      0.15       299\n",
      "         7.0       0.16      0.10      0.12       289\n",
      "         3.0       0.24      0.21      0.22       424\n",
      "         6.0       0.19      0.16      0.17       393\n",
      "         2.0       0.30      0.33      0.31       523\n",
      "         9.0       0.41      0.27      0.33       375\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.24      0.25      0.24      3438\n",
      "weighted avg       0.27      0.30      0.28      3438\n",
      "\n",
      "accuracy:  0.29697498545666084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.75      0.62       631\n",
      "         8.0       0.17      0.17      0.17       253\n",
      "         1.0       0.16      0.14      0.15       243\n",
      "         4.0       0.16      0.13      0.14       302\n",
      "         7.0       0.13      0.13      0.13       238\n",
      "         3.0       0.20      0.20      0.20       436\n",
      "         6.0       0.22      0.17      0.19       400\n",
      "         2.0       0.33      0.34      0.34       566\n",
      "         9.0       0.40      0.31      0.35       369\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3158813263525305\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.72      0.57       600\n",
      "         8.0       0.17      0.18      0.18       271\n",
      "         1.0       0.17      0.15      0.16       249\n",
      "         4.0       0.17      0.14      0.16       322\n",
      "         7.0       0.17      0.14      0.15       256\n",
      "         3.0       0.22      0.16      0.19       442\n",
      "         6.0       0.22      0.19      0.20       396\n",
      "         2.0       0.31      0.37      0.33       524\n",
      "         9.0       0.43      0.29      0.34       378\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.28      0.30      0.29      3438\n",
      "\n",
      "accuracy:  0.3045375218150087\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.72      0.62       644\n",
      "         8.0       0.15      0.18      0.17       251\n",
      "         1.0       0.15      0.14      0.14       247\n",
      "         4.0       0.15      0.16      0.16       279\n",
      "         7.0       0.14      0.11      0.12       271\n",
      "         3.0       0.21      0.22      0.22       418\n",
      "         6.0       0.25      0.18      0.21       397\n",
      "         2.0       0.33      0.33      0.33       565\n",
      "         9.0       0.39      0.28      0.33       366\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.30      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.31180919139034324\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.60       614\n",
      "         8.0       0.14      0.16      0.15       261\n",
      "         1.0       0.17      0.16      0.16       243\n",
      "         4.0       0.13      0.12      0.13       288\n",
      "         7.0       0.18      0.15      0.16       256\n",
      "         3.0       0.24      0.19      0.21       453\n",
      "         6.0       0.26      0.18      0.21       417\n",
      "         2.0       0.33      0.36      0.34       548\n",
      "         9.0       0.38      0.30      0.33       358\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.31297265852239675\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.72      0.60       630\n",
      "         8.0       0.15      0.18      0.16       261\n",
      "         1.0       0.14      0.13      0.14       253\n",
      "         4.0       0.15      0.12      0.13       313\n",
      "         7.0       0.18      0.13      0.15       271\n",
      "         3.0       0.22      0.24      0.23       407\n",
      "         6.0       0.21      0.18      0.19       376\n",
      "         2.0       0.32      0.30      0.31       541\n",
      "         9.0       0.37      0.28      0.32       386\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.25      0.25      0.25      3438\n",
      "weighted avg       0.28      0.30      0.29      3438\n",
      "\n",
      "accuracy:  0.3033740546829552\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.73      0.62       622\n",
      "         8.0       0.14      0.15      0.14       249\n",
      "         1.0       0.19      0.15      0.17       260\n",
      "         4.0       0.14      0.15      0.15       284\n",
      "         7.0       0.19      0.16      0.17       262\n",
      "         3.0       0.23      0.21      0.22       430\n",
      "         6.0       0.19      0.15      0.17       402\n",
      "         2.0       0.33      0.37      0.35       555\n",
      "         9.0       0.38      0.27      0.32       374\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.31239092495637\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.75      0.60       622\n",
      "         8.0       0.16      0.16      0.16       273\n",
      "         1.0       0.18      0.19      0.18       236\n",
      "         4.0       0.19      0.14      0.16       317\n",
      "         7.0       0.14      0.09      0.11       265\n",
      "         3.0       0.19      0.18      0.19       430\n",
      "         6.0       0.21      0.17      0.19       391\n",
      "         2.0       0.31      0.34      0.32       534\n",
      "         9.0       0.38      0.29      0.33       370\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.28      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.3065735892961024\n",
      "mean accuracy 0.3085805700988947\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model2 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.78      0.62       604\n",
      "         8.0       0.16      0.12      0.14       261\n",
      "         1.0       0.19      0.15      0.17       254\n",
      "         4.0       0.16      0.13      0.14       289\n",
      "         7.0       0.15      0.11      0.13       250\n",
      "         3.0       0.23      0.22      0.22       439\n",
      "         6.0       0.22      0.15      0.18       420\n",
      "         2.0       0.31      0.42      0.36       526\n",
      "         9.0       0.45      0.30      0.36       395\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3219895287958115\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.77      0.63       640\n",
      "         8.0       0.16      0.14      0.15       261\n",
      "         1.0       0.14      0.11      0.12       242\n",
      "         4.0       0.19      0.17      0.18       312\n",
      "         7.0       0.16      0.09      0.12       277\n",
      "         3.0       0.24      0.25      0.25       421\n",
      "         6.0       0.18      0.13      0.15       373\n",
      "         2.0       0.32      0.38      0.35       563\n",
      "         9.0       0.41      0.32      0.36       349\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.72      0.61       617\n",
      "         8.0       0.19      0.18      0.19       257\n",
      "         1.0       0.13      0.10      0.11       240\n",
      "         4.0       0.17      0.16      0.16       287\n",
      "         7.0       0.17      0.12      0.14       272\n",
      "         3.0       0.22      0.23      0.22       434\n",
      "         6.0       0.16      0.12      0.14       393\n",
      "         2.0       0.33      0.39      0.36       560\n",
      "         9.0       0.42      0.28      0.33       378\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.3100639906922629\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.77      0.62       627\n",
      "         8.0       0.14      0.11      0.12       265\n",
      "         1.0       0.18      0.15      0.16       256\n",
      "         4.0       0.18      0.12      0.14       314\n",
      "         7.0       0.15      0.09      0.12       255\n",
      "         3.0       0.20      0.21      0.21       426\n",
      "         6.0       0.17      0.12      0.15       400\n",
      "         2.0       0.33      0.42      0.37       529\n",
      "         9.0       0.40      0.33      0.36       366\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3193717277486911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.77      0.61       601\n",
      "         8.0       0.19      0.14      0.16       271\n",
      "         1.0       0.18      0.17      0.17       246\n",
      "         4.0       0.19      0.16      0.18       298\n",
      "         7.0       0.17      0.12      0.14       258\n",
      "         3.0       0.22      0.22      0.22       416\n",
      "         6.0       0.21      0.13      0.16       409\n",
      "         2.0       0.35      0.41      0.38       556\n",
      "         9.0       0.42      0.33      0.37       383\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.27      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.74      0.62       643\n",
      "         8.0       0.15      0.14      0.14       251\n",
      "         1.0       0.15      0.12      0.13       250\n",
      "         4.0       0.15      0.14      0.14       303\n",
      "         7.0       0.15      0.09      0.11       269\n",
      "         3.0       0.23      0.21      0.22       444\n",
      "         6.0       0.18      0.14      0.15       384\n",
      "         2.0       0.32      0.44      0.37       533\n",
      "         9.0       0.44      0.30      0.36       361\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3182082606166376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.75      0.61       619\n",
      "         8.0       0.17      0.14      0.15       261\n",
      "         1.0       0.15      0.12      0.14       259\n",
      "         4.0       0.19      0.15      0.17       299\n",
      "         7.0       0.17      0.11      0.14       261\n",
      "         3.0       0.20      0.23      0.21       417\n",
      "         6.0       0.16      0.13      0.14       378\n",
      "         2.0       0.32      0.38      0.35       550\n",
      "         9.0       0.44      0.29      0.35       394\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.3121000581733566\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.78      0.63       625\n",
      "         8.0       0.17      0.13      0.15       261\n",
      "         1.0       0.17      0.14      0.15       237\n",
      "         4.0       0.16      0.15      0.16       302\n",
      "         7.0       0.15      0.10      0.12       266\n",
      "         3.0       0.23      0.23      0.23       443\n",
      "         6.0       0.20      0.11      0.14       415\n",
      "         2.0       0.32      0.41      0.36       539\n",
      "         9.0       0.42      0.37      0.39       350\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.76      0.61       625\n",
      "         8.0       0.18      0.19      0.18       248\n",
      "         1.0       0.14      0.10      0.12       248\n",
      "         4.0       0.17      0.13      0.15       302\n",
      "         7.0       0.14      0.13      0.14       261\n",
      "         3.0       0.23      0.23      0.23       438\n",
      "         6.0       0.18      0.12      0.15       385\n",
      "         2.0       0.33      0.38      0.36       558\n",
      "         9.0       0.37      0.27      0.31       373\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.31442699243746364\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.76      0.62       619\n",
      "         8.0       0.22      0.14      0.17       274\n",
      "         1.0       0.13      0.12      0.12       248\n",
      "         4.0       0.16      0.14      0.15       299\n",
      "         7.0       0.14      0.09      0.11       266\n",
      "         3.0       0.22      0.23      0.22       422\n",
      "         6.0       0.25      0.16      0.19       408\n",
      "         2.0       0.30      0.40      0.34       531\n",
      "         9.0       0.38      0.32      0.35       371\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.31762652705061084\n",
      "mean accuracy 0.3194299011052938\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model3 = sum(acc)/10 \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.61       611\n",
      "         8.0       0.22      0.17      0.19       276\n",
      "         1.0       0.15      0.12      0.13       253\n",
      "         4.0       0.15      0.15      0.15       273\n",
      "         7.0       0.09      0.05      0.06       282\n",
      "         3.0       0.24      0.20      0.22       447\n",
      "         6.0       0.19      0.14      0.16       392\n",
      "         2.0       0.31      0.44      0.37       531\n",
      "         9.0       0.39      0.31      0.35       373\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.28      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.314717859220477\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.80      0.63       633\n",
      "         8.0       0.17      0.15      0.16       246\n",
      "         1.0       0.16      0.12      0.14       243\n",
      "         4.0       0.16      0.09      0.11       328\n",
      "         7.0       0.17      0.11      0.14       245\n",
      "         3.0       0.24      0.28      0.26       413\n",
      "         6.0       0.19      0.10      0.13       401\n",
      "         2.0       0.32      0.42      0.36       558\n",
      "         9.0       0.38      0.31      0.34       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.60       611\n",
      "         8.0       0.18      0.18      0.18       262\n",
      "         1.0       0.14      0.09      0.11       256\n",
      "         4.0       0.17      0.13      0.15       297\n",
      "         7.0       0.14      0.10      0.12       251\n",
      "         3.0       0.21      0.21      0.21       442\n",
      "         6.0       0.21      0.17      0.19       379\n",
      "         2.0       0.35      0.41      0.38       561\n",
      "         9.0       0.44      0.37      0.40       379\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.78      0.64       633\n",
      "         8.0       0.18      0.11      0.14       260\n",
      "         1.0       0.18      0.17      0.17       240\n",
      "         4.0       0.20      0.14      0.17       304\n",
      "         7.0       0.16      0.08      0.10       276\n",
      "         3.0       0.20      0.25      0.23       418\n",
      "         6.0       0.19      0.10      0.13       414\n",
      "         2.0       0.30      0.46      0.36       528\n",
      "         9.0       0.43      0.30      0.36       365\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.25      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32693426410703896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.76      0.62       642\n",
      "         8.0       0.17      0.15      0.16       261\n",
      "         1.0       0.19      0.15      0.16       254\n",
      "         4.0       0.15      0.14      0.15       281\n",
      "         7.0       0.10      0.05      0.07       264\n",
      "         3.0       0.20      0.21      0.20       425\n",
      "         6.0       0.22      0.15      0.18       396\n",
      "         2.0       0.31      0.38      0.34       543\n",
      "         9.0       0.41      0.33      0.36       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.31762652705061084\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.79      0.62       602\n",
      "         8.0       0.18      0.13      0.15       261\n",
      "         1.0       0.19      0.13      0.15       242\n",
      "         4.0       0.21      0.17      0.19       320\n",
      "         7.0       0.14      0.08      0.10       263\n",
      "         3.0       0.21      0.25      0.23       435\n",
      "         6.0       0.20      0.12      0.15       397\n",
      "         2.0       0.30      0.41      0.35       546\n",
      "         9.0       0.38      0.28      0.32       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3193717277486911\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.55      0.72      0.63       635\n",
      "         8.0       0.18      0.19      0.18       254\n",
      "         1.0       0.15      0.10      0.12       259\n",
      "         4.0       0.17      0.14      0.15       289\n",
      "         7.0       0.16      0.10      0.12       263\n",
      "         3.0       0.20      0.25      0.22       419\n",
      "         6.0       0.19      0.15      0.17       376\n",
      "         2.0       0.33      0.42      0.37       566\n",
      "         9.0       0.44      0.33      0.38       377\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.32      0.31      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.77      0.61       609\n",
      "         8.0       0.19      0.12      0.15       268\n",
      "         1.0       0.15      0.14      0.15       237\n",
      "         4.0       0.18      0.13      0.15       312\n",
      "         7.0       0.17      0.11      0.14       264\n",
      "         3.0       0.26      0.24      0.25       441\n",
      "         6.0       0.20      0.11      0.14       417\n",
      "         2.0       0.29      0.42      0.35       523\n",
      "         9.0       0.38      0.32      0.35       367\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3190808609656777\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.77      0.63       635\n",
      "         8.0       0.16      0.15      0.15       243\n",
      "         1.0       0.14      0.12      0.13       237\n",
      "         4.0       0.17      0.12      0.14       304\n",
      "         7.0       0.13      0.06      0.08       266\n",
      "         3.0       0.25      0.24      0.25       452\n",
      "         6.0       0.19      0.12      0.14       402\n",
      "         2.0       0.31      0.44      0.37       536\n",
      "         9.0       0.43      0.40      0.41       363\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3310063990692263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.78      0.62       609\n",
      "         8.0       0.19      0.14      0.16       279\n",
      "         1.0       0.21      0.12      0.16       259\n",
      "         4.0       0.21      0.21      0.21       297\n",
      "         7.0       0.14      0.10      0.11       261\n",
      "         3.0       0.22      0.26      0.24       408\n",
      "         6.0       0.17      0.12      0.14       391\n",
      "         2.0       0.33      0.39      0.36       553\n",
      "         9.0       0.41      0.29      0.34       381\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3240255962769052\n",
      "mean accuracy 0.32309482257126243\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model4 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.77      0.61       593\n",
      "         8.0       0.20      0.12      0.15       259\n",
      "         1.0       0.18      0.13      0.15       249\n",
      "         4.0       0.15      0.13      0.14       310\n",
      "         7.0       0.16      0.10      0.12       273\n",
      "         3.0       0.21      0.25      0.23       433\n",
      "         6.0       0.18      0.12      0.14       385\n",
      "         2.0       0.31      0.41      0.35       546\n",
      "         9.0       0.45      0.29      0.35       390\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3150087260034904\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.78      0.63       651\n",
      "         8.0       0.19      0.13      0.16       263\n",
      "         1.0       0.15      0.11      0.13       247\n",
      "         4.0       0.14      0.12      0.13       291\n",
      "         7.0       0.14      0.07      0.09       254\n",
      "         3.0       0.25      0.23      0.24       427\n",
      "         6.0       0.21      0.12      0.15       408\n",
      "         2.0       0.30      0.44      0.36       543\n",
      "         9.0       0.36      0.33      0.34       354\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32693426410703896\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.78      0.62       638\n",
      "         8.0       0.15      0.07      0.10       279\n",
      "         1.0       0.22      0.12      0.16       242\n",
      "         4.0       0.17      0.18      0.17       265\n",
      "         7.0       0.14      0.10      0.12       270\n",
      "         3.0       0.20      0.21      0.21       422\n",
      "         6.0       0.19      0.12      0.15       396\n",
      "         2.0       0.34      0.47      0.39       556\n",
      "         9.0       0.46      0.32      0.38       370\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.73      0.62       606\n",
      "         8.0       0.19      0.21      0.20       243\n",
      "         1.0       0.19      0.13      0.15       254\n",
      "         4.0       0.22      0.14      0.17       336\n",
      "         7.0       0.13      0.07      0.09       257\n",
      "         3.0       0.25      0.25      0.25       438\n",
      "         6.0       0.22      0.15      0.18       397\n",
      "         2.0       0.30      0.44      0.35       533\n",
      "         9.0       0.39      0.34      0.37       374\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.61       624\n",
      "         8.0       0.14      0.11      0.13       253\n",
      "         1.0       0.20      0.14      0.17       234\n",
      "         4.0       0.14      0.14      0.14       276\n",
      "         7.0       0.23      0.09      0.13       279\n",
      "         3.0       0.26      0.21      0.23       447\n",
      "         6.0       0.20      0.13      0.16       404\n",
      "         2.0       0.32      0.46      0.38       541\n",
      "         9.0       0.40      0.33      0.36       380\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3275159976730657\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.61       620\n",
      "         8.0       0.19      0.10      0.13       269\n",
      "         1.0       0.20      0.10      0.13       262\n",
      "         4.0       0.17      0.11      0.13       325\n",
      "         7.0       0.18      0.10      0.13       248\n",
      "         3.0       0.19      0.27      0.22       413\n",
      "         6.0       0.19      0.14      0.16       389\n",
      "         2.0       0.30      0.41      0.35       548\n",
      "         9.0       0.42      0.29      0.34       364\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3170447934845841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.78      0.62       617\n",
      "         8.0       0.19      0.11      0.14       261\n",
      "         1.0       0.21      0.11      0.15       253\n",
      "         4.0       0.17      0.11      0.14       305\n",
      "         7.0       0.11      0.07      0.09       258\n",
      "         3.0       0.20      0.23      0.21       412\n",
      "         6.0       0.17      0.13      0.15       382\n",
      "         2.0       0.31      0.44      0.36       553\n",
      "         9.0       0.41      0.28      0.34       397\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3179173938336242\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.75      0.62       627\n",
      "         8.0       0.15      0.13      0.14       261\n",
      "         1.0       0.23      0.12      0.16       243\n",
      "         4.0       0.18      0.17      0.18       296\n",
      "         7.0       0.13      0.07      0.09       269\n",
      "         3.0       0.24      0.26      0.25       448\n",
      "         6.0       0.17      0.08      0.11       411\n",
      "         2.0       0.29      0.42      0.35       536\n",
      "         9.0       0.38      0.36      0.37       347\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.31995346131471786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.79      0.63       620\n",
      "         8.0       0.24      0.13      0.17       267\n",
      "         1.0       0.16      0.11      0.13       240\n",
      "         4.0       0.20      0.14      0.16       301\n",
      "         7.0       0.13      0.08      0.10       258\n",
      "         3.0       0.19      0.20      0.19       426\n",
      "         6.0       0.18      0.12      0.14       401\n",
      "         2.0       0.33      0.44      0.38       544\n",
      "         9.0       0.39      0.37      0.38       381\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.79      0.63       624\n",
      "         8.0       0.17      0.13      0.15       255\n",
      "         1.0       0.21      0.14      0.17       256\n",
      "         4.0       0.19      0.16      0.17       300\n",
      "         7.0       0.14      0.06      0.08       269\n",
      "         3.0       0.24      0.25      0.24       434\n",
      "         6.0       0.19      0.15      0.16       392\n",
      "         2.0       0.32      0.44      0.37       545\n",
      "         9.0       0.42      0.29      0.35       363\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3307155322862129\n",
      "mean accuracy 0.3238510762070972\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model5 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.78      0.62       618\n",
      "         8.0       0.15      0.11      0.13       250\n",
      "         1.0       0.22      0.10      0.14       245\n",
      "         4.0       0.17      0.15      0.16       288\n",
      "         7.0       0.11      0.07      0.08       266\n",
      "         3.0       0.24      0.25      0.25       424\n",
      "         6.0       0.20      0.11      0.14       406\n",
      "         2.0       0.30      0.46      0.36       555\n",
      "         9.0       0.40      0.27      0.32       386\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3219895287958115\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.78      0.62       626\n",
      "         8.0       0.18      0.11      0.14       272\n",
      "         1.0       0.24      0.14      0.18       251\n",
      "         4.0       0.21      0.16      0.18       313\n",
      "         7.0       0.16      0.05      0.08       261\n",
      "         3.0       0.22      0.26      0.24       436\n",
      "         6.0       0.17      0.10      0.13       387\n",
      "         2.0       0.32      0.47      0.38       534\n",
      "         9.0       0.43      0.37      0.40       358\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.3368237347294939\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.79      0.63       617\n",
      "         8.0       0.17      0.12      0.14       266\n",
      "         1.0       0.16      0.12      0.14       234\n",
      "         4.0       0.18      0.13      0.15       303\n",
      "         7.0       0.17      0.05      0.07       266\n",
      "         3.0       0.22      0.27      0.24       429\n",
      "         6.0       0.20      0.13      0.16       394\n",
      "         2.0       0.33      0.40      0.36       575\n",
      "         9.0       0.34      0.38      0.36       354\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.61       627\n",
      "         8.0       0.13      0.07      0.09       256\n",
      "         1.0       0.22      0.10      0.14       262\n",
      "         4.0       0.17      0.13      0.15       298\n",
      "         7.0       0.15      0.09      0.12       261\n",
      "         3.0       0.18      0.20      0.19       431\n",
      "         6.0       0.20      0.11      0.14       399\n",
      "         2.0       0.28      0.51      0.36       514\n",
      "         9.0       0.45      0.24      0.32       390\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.25      0.24      3438\n",
      "weighted avg       0.28      0.31      0.28      3438\n",
      "\n",
      "accuracy:  0.31326352530541013\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.78      0.63       638\n",
      "         8.0       0.12      0.06      0.08       280\n",
      "         1.0       0.16      0.12      0.14       238\n",
      "         4.0       0.22      0.20      0.21       291\n",
      "         7.0       0.08      0.03      0.04       278\n",
      "         3.0       0.21      0.25      0.23       411\n",
      "         6.0       0.17      0.07      0.10       411\n",
      "         2.0       0.29      0.45      0.35       533\n",
      "         9.0       0.39      0.33      0.36       358\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.25      0.24      3438\n",
      "weighted avg       0.27      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3196625945317045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.80      0.62       606\n",
      "         8.0       0.15      0.12      0.13       242\n",
      "         1.0       0.17      0.09      0.11       258\n",
      "         4.0       0.23      0.13      0.17       310\n",
      "         7.0       0.09      0.05      0.06       249\n",
      "         3.0       0.22      0.24      0.23       449\n",
      "         6.0       0.19      0.14      0.16       382\n",
      "         2.0       0.32      0.47      0.38       556\n",
      "         9.0       0.44      0.32      0.37       386\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32897033158813266\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.81      0.63       627\n",
      "         8.0       0.19      0.09      0.12       260\n",
      "         1.0       0.17      0.14      0.15       236\n",
      "         4.0       0.22      0.15      0.18       322\n",
      "         7.0       0.12      0.05      0.07       264\n",
      "         3.0       0.19      0.24      0.21       416\n",
      "         6.0       0.20      0.10      0.13       398\n",
      "         2.0       0.30      0.46      0.36       542\n",
      "         9.0       0.40      0.28      0.33       373\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.24      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3263525305410122\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.77      0.61       617\n",
      "         8.0       0.18      0.14      0.16       262\n",
      "         1.0       0.24      0.11      0.15       260\n",
      "         4.0       0.14      0.12      0.13       279\n",
      "         7.0       0.16      0.07      0.10       263\n",
      "         3.0       0.22      0.22      0.22       444\n",
      "         6.0       0.19      0.12      0.14       395\n",
      "         2.0       0.29      0.44      0.35       547\n",
      "         9.0       0.39      0.32      0.35       371\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.31878999418266435\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.78      0.62       617\n",
      "         8.0       0.22      0.14      0.17       260\n",
      "         1.0       0.20      0.12      0.15       256\n",
      "         4.0       0.21      0.16      0.18       303\n",
      "         7.0       0.17      0.09      0.12       242\n",
      "         3.0       0.21      0.26      0.23       425\n",
      "         6.0       0.18      0.11      0.13       396\n",
      "         2.0       0.29      0.44      0.35       547\n",
      "         9.0       0.41      0.26      0.31       392\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32315299592786506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.82      0.63       627\n",
      "         8.0       0.18      0.11      0.13       262\n",
      "         1.0       0.18      0.10      0.13       240\n",
      "         4.0       0.23      0.15      0.19       298\n",
      "         7.0       0.11      0.03      0.05       285\n",
      "         3.0       0.21      0.24      0.22       435\n",
      "         6.0       0.18      0.08      0.11       397\n",
      "         2.0       0.31      0.48      0.38       542\n",
      "         9.0       0.38      0.36      0.37       352\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.33420593368237345\n",
      "mean accuracy 0.32515997673065733\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model6 = sum(acc)/10\n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.79      0.61       622\n",
      "         8.0       0.22      0.08      0.12       270\n",
      "         1.0       0.20      0.08      0.12       256\n",
      "         4.0       0.24      0.19      0.21       294\n",
      "         7.0       0.15      0.06      0.08       260\n",
      "         3.0       0.21      0.31      0.25       405\n",
      "         6.0       0.16      0.10      0.12       388\n",
      "         2.0       0.32      0.46      0.37       569\n",
      "         9.0       0.47      0.32      0.38       374\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.27      0.27      0.25      3438\n",
      "weighted avg       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.33507853403141363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       622\n",
      "         8.0       0.20      0.13      0.15       252\n",
      "         1.0       0.21      0.08      0.12       240\n",
      "         4.0       0.19      0.11      0.14       307\n",
      "         7.0       0.06      0.01      0.02       267\n",
      "         3.0       0.23      0.26      0.25       455\n",
      "         6.0       0.19      0.09      0.12       405\n",
      "         2.0       0.30      0.52      0.38       520\n",
      "         9.0       0.40      0.33      0.36       370\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3336242001163467\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.83      0.63       636\n",
      "         8.0       0.18      0.06      0.09       267\n",
      "         1.0       0.22      0.09      0.13       242\n",
      "         4.0       0.20      0.15      0.17       294\n",
      "         7.0       0.15      0.05      0.08       266\n",
      "         3.0       0.22      0.30      0.25       404\n",
      "         6.0       0.17      0.08      0.11       401\n",
      "         2.0       0.31      0.49      0.38       548\n",
      "         9.0       0.40      0.27      0.32       380\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.26      0.26      0.24      3438\n",
      "weighted avg       0.29      0.34      0.29      3438\n",
      "\n",
      "accuracy:  0.33595113438045376\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.79      0.61       608\n",
      "         8.0       0.21      0.12      0.15       255\n",
      "         1.0       0.21      0.09      0.13       254\n",
      "         4.0       0.24      0.17      0.20       307\n",
      "         7.0       0.12      0.02      0.04       261\n",
      "         3.0       0.22      0.27      0.24       456\n",
      "         6.0       0.17      0.10      0.13       392\n",
      "         2.0       0.30      0.48      0.37       541\n",
      "         9.0       0.42      0.36      0.39       364\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.80      0.64       647\n",
      "         8.0       0.22      0.11      0.14       269\n",
      "         1.0       0.20      0.09      0.12       233\n",
      "         4.0       0.18      0.18      0.18       277\n",
      "         7.0       0.18      0.06      0.09       276\n",
      "         3.0       0.23      0.26      0.24       441\n",
      "         6.0       0.14      0.07      0.09       392\n",
      "         2.0       0.30      0.45      0.36       541\n",
      "         9.0       0.38      0.34      0.36       362\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3321698662012798\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.83      0.61       597\n",
      "         8.0       0.17      0.07      0.10       253\n",
      "         1.0       0.18      0.10      0.12       263\n",
      "         4.0       0.19      0.09      0.12       324\n",
      "         7.0       0.05      0.02      0.02       251\n",
      "         3.0       0.21      0.28      0.24       419\n",
      "         6.0       0.17      0.10      0.12       401\n",
      "         2.0       0.29      0.48      0.36       548\n",
      "         9.0       0.44      0.29      0.35       382\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.25      0.23      3438\n",
      "weighted avg       0.27      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.31995346131471786\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.84      0.61       621\n",
      "         8.0       0.21      0.09      0.12       269\n",
      "         1.0       0.21      0.09      0.13       252\n",
      "         4.0       0.20      0.14      0.17       308\n",
      "         7.0       0.12      0.02      0.04       276\n",
      "         3.0       0.21      0.26      0.24       423\n",
      "         6.0       0.17      0.10      0.13       387\n",
      "         2.0       0.30      0.45      0.36       537\n",
      "         9.0       0.38      0.31      0.34       365\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32606166375799883\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.78      0.63       623\n",
      "         8.0       0.22      0.13      0.16       253\n",
      "         1.0       0.20      0.11      0.14       244\n",
      "         4.0       0.18      0.13      0.15       293\n",
      "         7.0       0.10      0.03      0.05       251\n",
      "         3.0       0.21      0.29      0.24       437\n",
      "         6.0       0.20      0.10      0.13       406\n",
      "         2.0       0.32      0.53      0.40       552\n",
      "         9.0       0.41      0.27      0.33       379\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.30      0.34      0.30      3438\n",
      "\n",
      "accuracy:  0.33507853403141363\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       609\n",
      "         8.0       0.18      0.05      0.07       281\n",
      "         1.0       0.20      0.12      0.15       251\n",
      "         4.0       0.19      0.19      0.19       302\n",
      "         7.0       0.31      0.04      0.07       278\n",
      "         3.0       0.22      0.22      0.22       441\n",
      "         6.0       0.19      0.11      0.14       388\n",
      "         2.0       0.27      0.47      0.35       527\n",
      "         9.0       0.39      0.31      0.35       361\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.26      0.24      3438\n",
      "weighted avg       0.29      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.32315299592786506\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.81      0.65       635\n",
      "         8.0       0.18      0.14      0.16       241\n",
      "         1.0       0.25      0.10      0.14       245\n",
      "         4.0       0.26      0.14      0.18       299\n",
      "         7.0       0.15      0.05      0.08       249\n",
      "         3.0       0.21      0.35      0.26       419\n",
      "         6.0       0.19      0.07      0.10       405\n",
      "         2.0       0.32      0.48      0.38       562\n",
      "         9.0       0.42      0.31      0.36       383\n",
      "\n",
      "   micro avg       0.35      0.35      0.35      3438\n",
      "   macro avg       0.28      0.27      0.26      3438\n",
      "weighted avg       0.31      0.35      0.31      3438\n",
      "\n",
      "accuracy:  0.34642233856893545\n",
      "mean accuracy 0.3320826061663758\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model7 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.80      0.63       630\n",
      "         8.0       0.19      0.11      0.14       255\n",
      "         1.0       0.27      0.08      0.13       239\n",
      "         4.0       0.21      0.15      0.17       310\n",
      "         7.0       0.12      0.00      0.01       265\n",
      "         3.0       0.19      0.31      0.24       410\n",
      "         6.0       0.15      0.05      0.07       404\n",
      "         2.0       0.29      0.50      0.37       548\n",
      "         9.0       0.40      0.29      0.34       377\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.23      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.84      0.61       614\n",
      "         8.0       0.08      0.01      0.02       267\n",
      "         1.0       0.20      0.07      0.10       257\n",
      "         4.0       0.20      0.14      0.16       291\n",
      "         7.0       0.15      0.06      0.09       262\n",
      "         3.0       0.19      0.18      0.19       450\n",
      "         6.0       0.16      0.07      0.10       389\n",
      "         2.0       0.30      0.56      0.39       541\n",
      "         9.0       0.43      0.31      0.36       367\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.25      0.22      3438\n",
      "weighted avg       0.27      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3266433973240256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.80      0.62       622\n",
      "         8.0       0.14      0.08      0.10       252\n",
      "         1.0       0.33      0.07      0.11       253\n",
      "         4.0       0.20      0.20      0.20       309\n",
      "         7.0       0.20      0.02      0.03       260\n",
      "         3.0       0.23      0.25      0.24       444\n",
      "         6.0       0.18      0.07      0.10       398\n",
      "         2.0       0.29      0.59      0.39       535\n",
      "         9.0       0.45      0.25      0.32       365\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.26      0.23      3438\n",
      "weighted avg       0.30      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.33391506689936007\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.84      0.62       622\n",
      "         8.0       0.21      0.04      0.06       270\n",
      "         1.0       0.21      0.06      0.10       243\n",
      "         4.0       0.26      0.14      0.18       292\n",
      "         7.0       0.17      0.03      0.05       267\n",
      "         3.0       0.21      0.34      0.26       416\n",
      "         6.0       0.14      0.04      0.07       395\n",
      "         2.0       0.29      0.51      0.37       554\n",
      "         9.0       0.37      0.29      0.33       379\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.23      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3344968004653869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.88      0.62       601\n",
      "         8.0       0.12      0.02      0.03       282\n",
      "         1.0       0.22      0.06      0.10       259\n",
      "         4.0       0.21      0.15      0.17       289\n",
      "         7.0       0.29      0.01      0.03       274\n",
      "         3.0       0.21      0.22      0.21       445\n",
      "         6.0       0.15      0.10      0.12       394\n",
      "         2.0       0.28      0.55      0.37       530\n",
      "         9.0       0.46      0.27      0.34       364\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.25      0.22      3438\n",
      "weighted avg       0.29      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3266433973240256\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.78      0.63       643\n",
      "         8.0       0.15      0.10      0.12       240\n",
      "         1.0       0.35      0.08      0.13       237\n",
      "         4.0       0.21      0.16      0.18       312\n",
      "         7.0       0.19      0.04      0.07       253\n",
      "         3.0       0.20      0.30      0.24       415\n",
      "         6.0       0.21      0.06      0.09       399\n",
      "         2.0       0.29      0.48      0.36       559\n",
      "         9.0       0.36      0.30      0.33       380\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.25      0.24      3438\n",
      "weighted avg       0.30      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.81      0.64       659\n",
      "         8.0       0.20      0.10      0.14       253\n",
      "         1.0       0.41      0.08      0.13       262\n",
      "         4.0       0.23      0.20      0.21       281\n",
      "         7.0       0.17      0.04      0.06       255\n",
      "         3.0       0.20      0.26      0.23       413\n",
      "         6.0       0.16      0.14      0.15       377\n",
      "         2.0       0.33      0.45      0.38       580\n",
      "         9.0       0.38      0.33      0.36       358\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.29      0.27      0.26      3438\n",
      "weighted avg       0.32      0.34      0.31      3438\n",
      "\n",
      "accuracy:  0.34467713787085513\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.84      0.60       585\n",
      "         8.0       0.19      0.03      0.05       269\n",
      "         1.0       0.16      0.07      0.09       234\n",
      "         4.0       0.26      0.16      0.19       320\n",
      "         7.0       0.22      0.01      0.03       272\n",
      "         3.0       0.20      0.23      0.21       447\n",
      "         6.0       0.18      0.05      0.07       416\n",
      "         2.0       0.26      0.61      0.37       509\n",
      "         9.0       0.44      0.25      0.32       386\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.25      0.21      3438\n",
      "weighted avg       0.28      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.31849912739965097\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.81      0.63       626\n",
      "         8.0       0.19      0.08      0.12       249\n",
      "         1.0       0.16      0.09      0.11       237\n",
      "         4.0       0.20      0.12      0.15       305\n",
      "         7.0       0.29      0.01      0.03       271\n",
      "         3.0       0.23      0.24      0.24       450\n",
      "         6.0       0.21      0.12      0.15       390\n",
      "         2.0       0.29      0.55      0.38       541\n",
      "         9.0       0.38      0.30      0.34       369\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.26      0.24      3438\n",
      "weighted avg       0.30      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3344968004653869\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.84      0.62       618\n",
      "         8.0       0.16      0.03      0.05       273\n",
      "         1.0       0.41      0.05      0.10       259\n",
      "         4.0       0.20      0.16      0.17       296\n",
      "         7.0       0.13      0.03      0.05       256\n",
      "         3.0       0.19      0.32      0.24       410\n",
      "         6.0       0.12      0.03      0.05       403\n",
      "         2.0       0.30      0.52      0.38       548\n",
      "         9.0       0.41      0.30      0.35       375\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.25      0.22      3438\n",
      "weighted avg       0.29      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3295520651541594\n",
      "mean accuracy 0.3307737056428156\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model8 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.62       623\n",
      "         8.0       0.16      0.02      0.03       272\n",
      "         1.0       0.12      0.03      0.05       252\n",
      "         4.0       0.21      0.19      0.20       300\n",
      "         7.0       0.00      0.00      0.00       265\n",
      "         3.0       0.21      0.19      0.20       443\n",
      "         6.0       0.18      0.13      0.15       364\n",
      "         2.0       0.27      0.54      0.36       539\n",
      "         9.0       0.41      0.29      0.34       380\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.23      0.25      0.22      3438\n",
      "weighted avg       0.26      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.83      0.63       621\n",
      "         8.0       0.09      0.02      0.04       250\n",
      "         1.0       0.26      0.05      0.08       244\n",
      "         4.0       0.20      0.14      0.16       301\n",
      "         7.0       0.13      0.03      0.05       262\n",
      "         3.0       0.17      0.29      0.22       417\n",
      "         6.0       0.22      0.01      0.02       429\n",
      "         2.0       0.27      0.53      0.36       550\n",
      "         9.0       0.43      0.27      0.34       364\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.24      0.21      3438\n",
      "weighted avg       0.28      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3196625945317045\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.46      0.82      0.59       605\n",
      "         8.0       0.29      0.01      0.01       286\n",
      "         1.0       0.23      0.07      0.10       246\n",
      "         4.0       0.20      0.12      0.15       292\n",
      "         7.0       0.20      0.01      0.01       262\n",
      "         3.0       0.19      0.33      0.24       410\n",
      "         6.0       0.17      0.05      0.08       392\n",
      "         2.0       0.30      0.50      0.37       572\n",
      "         9.0       0.40      0.33      0.36       373\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.25      0.21      3438\n",
      "weighted avg       0.29      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.32431646305991857\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.83      0.65       639\n",
      "         8.0       0.10      0.03      0.05       236\n",
      "         1.0       0.29      0.04      0.06       250\n",
      "         4.0       0.20      0.16      0.18       309\n",
      "         7.0       0.17      0.05      0.08       265\n",
      "         3.0       0.21      0.19      0.20       450\n",
      "         6.0       0.20      0.04      0.07       401\n",
      "         2.0       0.25      0.61      0.36       517\n",
      "         9.0       0.41      0.25      0.31       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.25      0.22      3438\n",
      "weighted avg       0.29      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.85      0.62       625\n",
      "         8.0       0.25      0.02      0.03       258\n",
      "         1.0       0.28      0.04      0.07       260\n",
      "         4.0       0.22      0.14      0.17       306\n",
      "         7.0       0.18      0.01      0.02       258\n",
      "         3.0       0.17      0.22      0.19       428\n",
      "         6.0       0.20      0.04      0.06       396\n",
      "         2.0       0.27      0.60      0.38       540\n",
      "         9.0       0.43      0.30      0.35       367\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.25      0.21      3438\n",
      "weighted avg       0.29      0.33      0.26      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.81      0.62       619\n",
      "         8.0       0.14      0.04      0.06       264\n",
      "         1.0       0.18      0.07      0.10       236\n",
      "         4.0       0.19      0.17      0.18       295\n",
      "         7.0       0.17      0.01      0.03       269\n",
      "         3.0       0.18      0.20      0.19       432\n",
      "         6.0       0.18      0.09      0.12       397\n",
      "         2.0       0.28      0.55      0.38       549\n",
      "         9.0       0.41      0.28      0.34       377\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.22      3438\n",
      "weighted avg       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3240255962769052\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.85      0.63       626\n",
      "         8.0       0.20      0.05      0.08       261\n",
      "         1.0       0.16      0.04      0.07       243\n",
      "         4.0       0.22      0.15      0.18       304\n",
      "         7.0       0.17      0.01      0.02       273\n",
      "         3.0       0.20      0.22      0.21       423\n",
      "         6.0       0.13      0.07      0.10       379\n",
      "         2.0       0.26      0.51      0.35       540\n",
      "         9.0       0.38      0.30      0.34       389\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.24      0.22      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       618\n",
      "         8.0       0.10      0.02      0.03       261\n",
      "         1.0       0.33      0.02      0.04       253\n",
      "         4.0       0.20      0.18      0.19       297\n",
      "         7.0       0.12      0.02      0.03       254\n",
      "         3.0       0.20      0.26      0.23       437\n",
      "         6.0       0.15      0.04      0.06       414\n",
      "         2.0       0.28      0.58      0.38       549\n",
      "         9.0       0.41      0.28      0.33       355\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.21      3438\n",
      "weighted avg       0.28      0.33      0.26      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.83      0.62       624\n",
      "         8.0       0.16      0.05      0.08       258\n",
      "         1.0       0.22      0.06      0.09       242\n",
      "         4.0       0.19      0.20      0.19       287\n",
      "         7.0       0.24      0.04      0.07       270\n",
      "         3.0       0.22      0.18      0.20       450\n",
      "         6.0       0.16      0.07      0.09       382\n",
      "         2.0       0.28      0.56      0.37       554\n",
      "         9.0       0.40      0.29      0.33       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.25      0.23      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3310063990692263\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.85      0.62       620\n",
      "         8.0       0.21      0.02      0.03       264\n",
      "         1.0       0.24      0.04      0.07       254\n",
      "         4.0       0.22      0.11      0.14       314\n",
      "         7.0       0.14      0.00      0.01       257\n",
      "         3.0       0.18      0.30      0.23       410\n",
      "         6.0       0.21      0.04      0.06       411\n",
      "         2.0       0.27      0.60      0.37       535\n",
      "         9.0       0.42      0.26      0.32       373\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.24      0.21      3438\n",
      "weighted avg       0.29      0.33      0.26      3438\n",
      "\n",
      "accuracy:  0.32722513089005234\n",
      "mean accuracy 0.3258580570098895\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'euclidean')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "\n",
    "        \n",
    "mean_accuracy_model9 = sum(acc)/10  \n",
    "mean_accuracy_model_euclidean.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29909831297265854, 0.3085805700988947, 0.3194299011052938, 0.32309482257126243, 0.3238510762070972, 0.32515997673065733, 0.3320826061663758, 0.3307737056428156, 0.3258580570098895]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_euclidean\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minkowski and k tuning on 5% noise dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.61      0.61      0.61       621\n",
      "         8.0       0.16      0.12      0.14       265\n",
      "         1.0       0.16      0.20      0.17       235\n",
      "         4.0       0.14      0.13      0.13       309\n",
      "         7.0       0.13      0.14      0.13       266\n",
      "         3.0       0.22      0.21      0.22       410\n",
      "         6.0       0.24      0.19      0.21       416\n",
      "         2.0       0.30      0.33      0.31       545\n",
      "         9.0       0.28      0.32      0.30       371\n",
      "\n",
      "   micro avg       0.29      0.29      0.29      3438\n",
      "   macro avg       0.25      0.25      0.25      3438\n",
      "weighted avg       0.29      0.29      0.29      3438\n",
      "\n",
      "accuracy:  0.28970331588132636\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.58      0.59      0.59       623\n",
      "         8.0       0.17      0.17      0.17       257\n",
      "         1.0       0.18      0.14      0.16       261\n",
      "         4.0       0.14      0.14      0.14       292\n",
      "         7.0       0.18      0.18      0.18       261\n",
      "         3.0       0.22      0.21      0.21       450\n",
      "         6.0       0.21      0.21      0.21       377\n",
      "         2.0       0.30      0.32      0.31       544\n",
      "         9.0       0.35      0.38      0.37       373\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.29813845258871435\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.65      0.61      0.62       661\n",
      "         8.0       0.16      0.17      0.17       247\n",
      "         1.0       0.18      0.15      0.16       262\n",
      "         4.0       0.15      0.15      0.15       290\n",
      "         7.0       0.18      0.18      0.18       269\n",
      "         3.0       0.20      0.18      0.19       436\n",
      "         6.0       0.20      0.21      0.21       390\n",
      "         2.0       0.31      0.33      0.32       526\n",
      "         9.0       0.30      0.35      0.32       357\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.3010471204188482\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.58      0.59       583\n",
      "         8.0       0.19      0.15      0.17       275\n",
      "         1.0       0.15      0.14      0.14       234\n",
      "         4.0       0.16      0.17      0.17       311\n",
      "         7.0       0.15      0.14      0.15       258\n",
      "         3.0       0.22      0.22      0.22       424\n",
      "         6.0       0.22      0.21      0.21       403\n",
      "         2.0       0.32      0.36      0.34       563\n",
      "         9.0       0.37      0.36      0.36       387\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.2975567190226876\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.65      0.62      0.63       623\n",
      "         8.0       0.20      0.16      0.18       281\n",
      "         1.0       0.14      0.15      0.15       241\n",
      "         4.0       0.15      0.15      0.15       297\n",
      "         7.0       0.15      0.16      0.16       243\n",
      "         3.0       0.22      0.23      0.23       417\n",
      "         6.0       0.19      0.19      0.19       400\n",
      "         2.0       0.30      0.31      0.30       553\n",
      "         9.0       0.29      0.31      0.30       383\n",
      "\n",
      "   micro avg       0.29      0.29      0.29      3438\n",
      "   macro avg       0.26      0.25      0.25      3438\n",
      "weighted avg       0.30      0.29      0.30      3438\n",
      "\n",
      "accuracy:  0.29435718440954045\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.61      0.58      0.60       621\n",
      "         8.0       0.16      0.20      0.18       241\n",
      "         1.0       0.18      0.14      0.16       255\n",
      "         4.0       0.17      0.16      0.16       304\n",
      "         7.0       0.20      0.17      0.18       284\n",
      "         3.0       0.24      0.22      0.23       443\n",
      "         6.0       0.20      0.22      0.21       393\n",
      "         2.0       0.28      0.30      0.29       536\n",
      "         9.0       0.32      0.36      0.34       361\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.29668411867364747\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.64      0.58      0.61       640\n",
      "         8.0       0.17      0.16      0.16       261\n",
      "         1.0       0.15      0.16      0.15       241\n",
      "         4.0       0.13      0.15      0.14       294\n",
      "         7.0       0.18      0.16      0.17       277\n",
      "         3.0       0.25      0.23      0.24       454\n",
      "         6.0       0.20      0.22      0.21       395\n",
      "         2.0       0.30      0.35      0.32       518\n",
      "         9.0       0.33      0.32      0.32       358\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.29668411867364747\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.58      0.62      0.60       604\n",
      "         8.0       0.19      0.17      0.18       261\n",
      "         1.0       0.21      0.18      0.20       255\n",
      "         4.0       0.15      0.15      0.15       307\n",
      "         7.0       0.17      0.18      0.18       250\n",
      "         3.0       0.20      0.22      0.21       406\n",
      "         6.0       0.22      0.20      0.21       398\n",
      "         2.0       0.31      0.31      0.31       571\n",
      "         9.0       0.33      0.34      0.33       386\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.29988365328679467\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.62      0.60      0.61       619\n",
      "         8.0       0.22      0.22      0.22       263\n",
      "         1.0       0.14      0.13      0.13       243\n",
      "         4.0       0.14      0.15      0.14       286\n",
      "         7.0       0.15      0.16      0.16       257\n",
      "         3.0       0.23      0.22      0.23       419\n",
      "         6.0       0.22      0.26      0.24       379\n",
      "         2.0       0.33      0.37      0.35       570\n",
      "         9.0       0.35      0.27      0.31       402\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.27      0.26      0.26      3438\n",
      "weighted avg       0.31      0.31      0.31      3438\n",
      "\n",
      "accuracy:  0.306282722513089\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.59      0.62      0.61       625\n",
      "         8.0       0.16      0.17      0.17       259\n",
      "         1.0       0.15      0.11      0.13       253\n",
      "         4.0       0.14      0.12      0.13       315\n",
      "         7.0       0.20      0.21      0.21       270\n",
      "         3.0       0.21      0.20      0.21       441\n",
      "         6.0       0.25      0.21      0.23       414\n",
      "         2.0       0.29      0.33      0.31       519\n",
      "         9.0       0.32      0.40      0.36       342\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.30      0.30      3438\n",
      "\n",
      "accuracy:  0.30133798720186156\n",
      "mean accuracy 0.29816753926701567\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "mean_accuracy_model_minkowski = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=1, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model10 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.60       611\n",
      "         8.0       0.18      0.16      0.17       277\n",
      "         1.0       0.17      0.17      0.17       243\n",
      "         4.0       0.14      0.13      0.14       290\n",
      "         7.0       0.14      0.09      0.11       266\n",
      "         3.0       0.25      0.24      0.24       432\n",
      "         6.0       0.22      0.16      0.19       405\n",
      "         2.0       0.32      0.36      0.34       540\n",
      "         9.0       0.40      0.30      0.34       374\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.3126817917393834\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.61       633\n",
      "         8.0       0.14      0.15      0.15       245\n",
      "         1.0       0.18      0.16      0.17       253\n",
      "         4.0       0.15      0.12      0.13       311\n",
      "         7.0       0.17      0.13      0.15       261\n",
      "         3.0       0.21      0.21      0.21       428\n",
      "         6.0       0.17      0.14      0.16       388\n",
      "         2.0       0.30      0.31      0.30       549\n",
      "         9.0       0.38      0.27      0.31       370\n",
      "\n",
      "   micro avg       0.30      0.30      0.30      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.30      0.28      3438\n",
      "\n",
      "accuracy:  0.3007562536358348\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.74      0.58       584\n",
      "         8.0       0.15      0.15      0.15       261\n",
      "         1.0       0.19      0.14      0.16       258\n",
      "         4.0       0.15      0.14      0.14       294\n",
      "         7.0       0.16      0.11      0.13       276\n",
      "         3.0       0.24      0.21      0.22       433\n",
      "         6.0       0.25      0.18      0.21       406\n",
      "         2.0       0.33      0.38      0.36       547\n",
      "         9.0       0.39      0.29      0.33       379\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.3100639906922629\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.72      0.61       660\n",
      "         8.0       0.13      0.15      0.14       261\n",
      "         1.0       0.18      0.16      0.17       238\n",
      "         4.0       0.18      0.14      0.16       307\n",
      "         7.0       0.13      0.10      0.12       251\n",
      "         3.0       0.23      0.22      0.22       427\n",
      "         6.0       0.24      0.18      0.21       387\n",
      "         2.0       0.31      0.33      0.32       542\n",
      "         9.0       0.39      0.30      0.34       365\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.31442699243746364\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.73      0.62       644\n",
      "         8.0       0.18      0.19      0.18       264\n",
      "         1.0       0.17      0.16      0.16       248\n",
      "         4.0       0.17      0.18      0.17       297\n",
      "         7.0       0.12      0.09      0.10       271\n",
      "         3.0       0.21      0.20      0.21       410\n",
      "         6.0       0.21      0.16      0.18       396\n",
      "         2.0       0.34      0.38      0.36       537\n",
      "         9.0       0.44      0.27      0.33       371\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3164630599185573\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.74      0.61       600\n",
      "         8.0       0.16      0.17      0.16       258\n",
      "         1.0       0.15      0.13      0.14       248\n",
      "         4.0       0.13      0.11      0.12       304\n",
      "         7.0       0.15      0.10      0.12       256\n",
      "         3.0       0.23      0.20      0.22       450\n",
      "         6.0       0.23      0.18      0.20       397\n",
      "         2.0       0.33      0.37      0.35       552\n",
      "         9.0       0.40      0.35      0.37       373\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.31239092495637\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.74      0.59       613\n",
      "         8.0       0.19      0.16      0.18       273\n",
      "         1.0       0.16      0.16      0.16       252\n",
      "         4.0       0.17      0.15      0.16       310\n",
      "         7.0       0.14      0.12      0.13       254\n",
      "         3.0       0.23      0.25      0.24       425\n",
      "         6.0       0.22      0.15      0.18       399\n",
      "         2.0       0.34      0.32      0.33       566\n",
      "         9.0       0.35      0.27      0.31       346\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.3080279232111693\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.71      0.61       631\n",
      "         8.0       0.17      0.18      0.18       249\n",
      "         1.0       0.17      0.14      0.16       244\n",
      "         4.0       0.14      0.13      0.13       291\n",
      "         7.0       0.12      0.10      0.11       273\n",
      "         3.0       0.23      0.19      0.21       435\n",
      "         6.0       0.21      0.19      0.20       394\n",
      "         2.0       0.30      0.38      0.34       523\n",
      "         9.0       0.38      0.26      0.31       398\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.25      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.306282722513089\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.73      0.59       631\n",
      "         8.0       0.12      0.13      0.12       263\n",
      "         1.0       0.18      0.13      0.15       251\n",
      "         4.0       0.20      0.17      0.19       310\n",
      "         7.0       0.13      0.11      0.12       247\n",
      "         3.0       0.20      0.23      0.22       404\n",
      "         6.0       0.22      0.17      0.19       391\n",
      "         2.0       0.35      0.37      0.36       549\n",
      "         9.0       0.40      0.22      0.28       392\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.25      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.30831878999418266\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.76      0.61       613\n",
      "         8.0       0.14      0.13      0.14       259\n",
      "         1.0       0.18      0.16      0.17       245\n",
      "         4.0       0.15      0.16      0.16       291\n",
      "         7.0       0.14      0.08      0.10       280\n",
      "         3.0       0.24      0.20      0.22       456\n",
      "         6.0       0.20      0.18      0.19       402\n",
      "         2.0       0.33      0.35      0.34       540\n",
      "         9.0       0.37      0.31      0.33       352\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.28      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.3106457242582897\n",
      "mean accuracy 0.31000581733566024\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=5, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model11 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.60       615\n",
      "         8.0       0.25      0.21      0.23       263\n",
      "         1.0       0.13      0.10      0.11       247\n",
      "         4.0       0.16      0.13      0.14       319\n",
      "         7.0       0.13      0.10      0.11       256\n",
      "         3.0       0.21      0.21      0.21       441\n",
      "         6.0       0.23      0.13      0.17       405\n",
      "         2.0       0.32      0.43      0.37       515\n",
      "         9.0       0.45      0.29      0.35       377\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32024432809773123\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.76      0.63       629\n",
      "         8.0       0.17      0.15      0.16       259\n",
      "         1.0       0.17      0.13      0.15       249\n",
      "         4.0       0.15      0.15      0.15       282\n",
      "         7.0       0.18      0.12      0.15       271\n",
      "         3.0       0.23      0.24      0.23       419\n",
      "         6.0       0.20      0.18      0.19       388\n",
      "         2.0       0.35      0.37      0.36       574\n",
      "         9.0       0.42      0.32      0.36       367\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.3266433973240256\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.75      0.62       640\n",
      "         8.0       0.15      0.15      0.15       248\n",
      "         1.0       0.15      0.15      0.15       225\n",
      "         4.0       0.16      0.13      0.14       304\n",
      "         7.0       0.20      0.13      0.15       262\n",
      "         3.0       0.23      0.24      0.24       433\n",
      "         6.0       0.20      0.13      0.16       416\n",
      "         2.0       0.32      0.38      0.35       538\n",
      "         9.0       0.40      0.32      0.35       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32111692844677137\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.78      0.62       604\n",
      "         8.0       0.20      0.14      0.16       274\n",
      "         1.0       0.21      0.15      0.17       271\n",
      "         4.0       0.17      0.16      0.16       297\n",
      "         7.0       0.18      0.11      0.14       265\n",
      "         3.0       0.20      0.20      0.20       427\n",
      "         6.0       0.18      0.13      0.15       377\n",
      "         2.0       0.32      0.41      0.36       551\n",
      "         9.0       0.43      0.31      0.36       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3216986620127981\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.74      0.62       623\n",
      "         8.0       0.17      0.13      0.15       267\n",
      "         1.0       0.14      0.12      0.13       245\n",
      "         4.0       0.22      0.17      0.19       317\n",
      "         7.0       0.15      0.11      0.12       263\n",
      "         3.0       0.22      0.24      0.23       437\n",
      "         6.0       0.22      0.16      0.19       395\n",
      "         2.0       0.30      0.37      0.33       538\n",
      "         9.0       0.35      0.29      0.31       353\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.30      3438\n",
      "\n",
      "accuracy:  0.3126817917393834\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.79      0.62       621\n",
      "         8.0       0.18      0.15      0.17       255\n",
      "         1.0       0.22      0.17      0.19       251\n",
      "         4.0       0.22      0.19      0.20       284\n",
      "         7.0       0.13      0.10      0.11       264\n",
      "         3.0       0.21      0.21      0.21       423\n",
      "         6.0       0.21      0.15      0.18       398\n",
      "         2.0       0.33      0.40      0.36       551\n",
      "         9.0       0.50      0.32      0.39       391\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.28      0.28      0.27      3438\n",
      "weighted avg       0.31      0.33      0.31      3438\n",
      "\n",
      "accuracy:  0.33275159976730656\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.71      0.61       631\n",
      "         8.0       0.17      0.16      0.16       263\n",
      "         1.0       0.14      0.13      0.14       239\n",
      "         4.0       0.17      0.13      0.14       319\n",
      "         7.0       0.14      0.11      0.12       259\n",
      "         3.0       0.23      0.21      0.22       434\n",
      "         6.0       0.18      0.13      0.15       380\n",
      "         2.0       0.29      0.39      0.33       526\n",
      "         9.0       0.42      0.32      0.36       387\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.3091913903432228\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.80      0.63       613\n",
      "         8.0       0.15      0.12      0.13       259\n",
      "         1.0       0.15      0.12      0.13       257\n",
      "         4.0       0.16      0.14      0.15       282\n",
      "         7.0       0.15      0.10      0.12       268\n",
      "         3.0       0.19      0.20      0.20       426\n",
      "         6.0       0.21      0.13      0.16       413\n",
      "         2.0       0.32      0.36      0.34       563\n",
      "         9.0       0.36      0.29      0.32       357\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.24      0.25      0.24      3438\n",
      "weighted avg       0.28      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.30948225712623617\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.76      0.62       646\n",
      "         8.0       0.18      0.15      0.16       257\n",
      "         1.0       0.16      0.13      0.14       250\n",
      "         4.0       0.17      0.10      0.12       312\n",
      "         7.0       0.15      0.09      0.12       255\n",
      "         3.0       0.20      0.23      0.21       407\n",
      "         6.0       0.17      0.12      0.14       390\n",
      "         2.0       0.32      0.41      0.36       542\n",
      "         9.0       0.40      0.30      0.34       379\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3170447934845841\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.76      0.61       598\n",
      "         8.0       0.19      0.14      0.16       265\n",
      "         1.0       0.17      0.13      0.15       246\n",
      "         4.0       0.15      0.16      0.16       289\n",
      "         7.0       0.16      0.10      0.12       272\n",
      "         3.0       0.23      0.25      0.24       453\n",
      "         6.0       0.21      0.14      0.17       403\n",
      "         2.0       0.33      0.39      0.36       547\n",
      "         9.0       0.43      0.32      0.36       365\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3193717277486911\n",
      "mean accuracy 0.31902268760907504\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=10, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model12 = sum(acc)/10 \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.76      0.62       622\n",
      "         8.0       0.17      0.14      0.15       251\n",
      "         1.0       0.15      0.11      0.13       225\n",
      "         4.0       0.12      0.10      0.11       285\n",
      "         7.0       0.11      0.07      0.09       273\n",
      "         3.0       0.21      0.22      0.21       444\n",
      "         6.0       0.28      0.14      0.18       417\n",
      "         2.0       0.28      0.39      0.32       548\n",
      "         9.0       0.38      0.34      0.36       373\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.31151832460732987\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.78      0.61       622\n",
      "         8.0       0.20      0.14      0.17       271\n",
      "         1.0       0.18      0.09      0.12       271\n",
      "         4.0       0.21      0.16      0.18       316\n",
      "         7.0       0.15      0.11      0.13       254\n",
      "         3.0       0.21      0.22      0.21       416\n",
      "         6.0       0.16      0.14      0.15       376\n",
      "         2.0       0.31      0.40      0.35       541\n",
      "         9.0       0.42      0.31      0.36       371\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3205351948807446\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.78      0.64       646\n",
      "         8.0       0.14      0.12      0.13       267\n",
      "         1.0       0.19      0.11      0.14       254\n",
      "         4.0       0.20      0.15      0.17       302\n",
      "         7.0       0.19      0.14      0.16       255\n",
      "         3.0       0.23      0.24      0.24       428\n",
      "         6.0       0.19      0.13      0.15       401\n",
      "         2.0       0.32      0.42      0.36       531\n",
      "         9.0       0.37      0.31      0.34       354\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.75      0.62       598\n",
      "         8.0       0.17      0.13      0.15       255\n",
      "         1.0       0.14      0.12      0.12       242\n",
      "         4.0       0.15      0.12      0.13       299\n",
      "         7.0       0.15      0.08      0.10       272\n",
      "         3.0       0.19      0.21      0.20       432\n",
      "         6.0       0.20      0.14      0.16       392\n",
      "         2.0       0.30      0.44      0.36       558\n",
      "         9.0       0.49      0.29      0.37       390\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.26      0.25      0.25      3438\n",
      "weighted avg       0.29      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.31180919139034324\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.74      0.60       609\n",
      "         8.0       0.17      0.12      0.14       258\n",
      "         1.0       0.20      0.19      0.20       239\n",
      "         4.0       0.21      0.16      0.18       298\n",
      "         7.0       0.13      0.07      0.09       275\n",
      "         3.0       0.20      0.21      0.21       435\n",
      "         6.0       0.19      0.14      0.16       388\n",
      "         2.0       0.33      0.42      0.37       545\n",
      "         9.0       0.43      0.33      0.38       391\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.27      0.26      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.320826061663758\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.78      0.63       635\n",
      "         8.0       0.20      0.16      0.18       264\n",
      "         1.0       0.18      0.10      0.13       257\n",
      "         4.0       0.18      0.13      0.15       303\n",
      "         7.0       0.15      0.11      0.12       252\n",
      "         3.0       0.21      0.23      0.22       425\n",
      "         6.0       0.19      0.11      0.14       405\n",
      "         2.0       0.31      0.44      0.37       544\n",
      "         9.0       0.37      0.30      0.33       353\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.32344386271087844\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.76      0.60       602\n",
      "         8.0       0.18      0.17      0.17       247\n",
      "         1.0       0.21      0.13      0.16       263\n",
      "         4.0       0.16      0.11      0.13       296\n",
      "         7.0       0.15      0.09      0.11       254\n",
      "         3.0       0.20      0.23      0.21       422\n",
      "         6.0       0.21      0.16      0.18       406\n",
      "         2.0       0.31      0.41      0.36       553\n",
      "         9.0       0.45      0.29      0.35       395\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3179173938336242\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.54      0.77      0.63       642\n",
      "         8.0       0.18      0.13      0.16       275\n",
      "         1.0       0.19      0.13      0.15       233\n",
      "         4.0       0.20      0.18      0.19       305\n",
      "         7.0       0.16      0.08      0.10       273\n",
      "         3.0       0.22      0.25      0.23       438\n",
      "         6.0       0.19      0.13      0.15       387\n",
      "         2.0       0.31      0.41      0.35       536\n",
      "         9.0       0.40      0.30      0.34       349\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.79      0.62       610\n",
      "         8.0       0.15      0.12      0.13       256\n",
      "         1.0       0.19      0.07      0.11       258\n",
      "         4.0       0.15      0.12      0.13       304\n",
      "         7.0       0.14      0.08      0.10       275\n",
      "         3.0       0.22      0.26      0.24       419\n",
      "         6.0       0.19      0.12      0.14       391\n",
      "         2.0       0.32      0.44      0.37       553\n",
      "         9.0       0.40      0.30      0.34       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3196625945317045\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.73      0.60       634\n",
      "         8.0       0.13      0.12      0.13       266\n",
      "         1.0       0.17      0.18      0.17       238\n",
      "         4.0       0.12      0.09      0.10       297\n",
      "         7.0       0.14      0.08      0.11       252\n",
      "         3.0       0.24      0.20      0.22       441\n",
      "         6.0       0.18      0.13      0.15       402\n",
      "         2.0       0.29      0.41      0.34       536\n",
      "         9.0       0.41      0.30      0.35       372\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.24      0.25      0.24      3438\n",
      "weighted avg       0.28      0.31      0.29      3438\n",
      "\n",
      "accuracy:  0.306282722513089\n",
      "mean accuracy 0.3187027341477603\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=15, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model11 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.77      0.63       624\n",
      "         8.0       0.17      0.14      0.16       269\n",
      "         1.0       0.18      0.13      0.15       238\n",
      "         4.0       0.15      0.11      0.13       297\n",
      "         7.0       0.23      0.10      0.14       260\n",
      "         3.0       0.25      0.25      0.25       440\n",
      "         6.0       0.20      0.13      0.16       405\n",
      "         2.0       0.30      0.46      0.37       539\n",
      "         9.0       0.42      0.31      0.36       366\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.60       620\n",
      "         8.0       0.18      0.12      0.15       253\n",
      "         1.0       0.16      0.09      0.11       258\n",
      "         4.0       0.17      0.13      0.15       304\n",
      "         7.0       0.18      0.10      0.13       267\n",
      "         3.0       0.22      0.28      0.24       420\n",
      "         6.0       0.20      0.12      0.15       388\n",
      "         2.0       0.31      0.42      0.36       550\n",
      "         9.0       0.44      0.33      0.38       378\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.79      0.62       620\n",
      "         8.0       0.19      0.12      0.15       273\n",
      "         1.0       0.14      0.08      0.10       257\n",
      "         4.0       0.18      0.15      0.16       302\n",
      "         7.0       0.15      0.09      0.11       260\n",
      "         3.0       0.22      0.25      0.23       426\n",
      "         6.0       0.18      0.10      0.13       401\n",
      "         2.0       0.29      0.42      0.34       535\n",
      "         9.0       0.36      0.27      0.31       364\n",
      "\n",
      "   micro avg       0.31      0.31      0.31      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.31      0.28      3438\n",
      "\n",
      "accuracy:  0.31326352530541013\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.74      0.61       624\n",
      "         8.0       0.16      0.14      0.15       249\n",
      "         1.0       0.18      0.12      0.15       239\n",
      "         4.0       0.17      0.11      0.13       299\n",
      "         7.0       0.19      0.12      0.15       267\n",
      "         3.0       0.20      0.20      0.20       434\n",
      "         6.0       0.21      0.16      0.18       392\n",
      "         2.0       0.32      0.45      0.37       554\n",
      "         9.0       0.43      0.32      0.37       380\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.26      3438\n",
      "weighted avg       0.30      0.32      0.30      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.79      0.64       604\n",
      "         8.0       0.18      0.15      0.16       245\n",
      "         1.0       0.16      0.10      0.12       245\n",
      "         4.0       0.21      0.17      0.18       303\n",
      "         7.0       0.17      0.05      0.08       292\n",
      "         3.0       0.18      0.21      0.19       423\n",
      "         6.0       0.18      0.11      0.14       403\n",
      "         2.0       0.32      0.45      0.37       556\n",
      "         9.0       0.38      0.37      0.37       367\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.79      0.62       640\n",
      "         8.0       0.20      0.13      0.16       277\n",
      "         1.0       0.18      0.12      0.14       251\n",
      "         4.0       0.15      0.12      0.13       298\n",
      "         7.0       0.14      0.12      0.13       235\n",
      "         3.0       0.22      0.23      0.22       437\n",
      "         6.0       0.19      0.13      0.15       390\n",
      "         2.0       0.32      0.44      0.37       533\n",
      "         9.0       0.46      0.28      0.35       377\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32693426410703896\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.77      0.61       625\n",
      "         8.0       0.19      0.12      0.15       256\n",
      "         1.0       0.15      0.09      0.11       249\n",
      "         4.0       0.20      0.13      0.16       310\n",
      "         7.0       0.12      0.08      0.09       250\n",
      "         3.0       0.22      0.21      0.21       450\n",
      "         6.0       0.19      0.12      0.15       400\n",
      "         2.0       0.28      0.45      0.35       526\n",
      "         9.0       0.39      0.29      0.33       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3150087260034904\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.77      0.62       619\n",
      "         8.0       0.17      0.12      0.14       266\n",
      "         1.0       0.15      0.11      0.13       247\n",
      "         4.0       0.19      0.17      0.18       291\n",
      "         7.0       0.19      0.07      0.10       277\n",
      "         3.0       0.19      0.25      0.22       410\n",
      "         6.0       0.17      0.11      0.14       393\n",
      "         2.0       0.31      0.40      0.35       563\n",
      "         9.0       0.43      0.31      0.36       372\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3182082606166376\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.77      0.62       616\n",
      "         8.0       0.18      0.17      0.17       260\n",
      "         1.0       0.11      0.05      0.07       253\n",
      "         4.0       0.21      0.16      0.18       307\n",
      "         7.0       0.13      0.07      0.09       258\n",
      "         3.0       0.20      0.23      0.21       435\n",
      "         6.0       0.18      0.13      0.15       383\n",
      "         2.0       0.31      0.42      0.36       553\n",
      "         9.0       0.42      0.31      0.35       373\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.25      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.31849912739965097\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.78      0.61       628\n",
      "         8.0       0.17      0.09      0.12       262\n",
      "         1.0       0.19      0.14      0.16       243\n",
      "         4.0       0.18      0.14      0.16       294\n",
      "         7.0       0.19      0.10      0.13       269\n",
      "         3.0       0.24      0.25      0.24       425\n",
      "         6.0       0.21      0.13      0.16       410\n",
      "         2.0       0.31      0.45      0.37       536\n",
      "         9.0       0.41      0.30      0.35       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "mean accuracy 0.32239674229203025\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=20, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model13 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.80      0.61       620\n",
      "         8.0       0.16      0.06      0.09       271\n",
      "         1.0       0.21      0.11      0.14       256\n",
      "         4.0       0.19      0.11      0.14       302\n",
      "         7.0       0.12      0.07      0.09       246\n",
      "         3.0       0.21      0.27      0.23       429\n",
      "         6.0       0.21      0.10      0.13       424\n",
      "         2.0       0.31      0.49      0.38       536\n",
      "         9.0       0.39      0.31      0.35       354\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.24      3438\n",
      "weighted avg       0.28      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.3251890634089587\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.76      0.62       624\n",
      "         8.0       0.18      0.18      0.18       251\n",
      "         1.0       0.22      0.14      0.17       240\n",
      "         4.0       0.21      0.18      0.19       299\n",
      "         7.0       0.15      0.04      0.06       281\n",
      "         3.0       0.23      0.24      0.23       431\n",
      "         6.0       0.20      0.14      0.16       369\n",
      "         2.0       0.31      0.44      0.36       553\n",
      "         9.0       0.40      0.31      0.34       390\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.75      0.62       635\n",
      "         8.0       0.17      0.13      0.15       252\n",
      "         1.0       0.15      0.12      0.13       241\n",
      "         4.0       0.17      0.12      0.14       310\n",
      "         7.0       0.16      0.04      0.06       273\n",
      "         3.0       0.22      0.32      0.26       421\n",
      "         6.0       0.21      0.13      0.16       403\n",
      "         2.0       0.30      0.42      0.35       539\n",
      "         9.0       0.40      0.25      0.31       364\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.25      0.24      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.31878999418266435\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.82      0.63       609\n",
      "         8.0       0.19      0.08      0.11       270\n",
      "         1.0       0.19      0.07      0.11       255\n",
      "         4.0       0.20      0.15      0.17       291\n",
      "         7.0       0.17      0.11      0.13       254\n",
      "         3.0       0.20      0.20      0.20       439\n",
      "         6.0       0.20      0.12      0.15       390\n",
      "         2.0       0.29      0.46      0.36       550\n",
      "         9.0       0.39      0.34      0.37       380\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.77      0.61       612\n",
      "         8.0       0.23      0.13      0.16       246\n",
      "         1.0       0.21      0.09      0.13       249\n",
      "         4.0       0.22      0.15      0.18       317\n",
      "         7.0       0.10      0.05      0.06       256\n",
      "         3.0       0.23      0.29      0.25       424\n",
      "         6.0       0.22      0.09      0.13       418\n",
      "         2.0       0.28      0.48      0.35       535\n",
      "         9.0       0.46      0.32      0.38       381\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.26      0.25      3438\n",
      "weighted avg       0.30      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32897033158813266\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.81      0.63       632\n",
      "         8.0       0.18      0.12      0.14       276\n",
      "         1.0       0.25      0.13      0.17       247\n",
      "         4.0       0.22      0.17      0.19       284\n",
      "         7.0       0.13      0.04      0.06       271\n",
      "         3.0       0.21      0.25      0.23       436\n",
      "         6.0       0.20      0.14      0.16       375\n",
      "         2.0       0.31      0.42      0.36       554\n",
      "         9.0       0.38      0.32      0.35       363\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.26      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33304246655031994\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.79      0.63       628\n",
      "         8.0       0.17      0.11      0.13       257\n",
      "         1.0       0.22      0.12      0.16       243\n",
      "         4.0       0.20      0.13      0.16       294\n",
      "         7.0       0.12      0.03      0.05       271\n",
      "         3.0       0.20      0.22      0.21       440\n",
      "         6.0       0.17      0.15      0.16       383\n",
      "         2.0       0.31      0.49      0.38       540\n",
      "         9.0       0.44      0.30      0.35       382\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.32897033158813266\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.82      0.63       616\n",
      "         8.0       0.20      0.14      0.16       265\n",
      "         1.0       0.23      0.13      0.17       253\n",
      "         4.0       0.19      0.11      0.14       307\n",
      "         7.0       0.14      0.11      0.12       256\n",
      "         3.0       0.20      0.23      0.21       420\n",
      "         6.0       0.22      0.09      0.12       410\n",
      "         2.0       0.30      0.42      0.35       549\n",
      "         9.0       0.38      0.33      0.36       362\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.27      0.25      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.32693426410703896\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.79      0.62       599\n",
      "         8.0       0.16      0.13      0.14       261\n",
      "         1.0       0.18      0.12      0.14       233\n",
      "         4.0       0.21      0.11      0.15       318\n",
      "         7.0       0.15      0.09      0.11       243\n",
      "         3.0       0.20      0.22      0.21       435\n",
      "         6.0       0.21      0.12      0.15       395\n",
      "         2.0       0.31      0.42      0.36       570\n",
      "         9.0       0.37      0.33      0.35       384\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.320826061663758\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.80      0.64       645\n",
      "         8.0       0.19      0.08      0.11       261\n",
      "         1.0       0.21      0.10      0.13       263\n",
      "         4.0       0.19      0.19      0.19       283\n",
      "         7.0       0.13      0.06      0.08       284\n",
      "         3.0       0.20      0.22      0.21       425\n",
      "         6.0       0.16      0.10      0.12       398\n",
      "         2.0       0.28      0.48      0.36       519\n",
      "         9.0       0.42      0.26      0.32       360\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.25      0.24      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.32344386271087844\n",
      "mean accuracy 0.3264688772542176\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=30, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model14 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model14"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.83      0.65       635\n",
      "         8.0       0.15      0.11      0.13       242\n",
      "         1.0       0.24      0.15      0.19       245\n",
      "         4.0       0.23      0.13      0.17       306\n",
      "         7.0       0.16      0.05      0.07       272\n",
      "         3.0       0.22      0.26      0.24       441\n",
      "         6.0       0.21      0.13      0.16       386\n",
      "         2.0       0.30      0.43      0.35       550\n",
      "         9.0       0.33      0.28      0.30       361\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.25      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.33420593368237345\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.77      0.60       609\n",
      "         8.0       0.18      0.07      0.11       280\n",
      "         1.0       0.23      0.09      0.13       251\n",
      "         4.0       0.20      0.17      0.18       295\n",
      "         7.0       0.08      0.02      0.03       255\n",
      "         3.0       0.20      0.28      0.23       419\n",
      "         6.0       0.24      0.12      0.16       407\n",
      "         2.0       0.29      0.48      0.37       539\n",
      "         9.0       0.47      0.32      0.38       383\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.24      3438\n",
      "weighted avg       0.29      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.32460732984293195\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.80      0.64       638\n",
      "         8.0       0.20      0.13      0.15       262\n",
      "         1.0       0.19      0.10      0.13       245\n",
      "         4.0       0.21      0.15      0.18       294\n",
      "         7.0       0.12      0.10      0.11       249\n",
      "         3.0       0.20      0.17      0.18       457\n",
      "         6.0       0.17      0.09      0.12       377\n",
      "         2.0       0.29      0.47      0.36       543\n",
      "         9.0       0.35      0.30      0.32       373\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.24      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.82      0.60       606\n",
      "         8.0       0.16      0.08      0.10       260\n",
      "         1.0       0.22      0.07      0.11       251\n",
      "         4.0       0.20      0.12      0.15       307\n",
      "         7.0       0.13      0.03      0.04       278\n",
      "         3.0       0.21      0.33      0.26       403\n",
      "         6.0       0.19      0.10      0.13       416\n",
      "         2.0       0.31      0.49      0.38       546\n",
      "         9.0       0.48      0.30      0.37       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.26      0.24      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.33042466550319954\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.80      0.61       597\n",
      "         8.0       0.16      0.07      0.10       269\n",
      "         1.0       0.16      0.10      0.12       260\n",
      "         4.0       0.18      0.16      0.17       281\n",
      "         7.0       0.12      0.01      0.02       280\n",
      "         3.0       0.22      0.25      0.23       443\n",
      "         6.0       0.19      0.10      0.13       387\n",
      "         2.0       0.30      0.55      0.39       533\n",
      "         9.0       0.47      0.27      0.34       388\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.26      0.23      3438\n",
      "weighted avg       0.28      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.3237347294938918\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.82      0.64       647\n",
      "         8.0       0.14      0.07      0.09       253\n",
      "         1.0       0.19      0.08      0.11       236\n",
      "         4.0       0.22      0.13      0.16       320\n",
      "         7.0       0.12      0.05      0.07       247\n",
      "         3.0       0.18      0.25      0.21       417\n",
      "         6.0       0.22      0.08      0.12       406\n",
      "         2.0       0.30      0.48      0.37       556\n",
      "         9.0       0.38      0.33      0.35       356\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.33187899941826643\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.80      0.64       636\n",
      "         8.0       0.20      0.13      0.16       264\n",
      "         1.0       0.21      0.08      0.11       254\n",
      "         4.0       0.18      0.11      0.13       300\n",
      "         7.0       0.12      0.08      0.10       250\n",
      "         3.0       0.19      0.22      0.21       427\n",
      "         6.0       0.14      0.08      0.10       371\n",
      "         2.0       0.30      0.45      0.36       557\n",
      "         9.0       0.38      0.33      0.35       379\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.24      3438\n",
      "weighted avg       0.28      0.32      0.29      3438\n",
      "\n",
      "accuracy:  0.3240255962769052\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.81      0.61       608\n",
      "         8.0       0.20      0.08      0.12       258\n",
      "         1.0       0.20      0.12      0.15       242\n",
      "         4.0       0.21      0.17      0.19       301\n",
      "         7.0       0.18      0.05      0.07       277\n",
      "         3.0       0.20      0.23      0.21       433\n",
      "         6.0       0.19      0.08      0.11       422\n",
      "         2.0       0.28      0.49      0.36       532\n",
      "         9.0       0.42      0.30      0.35       365\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.26      0.24      3438\n",
      "weighted avg       0.29      0.32      0.28      3438\n",
      "\n",
      "accuracy:  0.32315299592786506\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.53      0.78      0.63       654\n",
      "         8.0       0.21      0.06      0.10       266\n",
      "         1.0       0.18      0.12      0.14       245\n",
      "         4.0       0.24      0.16      0.19       294\n",
      "         7.0       0.11      0.03      0.04       251\n",
      "         3.0       0.19      0.28      0.23       422\n",
      "         6.0       0.17      0.09      0.12       388\n",
      "         2.0       0.31      0.50      0.38       547\n",
      "         9.0       0.39      0.29      0.34       371\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.26      0.24      3438\n",
      "weighted avg       0.29      0.33      0.30      3438\n",
      "\n",
      "accuracy:  0.3321698662012798\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.81      0.60       590\n",
      "         8.0       0.19      0.10      0.13       256\n",
      "         1.0       0.22      0.10      0.13       251\n",
      "         4.0       0.21      0.17      0.19       307\n",
      "         7.0       0.19      0.05      0.08       276\n",
      "         3.0       0.24      0.26      0.25       438\n",
      "         6.0       0.17      0.08      0.11       405\n",
      "         2.0       0.31      0.50      0.38       542\n",
      "         9.0       0.43      0.35      0.39       373\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.27      0.25      3438\n",
      "weighted avg       0.29      0.33      0.29      3438\n",
      "\n",
      "accuracy:  0.33304246655031994\n",
      "mean accuracy 0.32821407795229784\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=50, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model15 = sum(acc)/10 \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.82      0.63       634\n",
      "         8.0       0.23      0.08      0.12       266\n",
      "         1.0       0.28      0.08      0.12       239\n",
      "         4.0       0.19      0.13      0.15       311\n",
      "         7.0       0.11      0.06      0.08       240\n",
      "         3.0       0.19      0.16      0.17       460\n",
      "         6.0       0.19      0.08      0.11       375\n",
      "         2.0       0.27      0.54      0.36       551\n",
      "         9.0       0.42      0.34      0.37       362\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.25      0.23      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32926119837114604\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.84      0.61       610\n",
      "         8.0       0.26      0.07      0.11       256\n",
      "         1.0       0.16      0.06      0.09       257\n",
      "         4.0       0.19      0.13      0.16       290\n",
      "         7.0       0.11      0.01      0.01       287\n",
      "         3.0       0.20      0.33      0.25       400\n",
      "         6.0       0.17      0.05      0.08       418\n",
      "         2.0       0.30      0.53      0.38       538\n",
      "         9.0       0.40      0.25      0.31       382\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.22      3438\n",
      "weighted avg       0.28      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.32577079697498545\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.82      0.61       616\n",
      "         8.0       0.17      0.04      0.06       279\n",
      "         1.0       0.31      0.09      0.13       244\n",
      "         4.0       0.21      0.16      0.18       307\n",
      "         7.0       0.14      0.02      0.03       281\n",
      "         3.0       0.19      0.28      0.23       411\n",
      "         6.0       0.15      0.05      0.07       398\n",
      "         2.0       0.28      0.53      0.36       525\n",
      "         9.0       0.41      0.29      0.34       377\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.25      0.22      3438\n",
      "weighted avg       0.28      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3228621291448517\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.82      0.61       628\n",
      "         8.0       0.16      0.09      0.12       243\n",
      "         1.0       0.23      0.06      0.09       252\n",
      "         4.0       0.21      0.12      0.15       294\n",
      "         7.0       0.11      0.03      0.05       246\n",
      "         3.0       0.22      0.24      0.23       449\n",
      "         6.0       0.16      0.08      0.11       395\n",
      "         2.0       0.30      0.51      0.37       564\n",
      "         9.0       0.38      0.31      0.34       367\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.23      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.33013379872018617\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.83      0.62       604\n",
      "         8.0       0.16      0.03      0.06       265\n",
      "         1.0       0.24      0.10      0.14       245\n",
      "         4.0       0.23      0.13      0.17       317\n",
      "         7.0       0.11      0.03      0.04       261\n",
      "         3.0       0.18      0.21      0.20       434\n",
      "         6.0       0.17      0.04      0.07       408\n",
      "         2.0       0.26      0.57      0.36       522\n",
      "         9.0       0.43      0.27      0.33       382\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.22      3438\n",
      "weighted avg       0.28      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3193717277486911\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.82      0.63       640\n",
      "         8.0       0.18      0.07      0.11       257\n",
      "         1.0       0.24      0.05      0.08       251\n",
      "         4.0       0.18      0.18      0.18       284\n",
      "         7.0       0.10      0.03      0.04       266\n",
      "         3.0       0.20      0.25      0.22       426\n",
      "         6.0       0.18      0.08      0.11       385\n",
      "         2.0       0.31      0.50      0.38       567\n",
      "         9.0       0.41      0.33      0.37       362\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.26      0.26      0.23      3438\n",
      "weighted avg       0.29      0.34      0.29      3438\n",
      "\n",
      "accuracy:  0.33507853403141363\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       620\n",
      "         8.0       0.20      0.10      0.13       262\n",
      "         1.0       0.15      0.06      0.09       249\n",
      "         4.0       0.16      0.12      0.14       293\n",
      "         7.0       0.06      0.00      0.01       276\n",
      "         3.0       0.25      0.32      0.28       427\n",
      "         6.0       0.18      0.09      0.12       402\n",
      "         2.0       0.28      0.51      0.36       527\n",
      "         9.0       0.41      0.27      0.33       382\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.25      0.23      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3275159976730657\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.84      0.62       624\n",
      "         8.0       0.17      0.05      0.07       260\n",
      "         1.0       0.31      0.08      0.13       247\n",
      "         4.0       0.21      0.14      0.17       308\n",
      "         7.0       0.24      0.04      0.07       251\n",
      "         3.0       0.20      0.26      0.22       433\n",
      "         6.0       0.16      0.05      0.07       391\n",
      "         2.0       0.29      0.52      0.37       562\n",
      "         9.0       0.35      0.31      0.33       362\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.25      0.23      3438\n",
      "weighted avg       0.29      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3333333333333333\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.51      0.85      0.64       650\n",
      "         8.0       0.27      0.06      0.10       257\n",
      "         1.0       0.21      0.04      0.07       246\n",
      "         4.0       0.20      0.19      0.20       283\n",
      "         7.0       0.19      0.02      0.04       260\n",
      "         3.0       0.21      0.24      0.23       434\n",
      "         6.0       0.16      0.10      0.12       384\n",
      "         2.0       0.28      0.51      0.36       538\n",
      "         9.0       0.45      0.28      0.35       386\n",
      "\n",
      "   micro avg       0.34      0.34      0.34      3438\n",
      "   macro avg       0.28      0.25      0.23      3438\n",
      "weighted avg       0.30      0.34      0.29      3438\n",
      "\n",
      "accuracy:  0.3371146015125073\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.82      0.61       594\n",
      "         8.0       0.14      0.04      0.06       265\n",
      "         1.0       0.20      0.10      0.13       250\n",
      "         4.0       0.19      0.09      0.12       318\n",
      "         7.0       0.15      0.03      0.04       267\n",
      "         3.0       0.21      0.28      0.24       426\n",
      "         6.0       0.22      0.07      0.10       409\n",
      "         2.0       0.28      0.52      0.36       551\n",
      "         9.0       0.38      0.32      0.35       358\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.25      0.23      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3222803955788249\n",
      "mean accuracy 0.3282722513089006\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=100, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model16 = sum(acc)/10  \n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.84      0.63       640\n",
      "         8.0       0.18      0.03      0.05       272\n",
      "         1.0       0.22      0.05      0.09       237\n",
      "         4.0       0.20      0.16      0.18       299\n",
      "         7.0       0.14      0.04      0.06       253\n",
      "         3.0       0.20      0.21      0.20       435\n",
      "         6.0       0.17      0.07      0.10       397\n",
      "         2.0       0.26      0.46      0.34       553\n",
      "         9.0       0.33      0.33      0.33       352\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.24      0.24      0.22      3438\n",
      "weighted avg       0.27      0.32      0.27      3438\n",
      "\n",
      "accuracy:  0.3219895287958115\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.83      0.62       604\n",
      "         8.0       0.15      0.02      0.04       250\n",
      "         1.0       0.30      0.07      0.11       259\n",
      "         4.0       0.19      0.12      0.15       302\n",
      "         7.0       0.14      0.00      0.01       274\n",
      "         3.0       0.19      0.31      0.23       425\n",
      "         6.0       0.11      0.02      0.03       396\n",
      "         2.0       0.27      0.61      0.38       536\n",
      "         9.0       0.48      0.22      0.30       392\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.24      0.21      3438\n",
      "weighted avg       0.28      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.32344386271087844\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.52      0.84      0.64       644\n",
      "         8.0       0.14      0.04      0.06       245\n",
      "         1.0       0.20      0.09      0.12       257\n",
      "         4.0       0.21      0.17      0.18       309\n",
      "         7.0       0.07      0.01      0.01       276\n",
      "         3.0       0.22      0.16      0.19       438\n",
      "         6.0       0.11      0.04      0.06       395\n",
      "         2.0       0.25      0.59      0.35       518\n",
      "         9.0       0.43      0.31      0.36       356\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.24      0.25      0.22      3438\n",
      "weighted avg       0.27      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.3278068644560791\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.47      0.82      0.59       600\n",
      "         8.0       0.25      0.03      0.05       277\n",
      "         1.0       0.28      0.03      0.05       239\n",
      "         4.0       0.20      0.13      0.16       292\n",
      "         7.0       0.14      0.00      0.01       251\n",
      "         3.0       0.19      0.33      0.24       422\n",
      "         6.0       0.21      0.05      0.08       398\n",
      "         2.0       0.30      0.55      0.39       571\n",
      "         9.0       0.40      0.25      0.31       388\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.27      0.24      0.21      3438\n",
      "weighted avg       0.29      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3240255962769052\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.82      0.62       624\n",
      "         8.0       0.13      0.03      0.04       264\n",
      "         1.0       0.30      0.03      0.06       249\n",
      "         4.0       0.19      0.15      0.17       288\n",
      "         7.0       0.17      0.00      0.01       277\n",
      "         3.0       0.17      0.28      0.21       409\n",
      "         6.0       0.16      0.02      0.04       392\n",
      "         2.0       0.27      0.56      0.37       553\n",
      "         9.0       0.41      0.26      0.32       382\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.26      0.24      0.20      3438\n",
      "weighted avg       0.28      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.32140779522978474\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.84      0.61       620\n",
      "         8.0       0.16      0.03      0.06       258\n",
      "         1.0       0.24      0.09      0.13       247\n",
      "         4.0       0.23      0.15      0.18       313\n",
      "         7.0       0.16      0.04      0.06       250\n",
      "         3.0       0.23      0.22      0.23       451\n",
      "         6.0       0.16      0.06      0.09       401\n",
      "         2.0       0.27      0.55      0.36       536\n",
      "         9.0       0.41      0.31      0.35       362\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.26      0.25      0.23      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.3310063990692263\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.83      0.63       627\n",
      "         8.0       0.14      0.06      0.08       253\n",
      "         1.0       0.26      0.05      0.08       258\n",
      "         4.0       0.23      0.16      0.19       320\n",
      "         7.0       0.12      0.01      0.01       262\n",
      "         3.0       0.23      0.27      0.25       430\n",
      "         6.0       0.16      0.09      0.11       379\n",
      "         2.0       0.27      0.53      0.36       543\n",
      "         9.0       0.37      0.25      0.30       366\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.22      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32838859802210585\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.49      0.83      0.62       617\n",
      "         8.0       0.14      0.01      0.01       269\n",
      "         1.0       0.27      0.04      0.07       238\n",
      "         4.0       0.17      0.14      0.15       281\n",
      "         7.0       0.24      0.03      0.05       265\n",
      "         3.0       0.20      0.30      0.24       430\n",
      "         6.0       0.19      0.03      0.05       414\n",
      "         2.0       0.28      0.57      0.38       546\n",
      "         9.0       0.43      0.32      0.36       378\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.27      0.25      0.22      3438\n",
      "weighted avg       0.29      0.33      0.27      3438\n",
      "\n",
      "accuracy:  0.33304246655031994\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.50      0.81      0.62       627\n",
      "         8.0       0.17      0.04      0.06       255\n",
      "         1.0       0.30      0.09      0.14       235\n",
      "         4.0       0.20      0.15      0.17       290\n",
      "         7.0       0.08      0.01      0.02       244\n",
      "         3.0       0.20      0.26      0.23       432\n",
      "         6.0       0.15      0.04      0.06       412\n",
      "         2.0       0.28      0.53      0.37       568\n",
      "         9.0       0.39      0.30      0.34       375\n",
      "\n",
      "   micro avg       0.33      0.33      0.33      3438\n",
      "   macro avg       0.25      0.25      0.22      3438\n",
      "weighted avg       0.28      0.33      0.28      3438\n",
      "\n",
      "accuracy:  0.32897033158813266\n",
      "set(['5.0', '8.0', '1.0', '4.0', '7.0', '3.0', '6.0', '2.0', '9.0'])\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         5.0       0.48      0.86      0.62       617\n",
      "         8.0       0.17      0.03      0.05       267\n",
      "         1.0       0.19      0.02      0.04       261\n",
      "         4.0       0.21      0.16      0.18       311\n",
      "         7.0       0.12      0.01      0.01       283\n",
      "         3.0       0.18      0.24      0.21       428\n",
      "         6.0       0.16      0.03      0.05       381\n",
      "         2.0       0.27      0.60      0.38       521\n",
      "         9.0       0.43      0.25      0.31       369\n",
      "\n",
      "   micro avg       0.32      0.32      0.32      3438\n",
      "   macro avg       0.25      0.24      0.21      3438\n",
      "weighted avg       0.27      0.32      0.26      3438\n",
      "\n",
      "accuracy:  0.3248981966259453\n",
      "mean accuracy 0.3264979639325189\n"
     ]
    }
   ],
   "source": [
    "acc = []\n",
    "for i in range(5):\n",
    "    for train_index, test_index in kf.split(marketing):\n",
    "        X_train, X_test = data.iloc[train_index], data.iloc[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        all_labels = pd.concat([y_train, y_test], axis=0)\n",
    "        knn = KNeighborsClassifier(n_neighbors=150, algorithm = 'auto', metric = 'minkowski')\n",
    "        knn.fit(X_train, y_train)\n",
    "        predicted = knn.predict(X_test)\n",
    "        if len(set(predicted)) == len(set(y_test)):\n",
    "            unique_labels = set(predicted)\n",
    "            print unique_labels\n",
    "        elif len(set(predicted)) < len(set(y_test)):\n",
    "            unique_labels = set(y_test)\n",
    "        else :\n",
    "            unique_labels = set(predicted)\n",
    "        print classification_report(y_test, predicted, target_names=unique_labels)\n",
    "        acc.append(accuracy_score(y_test, predicted))\n",
    "        print \"accuracy: \", accuracy_score(y_test, predicted)\n",
    "        \n",
    "mean_accuracy_model17 = sum(acc)/10\n",
    "mean_accuracy_model_minkowski.append(sum(acc)/10)\n",
    "print \"mean accuracy\", mean_accuracy_model17"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.29816753926701567, 0.31000581733566024, 0.31902268760907504, 0.3187027341477603, 0.32239674229203025, 0.3264688772542176, 0.32821407795229784, 0.3282722513089006, 0.3264979639325189]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print mean_accuracy_model_minkowski\n",
    "k = [1, 5, 10, 15, 20, 30, 50, 100, 150]\n",
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure()\n",
    "ax = plt.axes()\n",
    "ax.plot(k, mean_accuracy_model_minkowski)\n",
    "ax.plot(k, mean_accuracy_model_euclidean)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
